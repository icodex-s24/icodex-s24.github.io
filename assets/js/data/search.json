[ { "title": "Building a Docker Environment Completely Offline on Windows 10", "url": "/posts/how-use-docker-windows-offline/", "categories": "Docker", "tags": "docker, win10, offline", "date": "2024-11-20 20:52:36 -0500", "snippet": "Deployed System EnvironmentOS: Windows 10 (19045.2364, KB5021233 up)Internet: Fully Offline (Can only be built through package installation.)Docker: Docker Desktop 4.27.2(137060)WSL: 2.1.4 up (Manu...", "content": "Deployed System EnvironmentOS: Windows 10 (19045.2364, KB5021233 up)Internet: Fully Offline (Can only be built through package installation.)Docker: Docker Desktop 4.27.2(137060)WSL: 2.1.4 up (Manual installation)To Using Windows Docker: No need to install Ubuntu (unnecessary). No need to use Hyper-V (deprecated in the new version). Using Linux Containers on Windows.Step 1: Verify Windows Features in Control PanelDocker has integrated WSL 2, so Hyper-V is no longer required. lol~Enable “Virtual Machine Platform” and “Windows Subsystem for Linux” in Docker’s New Version.After enabling, please proceed with a direct reboot.# Windows PowerShell1.Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform2.Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux3. Check enabled features- Virtual Machine Platform- Windows Hypervisor platform- Windows Subsystem for Linux4. Reboot PCStep 2: Package installation preparation Download and install WSL 2 from Github Microsoft WSL Download and install Docker Desktop When installing, just check all optionsAfter installation is complete, please don’t open it!Step 3: Setting WSL version to 2# CMDwsl -vwsl --set-default-version 2Step 4: Restart your PCStep 5: Start DockerDocker is running now!Though WSL, some missing elements are visible, but they do not affect usage.Notice!!If you have already installed and encountered a failure, please follow the steps below to remove the occupied files. To remove Docker using Add or Remove Programs, Check C:\\Program Files\\Docker Remove Delete C:\\Users\\user\\AppData\\Local\\Docker Delete C:\\Users\\user\\AppData\\Roaming\\Docker Reboot PC" }, { "title": "17-lessons-about-human-nature", "url": "/posts/17-lessons-about-human-nature/", "categories": "Human", "tags": "human nature, lessons, me2.0", "date": "2024-10-08 07:33:54 -0400", "snippet": "17 Raw Lessons About Human Nature Small Gains and Momentum: Small improvements, whether in business, personal habits, or relationships, create a sense of momentum. This psychological feeling o...", "content": "17 Raw Lessons About Human Nature Small Gains and Momentum: Small improvements, whether in business, personal habits, or relationships, create a sense of momentum. This psychological feeling of progress is a powerful driver of motivation and long-term success. Sweat the Small Stuff: Small, consistent actions and decisions compound over time, both positively and negatively. Marginal gains, like brushing teeth or optimizing small business processes, significantly impact success when maintained over the long term. The Importance of Course Correction: Consistent small adjustments, such as course corrections in aviation, prevent major deviations. This principle applies to life and work, where minor habit shifts can compound over time, leading to significant positive outcomes. Commitments to Yourself Build Self-Esteem: Keeping promises to yourself, even when no one is watching, fosters self-respect and confidence. Breaking these commitments, however, leads to a downward spiral, while honoring them results in personal growth and empowerment. Avoiding Discomfort as a Motivator: Procrastination often stems from a desire to avoid discomfort. By identifying and addressing the root causes of discomfort, you can reduce procrastination and stay on track toward your goals. Pressure as a Privilege: Embracing pressure, rather than avoiding it, is key to personal growth and achievement. Pressure indicates you care about something, and managing it well can lead to success and fulfillment. Intrinsic vs. Extrinsic Motivation: Long-term satisfaction comes from shifting focus away from external rewards like money or status and toward intrinsic motivators, such as personal joy, learning, and growth. Instincts and Authenticity: Trusting your instincts and staying true to your authentic self leads to fulfilling outcomes. Deviating from your instincts for external validation or novelty can cause dissatisfaction and regret. Execution Over Strategy: While planning and strategizing are important, consistent execution is what drives real success. High performers focus on executing small steps that accumulate into significant outcomes over time. Resisting Labels and Rigidity: Defining yourself by past achievements or rigid labels can be limiting. To avoid self-imposed limitations, remain flexible and open to new experiences. The Power of Context and Market Fit: Applying your skills in the right context or market, where demand is high and supply is low, can result in exponential gains. This can transform your value by aligning your abilities with the right opportunities. The Frame Matters More Than the Picture: Perception and context often matter more than content. Psychological framing influences how people interpret products, experiences, or even their own emotions, which can increase perceived value and drive success. Beliefs Are Not a Choice: People’s beliefs are shaped by experiences and evidence, not by conscious choice. Changing beliefs requires exposure to new evidence or perspectives, which can alter self-perception and personal growth. Managing Scrutiny and Public Perception: As public figures gain visibility, it’s crucial to maintain focus on core values and filter out unnecessary noise. Public interest can change how others perceive you, but staying grounded in principles helps navigate scrutiny. The Value of Discomfort and Challenge: Continually seeking out challenges and pushing through discomfort—whether through imposter syndrome or new experiences—leads to personal and professional growth. Avoiding Mono-Thinking: Avoid rigidly adhering to a single ideological belief system, as it can limit critical thinking. Embrace multiple perspectives and resist falling into ideological echo chambers for a more nuanced worldview. Long-Term Thinking and Compounding: Sustainable success comes from designing systems that consider long-term needs. Balancing intensity with consistency ensures that efforts compound over time, leading to enduring results. These lessons emphasize the importance of small, consistent efforts, staying true to your instincts, embracing challenges, and aligning skills with the right context for maximum impact.CitesSteven Bartlett" }, { "title": "AI Training vs Inference", "url": "/posts/ai-training-inference/", "categories": "AI, Basics", "tags": "ai, basics, training, inference", "date": "2024-09-09 01:39:16 -0400", "snippet": "The AI market is increasingly divided into two primary segments: training and inference. Understanding these segments is crucial for grasping the dynamics of AI development and deployment.Training ...", "content": "The AI market is increasingly divided into two primary segments: training and inference. Understanding these segments is crucial for grasping the dynamics of AI development and deployment.Training vs. InferenceTraining Definition: Training is the process of creating AI models by feeding them large datasets. This involves teaching the model to recognize patterns and make predictions based on the input data. Key Players: The training market is dominated by major tech companies like OpenAI, Anthropic, and Mistral. These companies invest heavily in the computational resources required for training large models, such as GPUs and specialized hardware. Market Characteristics: The training market does not see significant increases in traffic because many companies, such as xxx, have paused the development of foundational models. Training requires substantial resources and is often a one-time or infrequent process as models are developed and then deployed.Inference Definition: Inference is the process of using a trained model to make predictions or decisions based on new, unseen data. This is where the model is put into action. Market Growth: The inference market is expected to grow significantly as more users begin to utilize AI models. This includes accessing models via APIs or using open-source implementations. Key Characteristics: Inference is less resource-intensive than training and can be performed on a broader range of hardware, including less powerful CPUs. As more applications and services integrate AI capabilities, inference traffic is projected to increase dramatically.Examples of Training and Inference Training Example: Image Recognition: A model is trained using thousands of labeled images (e.g., pictures of cats and dogs). During training, the model learns to identify features that distinguish cats from dogs based on the provided examples. Inference Example: Retail Application: Once the image recognition model is trained, it can be deployed in a retail setting. When a customer scans an item, the model processes the image to determine if it is a cat or a dog, providing a quick response without human intervention. Infrastructure and Resource Requirements Training: Requires high-performance GPUs and significant power consumption. For example, training a large language model (LLM) can involve clusters of GPUs operating at high power levels. Companies like Nvidia dominate the training hardware market, providing specialized GPUs optimized for these workloads. Inference: More efficient and can be executed on commodity hardware. The model weights are fixed after training, allowing for easy duplication across multiple machines. Inference workloads are expected to consume a larger share of computing resources as AI applications proliferate. Future Trends Shift from Training to Inference: As the AI market matures, there is a noticeable shift from investing in training capabilities to optimizing inference processes. This includes enhancing infrastructure to handle the increasing demand for real-time predictions and decisions. Emergence of New Players: While established companies like Nvidia and Google lead in training, new companies are emerging to focus on inference optimization, potentially disrupting the market. Integration with Existing Systems: Businesses are likely to find it easier to integrate pre-trained AI models into their workflows, leading to broader adoption of AI technologies.ConclusionThe AI market’s division into training and inference highlights the different challenges and opportunities within the field. While training remains a resource-intensive process primarily handled by large tech companies, inference is becoming increasingly accessible and essential as more applications leverage AI capabilities. As the demand for AI solutions grows, the focus on optimizing inference will likely lead to significant advancements in how AI is deployed across various industries." }, { "title": "QLoRA - How to Fine tune an LLM on a Single GPU", "url": "/posts/qlora-fine-tuning-on-a-single-gpu/", "categories": "AI, LLM", "tags": "fine-tuning, single gpu", "date": "2024-09-07 07:17:21 -0400", "snippet": "(00:00) fine-tuning is when we take an existing model and tweak it for a particular use case although this is a simple idea applying it to large language models isn’t always straightforward the key...", "content": "(00:00) fine-tuning is when we take an existing model and tweak it for a particular use case although this is a simple idea applying it to large language models isn’t always straightforward the key challenge is that large language models are very computationally expensive which means fine-tuning them in a standard way is not something you can do on a typical computer or laptop in this video I’m going to talk about Cur which is a technique that makes fine-tuning large L language models much more accessible and(00:33) if you’re new here welcome I’m sha I make content about data science and Entrepreneurship and if you enjoy this video please consider subscribing that’s a great no cost way you can support me in all the content that I make since I talk in depth about fine-tuning in a previous video of this series here I’ll just give a highlevel recap of the basic idea so like I said before fine tuning is tweaking an existing model for a particular use case so an analogy for this fine tuning is is like taking a raw(01:01) diamond and refining it and Distilling it into something more practical and usable like a diamond you might put on a diamond ring in this analogy the raw diamond is your base model so this would be something like gpt3 while the final Diamond You Come Away with is your fine-tuned model which is something like chat GPT and so again the core problem with fine-tuning large language models is that they are computationally expensive to get a sense of this let’s say you have a pretty powerful laptop and it comes with a CPU and a GPU where(01:34) the CPU has 16 GB of RAM and your GPU has 16 GB of RAM let’s say we want to finetune a 10 billion parameter model each of these parameters corresponds to a number which we need to represent on our machine standard way of doing this is using the fp16 number format which requires about two bytes of memory per parameter so just doing some simple math here 10 billion parameters time 2 bytes per parameter comes to 20 GB of memory just to store the model parameters so one problem here is that this 20 GB model won’t fit on the CPU or GPU but(02:14) maybe we can get clever in how we distribute the memory so the load of the model is split between the CPU and GPU and that allows us to do things like inference and make predictions with the model however when we talk about fine-tuning we’re talking about retraining the model par parameters which is going to require more than just storing the parameters of the model another thing we need are the gradients these are numbers that we use to update the model parameters in the training process we’ll have a gradient which is(02:46) just going to be a number for every parameter in the model so this adds another 20 GB of memory so we’ve went from 20 to 40 and now even if we get super clever with how we distribute it across our CPU and GPU GPU it’s still not going to fit so we’d actually need to add another GPU to even make that work but of course this isn’t the whole story you also need room for the optimizer States so if you’re using an Optimizer like atom which is very widely used this is going to take the bulk of the memory footprint for model training(03:18) where this is coming from is an Optimizer like atom is going to store a momentum value and variance value for each parameter in your model so we’ll have two numbers per parameter additional these values need to be encoded with higher Precision so instead of the fp16 format these are going to be encoded in the fp32 format and so when it’s all said and done there’s about a 12x multiplier for the memory footprint of these Optimizer states which means we’re going to need a lot more gpus to actually fine-tune this model these(03:51) calculations are based on reference number two which is a paper about zero which is a method for efficiently fine-tuning these deep neural networks works so we come to a grand total of 160 GB of memory required to train a 10 billion parameter model of course these enormous memory requirements aren’t going to fit on your laptop and it’s going to require some heavyduty Hardware to run so 160 GB if you get like a 80 gb GPU like the a100 you’ll need two of those at least and those are about $20,000 a pop so you’re probably talking(04:28) about like $50,000 just for the hardware to fine-tune a 10 billion parameter model in the standard way this is where Cur comes in so Cura is a technique that makes this whole fine-tuning process much more efficient so much so that you can just run it on your laptop here without the need for all these extra gpus before diving into Cur a key concept that we need to understand is quantization and even though quantization might sound like this scary and sophisticated word it’s actually a very simple idea whenever you hear(05:02) quantization just think splitting a range of numbers into buckets so as an example let’s consider any number between 0 and 100 obviously there are infinite numbers that can fit in this range you know there’s like 27 55.3 83.7 823 and so on and so forth what quantization consists of is taking this infinite range of numbers and splitting it into discrete bins one way of doing this is quantizing this infinite range using whole numbers so what that would look like for our three numbers here is that 27 would go into this 27th bucket(05:38) 55.3 would go into this 55 bucket and then 83.78% would go to 20 55 would go to 50 and 83 would go to 80 so that’s the basic idea and the reason this is important is that quantization is required whenever you want to represent numbers in a computer and the reason is that if you wanted to encode a single number that lives in an infinite range of possibilities this will require infinite btes of memory it just can’t be done at some point when you’re talking about a physically constrained system like a computer you have to make some(06:25) approximations and so if we go from this infinite range to this range quantized by whole numbers this would require about 0.875 bytes per number and then if we go one step further and just split it into these 10 different buckets it would require about half a bite per number one thing to point out here is that there’s a natural tradeoff you know we could have a lot of buckets which would give us a lot of precision but it’s going to increase the memory footprint of our model however you could have very few(06:56) buckets for quantization which would minimize the memory footprint but this would be a pretty crude approximation of the model you’re working with so balancing this tradeoff is a key contribution of Q Laura there are actually Four ingredients that come together to make up Q Laura the first is 4bit normal float the second is double quantization the third are paged optimizers and then finally is loraa I’m going to talk through each of these ingredients one by one starting with ingredient one one 4bit normal float all(07:32) this is is a better way to bucket numbers it’s a better way to do quantization so let’s break it down when we say something is 4 bit what we mean is we’re using four binary digits to represent that piece of information and since each digit can be either zero or one this gives us 16 unique combinations which means with a 4bit representation we have 16 buckets at our disposal for quantization compressing a range of numbers into just 16 buckets is great for memory saving you know we only have four bits which translates to half a(08:11) bite per parameter so if we have 10 billion parameters that’s going to translate to 5 GB of memory but of course this brings up the same problem I mentioned earlier which is we have this tradeoff it’s like yeah we get huge memory savings but now we have a very crude approximation of the number we’re trying to represent the way ingredient one this 4bit normal float works is it buckets the numbers in a particular and clever way suppose we have all the parameters in our model and we plot their distribution when it comes to(08:41) these deep neural networks it turns out that most of the parameter values are going to be around zero and very few values are going to be much smaller and much larger than zero what that means is we have something that resembles a normal distribution when it comes to our model parameters so if we follow a quantization strategy that I talked about a couple slides ago where we just split the numbers into these equally spaced buckets we’re going to get a pretty crude approximation of these model parameters because most of our(09:12) numbers are just going to be sitting in these two buckets here with very few numbers sitting in these end buckets here an alternative way we can do quantization is instead of using equally spaced buckets we can consider using equally sized buckets so instead of mapping each parameter into these eight buckets we map these parameter values into these eight buckets so now you can see that we have a much more even distribution of model parameters across these buckets and this is exactly the idea that 4-bit normal float uses to balance that(09:49) tradeoff between low memory and accurately representing model parameters so the next ingredient is double quantization which consists of quantizing the quantization constants I know the word quantize is appearing way more than anyone would ever like on this slide but let’s break it down step by step to see what this all means so consider this simple quantization strategy so let’s say we have this array of numbers X that’s represented using 32 bits and we want to translate it into an 8bit representation on the left hand(10:22) side here and then we want this 8bit representation to have values in between minus 127 and 127 essentially what we’re doing is we’re quantizing by whole numbers forcing it to live in this range ofus 127 to 127 so that’s what we’re trying to do so a simple way of doing that is we rescale all the values in this array by the absolute maximum value in the array and then we’ll multiply it by the new maximum value which is 127 in our quantized range and then we’ll round it just so that there are no decimal(10:56) points so this is a very simple way we can quantize this arbitrary array encoded in 32bit into a 8bit integer representation and just to make this more simple we can translate this prefactor here into a constant encoded in 32bit so while this simple quantization strategy isn’t how we do it in practice cuz again if we’re doing the equally sized buckets it’s not just going to be this linear transformation that we’re seeing here but this does illustrate the point that anytime you do Quant ization there’s going to be some(11:31) memory overhead involved in that computation so in other words these constants are going to take up precious memory in your system so as an initial strategy you might think well if we have this input tensor or input array and we rescale all the parameters we’re only going to have one new constant a 32bit number for all the parameters in our model what’s the big deal about that what’s another number compared to 10 billion parameters for example so while this does have trivial memory implications it may not be the best way(12:01) to quantize our model parameters because this is going to be very sensitive to extreme values in our input tensor and the reason is if we’re talking about these model parameters where most of them are close to zero but then you have this one parameter way far off in the tails that is your absolute Max it’s going to introduce a lot of bias in your quantization process so this standard quantization approach does minimize memory but it comes with maximum potential for bias an alternative strategy could be as follows where we(12:35) take the input tensor we reshape it to look like this and then we split this tensor into buckets and then within each bucket we do the rescaling process so this significantly reduces the odds of one extreme value skewing all the model parameters in the quantization process this is called blockwise quantization and although it comes with with a greater memory footprint it has a lot less bias so to mitigate the memory cost of this blockwise quantization approach we can employ double quantization which will do this quantization process here(13:14) but then we’ll do the quantization process once again on all these constants that pop up from this blockwise approach so if we just kind of repeat this very simple strategy here now we have an array of constants we have multiple constants popping out they’re encoded in 32bit and then we can quantize them into a lower bit format using this simple approach that’s double quantization so we are indeed quantizing the quantization constants while it might be an unfortunate name it is a pretty straightforward process so(13:45) ingredient three is a paged Optimizer all we’re doing here is looping in your CPU into the training process so let’s say we have a small model like 51 which has 1.3 billion parameters which which based on those same calculations we saw earlier would require about 21 GB of memory for full fine-tuning the dilemma here is that although we have enough memory across the GPU and CPU for all 21 GB needed to fully fine-tune 51 this isn’t something that necessarily just works out of the box these are independent modules on your machine and(14:23) typically the training process will just be restricted to your GPU and so this paged Optimizer what that means is instead of just restricting training to only fitting on your GPU you can move memory as needed from the GPU to the CPU and then bring it back onto the GPU as needed what that might look like is you’ll start model training and you’ll have one page of memory and a page of memory is like a fundamental unit or block of memory on the GPU or CPU the pages will start accumulating during the training process until your memory gets(14:59) full and then at which point if you have this paged Optimizer approach you can start moving pages of memory over to the CPU to make room for new memory for training and then if you need a page of memory that was moved to the CPU back onto the GPU you can make room for it there and then you can just move it back over using this paged Optimizer this is the basic idea honestly I don’t know exactly how this all works I’m not like a hardware guy I don’t know how computer architecture fully works but this is(15:29) like my highlevel understanding as a data scientist so if you want to learn more check out the cura paper where they talk a little bit more about it and provide some additional references the final ingredient of cura is loraa which stands for low rank adaptation and so I actually talked about Lura in depth in a previous video on fine-tuning so here I’m just going to give a brief highlevel description of how it works if you want more details you can check out that previous video or check out the low R paper Linked In the description below(16:01) what Laura does is that it fine-tunes a model by adding a small number of trainable parameters so we can see how this works by contrasting it with the standard full fine-tuning approach so let’s say this is our model here this is our neural network and we have this input layer we have some hidden layer and then we have the output layer here full fine tuning consists of retraining every single parameter in this model we’re just considering one layer at a time we’ll have this weight Matrix corresponding to all these lines in this(16:35) picture here we’ll have this Matrix W KN consisting of all the parameters for that particular layer and all of these are trainable while that’s probably not going to be a big deal about these six parameters in this shallow Network here if you have a large language model these matrices will get pretty big and you’ll have a lot of them because you’ll have a lot of layers Lura on the other hand instead of fine-tuning every every single parameter in your model it’ll actually freeze every parameter in the(17:03) model and it works by adding a small set of trainable parameters which you’ll then fine-tune the way this works is you’ll have your same hidden layer and then you’ll add a small set of trainable parameters through this Delta W Matrix so if you’re looking at this you might think well how does this help us because Delta W is going to be the same size as W KN so how is this adding a smaller set of trainable parameters and so the trick with Laura is that this Delta W will actually be the product of two smaller(17:38) matrices b and a which have the appropriate Dimensions to make all the math work out so visually what that looks like is you have your W KN here but then you have BNA a which have far fewer parameters than W KN but when you multiply it together their product it’ll have the proper shape to make all the Matrix operations work here so you’ll actually freeze W KN so you won’t train these parameters and then these parameters housed in BNA will be the trainable ones the result of training the model this way is that you can get(18:11) 100 to even 1,000x savings and model parameters so instead of having to train 10 billion parameters you’re only having to train like 100 million parameters or 50 million parameters so let’s bring these four ingredients together let’s first look at the standard fine tuning approach as a baseline so here let’s say we have our base model represented in fp16 so we’ll have this memory footprint from the base model and then we’ll have this larger memory footprint from the optimizer States and then we won’t have(18:42) any adapters because adapters only come in when doing lowra or another parameter efficient fine-tuning method and so we’ll do like the forward pass on the model it’ll go to the optimizer and then the optimizer will do the backward pass and will update the model parameters this is the same standard fine-tuning approach we talked about earlier so a 10 billion parameter model will require about 160 gbes of memory another thing we could do is use lowra so we can get that 100 to 1,000x Savings in the number(19:14) of trainable parameters we still have our model represented in 16bit but now instead of fine-tuning every single parameter in the model we only have a small number of trainable parameters and then each of those parameters will have an Associated op Optimizer state which significantly reduces the memory footprint so that a 10 billion parameter model would only require about 40 GB of memory while this is a tremendous savings like a 4X Savings in memory 40 GB is still a lot to ask for from consumer Hardware so let’s see how Cur(19:47) helps the situation even further the key thing here is that instead of using the 16bit representation we can use ingredient one and encode the base model as 4bit normal float and then we’ll have the same number of trainable parameters from Lura so that’ll be exactly the same and then we can use ingredient 3 with the paged optimizers to avoid any out of memory errors that might come up during the training process with that and including the double quantization here we can use Cur to fine-tune a 10 billion(20:21) parameter model with just about 12 gigabyt of memory which is something that can easily fit in consumer Hardware and can even run using the free resources available on a Google collab so let’s see a concrete example of that here we’re going to do fine-tuning using mistol 7B instruct to respond to YouTube comments this example is available on the Google collab associated with this video the model and data set are freely available on hugging face and then additionally there is a GitHub repo that has all the resources put together as(20:58) well as the code to generate the training data set here first thing we need to do is import some libraries everything here is coming from hugging face their Transformers Library their PFT Library which is parameter efficient fine-tuning this is what’s going to allow us to do Q lur and then we’re using hugging fa’s data sets library because I uploaded the training data set onto hugging faes Hub and then finally we just import the Transformers Library these are kind of like sub dependencies to ensure some of these modules work I(21:27) think it’s mainly this one prepare mod for kbit training you don’t need to import these but you need to make sure they’re installed in your environment and this was actually a paino because bits and bytes only works on Linux and windows and on Nvidia hardware and then gptq this format for encoding models it doesn’t run on Mac so as a Mac User this was kind of frustrating lots of trial and error to try to get it to work on my machine locally but I wasn’t able to get it to work so if anyone was able to get(21:56) it to run on a M1 or a M2 or or even M3 send me your example code or send me any resources you found helpful I would love to get a version working on my personal machine but since collab they have a Linux environment using Nvidia Hardware the code here works fine next we can load the quantized model and so here we’re going to grab a version of mistol 7B instruct from the bloke and so if you’re not familiar with the bloke he’s actually quantized and shared thousands of these large language models(22:28) completely for free on the hugging face Hub and then we can just import this model using this from pre-trained method so we just need to specify the model name on The Hub device map set to Auto just has the Transformers Library kind of figure out the optimal way to spread the load between the GPU and CPU to load in the model trust remote code basically it’s not going to allow like a custom model file to run on your local machine so this is just a way to protect your machine when downloading code from The(22:59) Hub and then revision main is just saying we want the main version of the model available at this repo here then again gptq which is the format used here does not run on Mac there are some other options with Mac but I wasn’t able to get it working on my machine once we have the quantized model loaded we can load the tokenizer so we can do this actually pretty easily using this from pre-train method so we just specify the model name and then specify this use fast argument as true with just those two simple blocks of code we can use the(23:33) base model one thing we do here is we put the model into evaluation mode which apparently deactivates the Dropout modules next we can craft our prompt so let’s say we have a command from YouTube that says great content thank you and then we put it into the proper prompt format so mistal 7B instruct is an instruction tuned model so it’s actually Expecting The Prompt in a very particular format and namely it’s just expecting this instruction start and instruction end special tokens in the prompt so we set that up very easily(24:09) what this is doing is it’s just going to dynamically take this comment variable and stick it into this prompt here and then once we have that we can pass the prompt to the tokenizer so basically we’re taking this prompt and We’re translating it from a string into an array of numbers and then we can take that array of numbers and we can pass it into our model to generate more text once we do that we can get the outputs and then pass them back into our tokenizer and have the tokenizer decode the vector back into English the output(24:42) of this great content thank you comment is I’m glad you found the content helpful if you have any specific questions or topics you’d like me to cover in the future feel free to ask I’m here to help in the meantime I’d be happy to answer any questions you might have about the content I’ve already provided just just let me know which article or blog post you’re referring to and I’ll do my best to provide you with accurate and up-to-date information thanks for reading and I look forward to(25:06) helping you with any questions you may have so while this is a fine response there are a few issues with it one it’s very long I would never respond to a YouTube comment like this second is it kind of just like repeats itself it’s like glad you found it helpful feel free to ask and then it says happy to answer questions that you have happy to provide you with acurate update information and like look forward to helping you with questions so saying the same thing in different words like a few different(25:32) times and then finally it says thanks for reading and if this is for YouTube comments people aren’t reading this stuff they’re watching videos so one thing we can do to improve model performance is by doing so-called prompt engineering I actually have a in-depth guide on prompt engineering where I talk about seven tricks to kind of improve your prompts in a previous video of the series so feel free to check that out if you’re interested The Prompt that I ended up using here is something that I generated through trial and error and(26:00) the way I did that is using a website called together. which I can link in the description below essentially together. a they have a chat interface kind of like chat GPT but for a lot of open- source models including mistl 7B instruct version 0.2 so I was able to test a lot of prompt ideas and get feedback and just kind of eyeball which gave the best performance and I ended up using that one so I have this set of instructions here sha gbt functioning as a virtual data science consultant on YouTube communicates in clear accessible(26:34) language escalating to technical depth upon request it reacts to feedback aply and ends responses with its signatur sha GPT sha gbt will tailor the length of its responses to match the viewers comment providing concise acknowledgements to brief expressions of gratitude or feedback thus keeping the interaction natural and engaging then I have this instruction please respond to the following comment and then I have this Lambda function where given a comment I’ll piece together this instruction string and comment together(27:07) within the instruction special tokens that the model is expecting with that I can just pass the comment into the prompt template and generate a new prompt what that looks like is this so you see we have the instruction special tokens you see it’s well formatted this is the instructions please respond to the following comment and says great comment thank you Now using this new prompt instead of just passing the comment directly to the model we have this set of instructions with the comment this is the response thank you(27:36) for your kind words I’m glad you found the content helpful sha GPT so this is really good this is actually already pretty close to how I typically respond to YouTube comments and a lot of them tend to be something like this and it appropriately signed off as Sha GPT so people know that it came from an AI and not from me personally well maybe we could just call it here it’s like okay this is good enough let’s just start using this as the comment responder let’s see how we can use Q Laura to improve this model even further using(28:05) fine tuning so the way to do that is we need to prepare the model for training so we’ll put it from eval mode into training mode we’re going to enable gradient checkpointing which isn’t something I talked about and it’s not necessarily part of the qora technique because it’s actually pretty standard it’s just a memory saving technique that clears specific activations and then recomputes them during the backward path of the model and then we need to enable quantized training the base model is(28:32) going to be in 4 bit and we’re going to freeze them but we still want to do training in higher Precision with Lowa we need to make sure we enable this quantize training option next we want to set up lowra so we can use that using this low ra config file I talk more about low RA in the fine-tuning video so just briefly going through this we’re going to set the rank as 8 set the alpha s32 we’re going to Target the query modules in the model we’re going to set drop out to 0.5 we’re not going to have(29:01) any bias values and then we’re going to set the task as causal language modeling with the config file we can pass the model and the config into this method get PFT model so this will just create a lowr trainable version of the model and then we can print the number of trainable parameters so doing that we see that we actually have a significant saving so less than 1% of the original number of trainable parameters just one point of confusion for me personally is it’s showing that mistol 7B instruct has(29:33) 264 million parameters here based on the quick research I did seemed like when you do quantization there could be some terms that you can drop but honestly I don’t fully understand why we went from 7 billion parameters to just 264 million parameters so if anyone knows that please drop it in the comments I’m very curious but the main point here is that we’re only using 0.(29:58) 8% of the original number of train parameters so huge memory savings using lowon next we’re going to load the data set which is freely available on the hugging face Hub it’s called shot GPT YouTube comments also the code to generate this data set is available at the GitHub repo if you’re curious on how to do the formatting and stuff and then here’s an example from this data set you’ll see we have the special token the start string and the end string we have the start instruction and end instruction and then we have the same(30:26) set of instructions as before and then we have the comment here which is a real comment from the YouTube channel then after the instruction string we have the actual response I left to this comment and then I just appended this Shaw GPT sign off so the model learns the appropriate format in style that it should respond to we’ve got a data set of 59 of these examples so not a huge data set at all and then next we need to pre-process the text so this is very similar to how I did it in the previous find tuning video basically we Define(30:59) this tokenized function which if the example is too long so if it’s longer than 52 tokens it’s going to truncate it so it’s not more than this max length and then we’ll return it as numpy values and then we can apply this tokenized function to every single example in the data set using this method here the map method where we have our data set and then we just pass in the tokenized function and set batched equal to true so it doesn’t batches I guess instead of doing it one by one the other thing we(31:32) need to do is this handles if the examples are too long but when you’re training the model each example in a batch they actually need to be the same size so you can actually do matrix multiplication so for that we can create a data cator what that does is if you have multiple examples of different lengths so let’s say you have like four examples in a batch and they’re all of different lengths the data cator will dynamically pad each example example so they have the same length for that we need to define a pad token which I set(32:02) as the end of string token and then I create the data collator using this method here and then I think this is masked language modeling set equal to false and that’s because we’re doing so-called causal language modeling not masked language modeling now we’re ready to start setting up the training process so here we’re setting hyperparameters we have the learning rate batch size number of epoch we’re setting the output directory of the model the learning rate the batch size goes here number epochs(32:28) goes here weight Decay we set it as 0.01 for logging evaluation and save strategy we set it to every Epoch that means every Epoch will print the training loss we’ll evaluate at every Epoch we’ll also print the validation data set loss and then save strategy so we’ll save the model every Epoch in case something goes wrong we’re going to load the best model at the end because maybe the best model was actually at the eighth Epoch and it got worse on the ninth Epoch or something like that gradient(32:54) accumulation is equal to four warm-up steps equal to two so I actually talk a lot about gradient accumulation and weight decay in the previous video on training a large language model from scratch so if you’re curious about what’s going on there you can check out those videos next we’ll set fp16 equal to true so here we’re going to use 16bit values for training and then we’ll enable the paged Optimizer by setting this optim equal to paged atom W 8bit so this is ingredient three from before(33:26) lots of hyper parameters and of course you can spend your whole life tuning and tweaking this but once we have that we can run the training job so we initialize our trainer we give it the model give it our training data set our validation data set training arguments we defined on the previous slide and then the data collator we’re going to silence warnings this is what I saw on an example from hugging face when they were introducing bits and bites so I just did it again here and then we can run the training process this took about(33:53) 10 minutes to run on Google collab so it’s actually pretty quick and this is this is what will get printed the training loss and validation loss so we can see a smooth monotonic decrease of both implying stable training which is good and then once it’s all said and done we have our model and we can use it so if we pass in that same test comment great content thank you we get the response glad you enjoyed it shot GPT and then it even adds this disclaimer that note I am an AI language model I don’t have the ability to feel emotions(34:22) or watch videos I’m here to answer questions and provide explanations so this is good I feel like this is exactly how I would respond to this comment if I wanted to remove the disclaimer I could easily do that with some like string manipulation just keeping all the text before the sign off or something like that but the point is that the fine-tuning process at least from this one example seemed to work pretty nicely let’s try a different comment something more technical like what is fat tailedness the response of the model is(34:50) actually similar to what we saw in the previous video when we fine-tuned open AI model and then we asked it the same question where where it gives a good concise explanation of fat tailedness the only issue is it doesn’t explain fat tailedness the same way that I explained it in my video series on the topic so this brings up one of the limitations of fine-tuning which is that it’s great for capturing style but it’s not always an optimal way to incorporate specialized knowledge into model responses which(35:22) brings us to what’s next instead of trying to give the model even more examples trying to include this specialized knowledge a simpler approach is that we can improve the model’s responses to these types of technical questions by providing it specialized domain knowledge the way we can do that is using a so-called rag system which stands for retrieval augmented generation right now we just get the comment and we pass it into the model with the appropriate prompt and it spits out a response the difference with a rag(35:53) system is that we take the comment we use the comment to extract ra relevant information from a knowledge base and then we incorporate that into the prompt that we pass into the model so that it can generate a response so that’s going to be the focus of the next video in this series we’re going to see how we can improve shot GPT using specialized knowledge coming from my medium blog articles and speaking of medium blog articles if you enjoy this video but you want to learn more check out the article(36:22) published in towards data science on Cur there I cover details that I might have missed in this video here and even though this is a member only story you can access it completely for free using the friend Link in the description below other things I’ll point out is that the code example is available for free on collab there’s more code available on the GitHub and then again the model and the data set are available on hugging face and as always thank you so much for your time and thanks for watching" }, { "title": "3 Ways to Build your Own LLMs", "url": "/posts/3-ways-build-own-llms/", "categories": "AI, LLM", "tags": "llm, build, ways", "date": "2024-09-07 07:10:25 -0400", "snippet": "(00:00) one of the most common asks I get from clients is how do I build a custom AI chatbot well a few months ago this was something you needed to hire a consultant for today that’s not necessaril...", "content": "(00:00) one of the most common asks I get from clients is how do I build a custom AI chatbot well a few months ago this was something you needed to hire a consultant for today that’s not necessarily the case in this video I’ll walk through three ways to make a custom AI assistant using a few new releases from open AI I’ll review a no code solution via their new gpts feature a python solution via the new assistance API and then finally talk about how to fine-tune an assistant with your data and if you’re new here welcome I’m sha I(00:33) make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that’s a great no cost way you can support me in all the videos that I make before jumping into the solutions I wanted to quickly differentiate a AI chatbot from a AI assistant while these terms might be used interchangeably the way I like to think about it is that a chatbot is an AI you can have a conversation with while an AI assistant is essentially a chatbot that can use tools a tool could be web browsing it(01:02) could be a calculator it could be a python interpreter or anything else that helps augment the abilities of a chatbot for example if you have the free version of chat GPT that’s a chatbot because it comes with a basic chat functionality without any additional tools however if you have the premium version of chat GPT that’s an assistant because it has built-in tools like web browsing and document retrieval turning chatbots into assistance via tools is a powerful way to to expand its capabilities and Effectiveness each of the approaches I(01:34) talk about in this video can be used to develop an AI assistant and I’ll use each of these approaches to build an AI that can respond to my YouTube comments in my likeness which I’ll call Shaw GPT the first way I’m going to talk about is the easiest and fastest of the three and it consists of using open ai’s new gpts feature so the way to get to that is we go to chat GPT and if you have the premium version version you can go to this Explorer GPT thing here and then you can go look at my gpts and so you(02:07) can make these custom gpts and they’ll appear here so I’ve already made a version of this Shaw GPT but we’ll go ahead and create a new GPT so we can see what this looks like from scratch the thing about this gpts feature is it actually has a chat interface to help you build the AI assistance so not only is this a no code solution it has a step bystep chatbot that will help you build this Ai and so here the GPT Builder says hi I’ll help you build a new GPT you can say something like make a creative who(02:40) helps generate visuals for my new product or make software engineer who helps format my code what would you like to make make a YouTube comment responder let’s see what that spits out so you can see that GPT Builder is thinking what’s happening is you say this to GPT Builder and then behind the scenes it’s going to go ahead and update the instructions for the AI assistant so if you go to the configure tab it’ll start filling in these things it’ll film the description for the assistant and it’ll give it its(03:11) instructions but let’s go back how about the name shot GPT for this GPT does that work that’s perfect okay so now it’s going to generate a profile picture so it’ll use Dolly 2 which is if you’re not familiar that’s a different AI that instead of generating text like chat GPT it generates images like this one here so that’s a cool thing about this gbt Builder it’ll generate a profile picture for you you can also just upload your own so I’ve got a folder here called sha GPT and I’ve got some different images(03:40) that we can use so I’ll just upload this one and then I’ll tell GPT Builder to use this image instead so it’s going to use the picture that I uploaded okay so now it’s asking for a bit more guidance on how to answer questions outside the expertise in data science and Entrepreneurship so a lot of questions that I get on YouTube aren’t actually about data science or entrepreneur preneurship it’s usually people just saying great content or something like that just sharing feedback about the(04:05) video so I’ll provide those instructions here okay so here I wrote out instructions for both positive and negative feedback so for positive feedback unrelated to data science Express gratitude for their viewership suggestions or positive feedback given negative feedback thank the viewer rephrase their criticism to convey not convert convey understanding and offer an idea for improvement so these are just some general rules of thumb of how to respond to comments how do you envision sha GPT addressing comments(04:35) that ask for personal advice on Career changes to Day signs or starting a new business okay so that’s a good question I didn’t see this last time I made sha GPT through this interface basically when people ask me these questions I tell them that there’s no way to just give general advice to anyone without knowing more details and ultimately I only know what’s worked for me and I don’t necessarily know what’s the best choice for other people and that’s ultimately up to them but I’m always(04:59) happy to kind of share my personal experience and what has worked for me so basically going to try to put that into words so that GPT Builder can instruct the model accordingly so the way I tried to convey that is acknowledge that advice for their unique situation is hard to give without a full understanding of their situation however sha GPT can share Shaw’s personal experience in case it is helpful for the viewer okay here we go sha GPT will now take the approach that when addressing personal advice inquiries okay so for(05:28) the Final Touch what tone of L language do you prefer shbg use more formal and professional or casual and friendly casual and friendly and concise I don’t like reading a lot of text and so I try not to put that burden on other people so I like to keep text very concise okay sweet feel free to try out shot yout in the preview and see how it responds if there’s anything else you’d like to address what would it be okay so we can now give it a whirl how do I start in data science so it gives a very long(05:56) response doesn’t feel very concise starting data science is exciting here’s a quick guide to getting you going learn the basics explore online courses Hands-On practice understanding data manipulation diving into machine learning build a portfolio Network and learn continuously so it’s casual it’s friendly but it’s not very concise and doesn’t really sound like me the response sounds a lot like what chat GPT would say so if someone reads this comment or response from me they’ll be(06:19) like did sha just copy paste my comment into chat GPT and paste the response so one thing we can do to remove that ambiguity is we can have sh GPT sign off with its name just so people know that it was generated by an AI assistant and not by me personally we can add this to the GPT Builder Okay so it updated the instructions so now it should sign up as Shaw GPT so here’s another one can you explain machine learning in simple terms so it gave a pretty good response I like this example just learning by example(06:51) and experiences it signed off with sha GPT which is good again however the response is way too long I would never respond like this but it’s good that it identifies itself I want to move over to this configure tab so I guess some weird things even though it said it was going to use the name sha GPT that never made it over it’s still I guess in beta they’re still working out some Kinks if anything isn’t what you want from this chat interface you can always move over to the configure Tab and just set these(07:18) things manually so you can set the name of the assistant you can set the description this was autogenerated friendly and informative responder for a data science and Entrepreneurship YouTube channel and then these are the instructions that it generated through the conversation we were having sha GPT is the Casual friendly and concise voice behind a YouTube channel on data science and Entrepreneurship it acknowledges the complexity of giving personalized advice offering insights from Shaw’s experiences instead positive feedback is(07:47) met with gratitude and criticism is handled constructively sha gbt always signs off its messages with its name sha gbt so we can see like a lot of the aspects of the conversation were incorporated into to the instructions another cool thing here is you can set these conversation starters which is nice for the UI you can just put like frequently asked questions for your AI assistant here another really cool thing about gpts is that it has builtin retrieval also called retrieval augmented generation so basically what(08:17) this means is you can equip your assistant with specialized knowledge that lives in like a PDF or like a Word document or whatever so you can upload those files and the assistant will be able to read those files and incorporate the appropriate context as needed so for example I have a bunch of my articles here so I could upload four ways to quantify fat taals with python I can upload that PDF and then I can ask it a question what is a fat tail it didn’t use the PDF in this case it just gave a standard definition of fat Tails as(08:53) something more extreme than a normal distribution which is a common way of defining bat Tales however it’s not the definition of fat tails that I used in that article so another thing we could try is like how can we quantify fat tails so even though it didn’t use the definition of fat tailedness from that article it did seamlessly grab the four ways to quantify fat taals from the article the way this is working is that GPT 4 or whatever GPT is underlying these custom gpts here it has been trained to know when to seek outside(09:27) knowledge if it doesn’t know something and so so here it does it pretty seamlessly it feels like it read the article and is like rephrasing it in its own words which is pretty interesting okay so that’s knowledge you can upload I think up to like 10 files or something actually probably more the next thing I want to point out are these capabilities so this is what makes these gpts assistant and not just chat Bots it’s because it has these extended capabilities such as web browsing image generation and code interpreter so web(09:54) browsing is exactly what you expect if the user says hey look up something or search the web for some resources or whatever the assistant knows when to call upon this web browsing capability to do that if the user asks the assistant to generate an image it knows to use Dolly and the code interpreter allows the assistant to not only write python code because that’s what a chatbot can do but actually run that python code and then return the values to the user or use the values from that computation to inform the response to(10:28) the user so since for this YouTube responder use case we don’t need image generation because you can’t have images on YouTube probably don’t need a code interpreter either web browsing could be cool to provide additional resources and of course I could upload all these other articles as well so we can just do that to give it more knowledge so we can’t bulk upload PDFs at this point but I can go through one by one and upload all of these articles to have it have a bit more specialized knowledge about the(10:56) actual content that is on the channel the last thing are these actions which are pretty interesting so essentially this allows gpts to make API calls so we can learn more here how to create a GPT okay so this answers one question you can have 20 files uploaded that answers that okay here custom actions you can make third party API calls available to your GPT so that’s pretty cool like the canonical case here is like if you want to grab weather data so if you want to get weather from a particular City you(11:25) can endow the assistant with that capability to look up the weather in a particular City not necessarily for this YouTube comment responder I think here just knowledge and the web browsing is sufficient and so what’s cool about these is you can save these gpts and make it available to everyone so you don’t have to worry about like deploying your assistant into production because they have this built-in way to do it so you can make it published only to yourself anyone with the link or everyone in the world and then you can(11:52) select a category so that’s pretty cool it got the category right we can uh confirm but I won’t because I’ve already published a version of sha GPT and I spent a bit more time making that one so I don’t want to put this example version out there but if you want to interact with sha GPT I’ll put a link in the description and you can kind of see for yourself the performance I guess another thing is people say like you can become a millionaire making a GPT or something and I don’t know maybe based on my(12:20) interaction with this interface I don’t know how that would work however looking at the other gpts this doesn’t feel so much as like a direct way to generate Revenue however it feels like a very big opportunity for these companies or really any company to do promotion and lead generation for their business so like canva being number one on here it’s like that’s amazing advertising cuz whatever it is they get 100 million daily users of chat GPT that’s a lot of eyeballs seeing canva basically for free(12:49) feels more like an opportunity for these companies to promote their business so like Ali Academy I don’t know who that is but I can just like go to her website look at that geni Innovation for Creative so look at that of course this might be a way to generate revenue or whatever but to me it feels more an opportunity for advertising a business or something like that as opposed to direct monetization so just my thoughts on becoming a millionaire with custom gpts okay so when it’s all done and published so this is publicly available(13:21) data scientist GPT for YouTube comments and so we can have these things ready to go so people can ask their data science questions even though it has the Articles available to it that I wrote it doesn’t explain it in the way that I explained it in the Articles it kind of gives a more traditional explanation of these things so that’s kind of one downside for these custom gpts it still feels a lot like chat GPT but just with slightly different wrapping paper and of course if I spent a lot more time with(13:51) it I could build something that doesn’t sound so much like Chad GPT but the second obvious downside of this is it requires a pre premium version of chat GPT and some people just like a hobbyist or something or you’re like a student it may not make sense to pay $2 a month for this and of course this isn’t something you can easily integrate into like an app that you make or a website for people to use this they have to come to the chat GPT website so if you don’t have premium or you want to incorporate(14:21) this assistant into some application or some website we can turn to the assistance API which is way number two I’m going to talk about so if you don’t have a premium account there’s still a no code solution for building a AI assistant and that’s through the assistance playground this is a relatively new feature so the playground’s been around for a while this is the original version the complete playgrounds you can pick whatever model you like here and it’ll just do the auto regressive text(14:50) generation it’ll just keep predicting the next word recursively chat is similar where you can set the model and whatever parameters you like and set the system message and just do like a chat however assistance takes things to the next level where it’s a lot like the gpts configure tab that we saw earlier where you can have a name instructions models and then you can add tools to the chat bot so we can really just copy paste all this over so we have sha GPT here are your instructions we can select the model so we can have GPT for Turbo(15:21) we can add functions so we can write custom functions here it has to be in a particular format shown here so you need to give it a name for the function you need to describe it in words and then you need to provide the inputs for the function so what information does the assistant need to pass to the function in order to generate an output also has the code interpreter so it can not only write python code it can run the python code receive a response and then it also has retrieval like we saw with the custom GPT the one thing missing here is(15:53) web browsing natively I think that’s something that they’re going to add later but as of right now it’s January 30th 2024 web browsing is not integrated into the assistance playground or assistance API but we do have functions code interpreter retrieval and we can also add files like we did before let’s see if we can do bulk file upload here aha we can there we go bulk file upload available in the playground okay so we can just see how this works we can do like a side by-side comparison so we’ll(16:23) ask sha GPT what is fat tailedness and we’ll ask sha GPT playground what is fat tailedness okay so again it’s kind of giving the more traditional definition of something fat tailed has heavier Tails compared to a normal distribution so the response is very similar to what we see over here in the gpt’s case so while using the playground to make an assistant in this way seems like a free hack a free version of the custom gpts it has one serious limitation which is that if you only use the playground your assistant(16:55) is trapped in the playground it’s not like the GPT T’s interface we saw earlier where you can with one click deploy the assistant onto the chat GPT website so if you want to release the assistant from this playground you want to put it into a website an app or whatever you’re going to need to do some coding so let’s do that okay so here we got a jupit notebook this is available in the GitHub repository Linked In the description below so if you want this code feel free to grab it there kind of(17:21) walking through this just importing some modules of course we’re going to use the open AI assistance API which is still in b so that’s why I put that there you’ll need to import your secret key and if you don’t have a secret key or don’t know how to get your secret key for the open AI API I have a video all about that I’ll link it in the description below maybe pop it up on the screen here so you can check that out but you can get a key in a very straightforward way you don’t need to be premium user if you(17:50) just have an open AI account you can get an API key and if you’re new you get some free credits to start what I have is a text file called sk. and I’m in importing the secret key from that text file that’s the way I like to do it and also importing time for a helper function we’ll see in a second here the first thing we need to do is to set up the client so set up communication with the opening eye API so you just do that oneline in code I make a quick helper function because when we’re using these(18:17) assistants they have to think so it takes time for us to get a response from the API so what this function does is weights and periodically grabs the status of the API call and then when it’s done it’ll move on to the next step of the code and it’ll tell us how long it took to do that so that’s just like a helper function it’s not super important this is kind of what we were doing before in the playground and in the gpt’s chat interface but now we’re going to do it in Python the first step is(18:46) defining our instructions so this is essentially the system message so I’m using the same thing we had in the playground here sha GPT functioning as a virtual data science consultant on YouTube blah blah blah blah blah just copy paste that here and then we can create the assistant using this chunk of code here the way this works is we created this client object earlier and then we’re going to be accessing the assistance API and we’re just going to use this create method what that allows us to do is create an assistant set its(19:16) name its description its instructions and the model that we want to use notice that this is everything we said here the name description isn’t here but you can set the description via the assistance API the instructions and the model so it’s all the same and then I just print the assistant not that it gives us a whole lot of useful information but it’s just this object that looks like this okay in order to talk to the assistant you can set up this thread object which is new in the assistance API compared to(19:46) the chat completion API which is what I talked about in a previous video we can create this thread which is just like a back and forth conversation it helps avoid doing this boiler plate code of keeping track of what the system message is and what the user says and what the assistant says and what the user says and assistant says so on and so forth It’s all handled by the thread so that’s super convenient you can generate the user message so here I just put something like great content thank you so that’s a comment and then we can add(20:13) this comment to the thread using this so we have to specify which thread we’re going to add the message to which role is saying the message and then the content of the message we get the thread idea from here role is user and then content is here then finally we can send the message to the assistant to generate a response the way that works is you use this method here so we’re in the threads module the runs subm module and we’re going to use this create method to create a run which is just an API call(20:47) essentially so we’ll need to provide the thread ID which we just added this user message to and we need to provide the assistant ID and so where this comes from is the assistant object we made ear earlier if we do that we can run that and then it’ll take a few seconds to run so that’s why I run this helper function I made and then once the run is complete we can view that returned object so it took about 5 Seconds to do that first one so actually we can just like run this whole thing I think that’s fine(21:14) yeah we’ll just run this whole thing all right so it took less time this time so it took about 3 seconds to run so automatically when we do this run when the assistant generates the response it gets automatically added to the thread so we don’t have to do anything extra so all we can do is grab the messages from this thread that was just updated by the assistant and we can print the most recent message in the thread so the response from the assistant is you’re welcome I’m glad you found it helpful if(21:42) you have any more questions or topics you’re curious about feel free to ask shot GPZ it signed off correctly which is nice and it is a pretty positive response it captures the sentiment that I convey it’s just super long I never talk like this again I don’t like reading and I don’t want to burden people people with just putting way too much text for them to read I try to keep my responses pretty concise then here I just delete the assistant because what happens is every time you make an assistant it’ll pop up here and if(22:10) you’re running this notebook as just like a tutorial or you’re running it multiple times debugging or something you’re going to have a very long list of assistants here if you don’t delete them so that’s what this line of code’s doing okay so we have this problem that it’s giving nice responses but these are just way too long they don’t really sound like me one thing we can do to get sha GPT to sound more like me is to give it some examples to learn from so that’s what I do here through so-called fuse(22:34) shot prompting it’s essentially where you put a set of examples in the instructions for the assistant so these are real past comments and my responses to these comments and all I did was append this sha GPT to them so it has that format we’re looking for and then we just do the same thing we create another system in the same way we name it shot GPT we give it a description we give it the new set of instructions which is is what we defined here and then we Define the model then we can create a new thread so we can talk to(23:03) the assistant we’ll use the same user message great content thank you we’ll add the message to the thread so again specifying the thread ID the role of who’s saying it and the message then we’ll run the thread so we’ll send it to the assistant to generate a response by passing in the thread ID and assistant ID and then we’ll wait a few seconds for the assistant to generate a response so it took about 3 seconds and then here we’re just printing the assistance response so this is what sha GPT says(23:31) now with the updated instructions to the same exact comment which is you’re welcome happy to hear you found it useful shot GPT so that’s a lot better I would probably have said just something like glad it was helpful happy to hear you found it useful maybe just one of these to make it a bit more concise cuz you’re just saying the same thing in more words but this is a lot better than the original and a lot better than what we were seeing in the playground with the no code Solutions but of course we(23:58) could have have added the few shot examples in the no code solution and we would have gotten the same results so another thing is technical questions which Chad GPT might respond in a certain way but it may not be the same way that I would respond to that technical question so before we tried adding PDF versions of my article so we could get the context of how I talk about fat tailedness for example but it for whatever reason it wasn’t capturing that but now we’ve done this few shot prompting so let’s see how its response(24:26) changes we’re just doing the same exact thing as before we’re creating a new thread we’re generating a user message so a new thread is as if we opened a new chat GPT chat so it’s not going to remember this previous message that’s why we’re making the new thread add user message to thread so exactly what we did before but now the user message is what is fat tailedness and then we’re going to send the message to the assistant we actually don’t need this here I haven’t gone through and cleaned up this code(24:51) okay and then we wait for the assistant to process the prompt it takes a bit longer now presumably because it’s a longer response it’s not just like glad you liked it it’s explaining what fat tailedness is and in fact when we print the results this is what we get fat tailedness refers to property probability distribution which tails are fatter than those of a normal distribution okay so it’s just giving the same thing as we saw before but notice that we haven’t added any documents like we did in the no code(25:19) Solutions so let’s do that I quickly deleted the assistant so we don’t have them piling up here’s how we can add documents to the assistant so the way that works is like this so we go to the files module and just do this create method so we just use this syntax open which is just opening a file so it’s creating a python readable version of the file so we’re going to open the file we specify the path which is in this articles folder this is just setting that we’ll be opening this file for(25:51) reading purposes as opposed to writing to it we also need to specify the purpose of this file so we’ll say it’s for assistant the other option is for fine-tuning which we’ll see here in a little bit once we create this file it’ll actually populate here but I think it actually got deleted at the end of this notebook so if you were to just run this chunk of Code by itself it’s going to create a file in your files tab on your open AI account and actually any assistant that you make can use these(26:21) files for retrieval all you have to do is provide the file ID as we’ll see in a second here but notice where all these came from is when we were in the playground and we uploaded all 10 of these articles with the file uploaded we can create a new assistant that can access that particular file so the way that works is we created this file object and now we can just pass it to the assistant so this is the same exact method we were using before to generate the assistant we said the name the description the instructions but now(26:51) we’re going to define the tools so here we’re going to use retrieval as a tool and when we do that we also need to specify the file IDs of the documents that the assistant can use so in this case we’re just going to use this one that we created up here but of course if you have a ton of files uploaded here you can use all of them and you just need to specify all of the file IDs in this list here and then finally you specify the model so now that we added retrieval let’s try the technical question once again so this one took a(27:20) lot longer to run took about 45 seconds but we’re running it in the same way this is interesting it generated kind of like a mixed response something close to what we were seeing before where it’s kind of giving a traditional definition of fat tailedness it’s just more fat tailed than a normal distribution but then it’s bringing in Concepts from the articles that I provided which is you know fat tailedness ranges from thin tailed to very fat tailed which is almost verbatim from the article and(27:47) then it adds these four different ways to quantify F tailedness and then it kind of gives some context these methods offer different insights into distributions shape allowing for more detailed understanding of the data’s Behavior especially regarding the occurrences and impact of rare and extreme events sha GPT yeah that’s pretty interesting that it added in those four heris and gave this dichotomy of thin tailed versus fat tailedness which is something I talk about in the article and then finally I delete the(28:15) assistant and delete the file if you’re running this multiple times you don’t accumulate a bunch of files and assistants on your account and then some more resources I’ll put all these in the description you can read more about the assistance AP API just from like a high level the documentation which is the API reference breaks down the python side of things and then finally more On Tools so here we just used retrieval but there are two other tools that you can use which are the code interpreter which we(28:44) saw in both the playground and the gpts and the functions tool which is something we saw in the assistants playground so just going back to that we only used retrieval here but we could have also used the code interpreter and functions so if you want to read more about that I’ll provide this link in the description kind of talks about the tools talks about the code interpreter which as it says here allows assistance API to write and run python code so that’s on the code interpreter reading images and files okay that’s cool and(29:13) also had the input output logs that’s cool knowledge retrieval which is what we did in all the different examples I talked about here and then function calling so this is the cool thing and you can almost have any function with this capability so the way this works is you need to give the name of the function along with a natural language description and then you need to also provide a so-called Json schema which essentially outlines what the inputs of the function are so the assistant knows what information to give to this(29:41) function in order to get a response at this point we’ve seen both no code and python ways of generating an assistant and we saw that we can get pretty far by using things like retrieval and fuse shot prompting to improve the assistant performance however there’s still still something missing from the assistant responses it just doesn’t really feel like something I would say it’s very verbose and doesn’t quite explain things how I would explain it so to kind of better capture this feel aspect of the(30:11) assistant responses let’s turn to fine-tuning the model unlike the assistance API there’s no no code solution for the fine-tuning bit you have to write some python code to fine-tune the model and it requires a bit more work because in order to do fine tuning you need to curate a training data set so here I’m going to kind of walk through the process of how I did that for this YouTube comment responder use case here’s another Jupiter notebook example code is on the GitHub repository so check it out if you(30:42) like here we’re importing some modules again importing the open AI python Library importing the secret key in the same way that we did in the previous example and then here importing a few other libraries that we’re going to use for the data preparation so again we’re going to set up our client and then we’re going to need to prepare the training data so this is what makes fine tuning a lot of work it’s just acquiring the right data and a important rule in machine learning if there are any rules(31:11) in machine learning is that data quality is the most important thing not how fancy your model is not how efficient your code is but how good your data is at capturing the thing that you’re trying to model and so in this case what I did was I went to my YouTube channel and I took real comments that people left and then real comments that I responded with to create this input output pair so what that looks like is this so I got about 60 comments and responses just copy pasted into this numbers sheet I use numbers because I’m(31:46) on Mac but you could use Excel to just copy paste it in there and then I exported it as a CSV file just to see some examples of this comment was this was a very thorough introduction to llms and answer answered many questions I had thank you to which I said great to hear glad it was helpful smiley face and of course we can have emojis in here this one says Mr Moneybags over here which was funny so these are just real world comments which we incorporate into F shop prompting but you just can’t fit 60 comments and responses into your(32:17) assistant instructions and if you do it’s just going to create this like bulky overhead because the instructions are going to be passed to the assistant every time a thread is initiated so you can imagine that the API calls are just going to become more costly over the long run if you just have a really large instruction set and so that’s where fine tuning is helpful in fact open AI has a nice guide on fine tuning here we go common use cases so I’ll also put this in the description some common use cases(32:46) when fine-tuning can improve results setting the style tone format and other qualitative aspects so that’s the main motivation for using it in this use case another one’s improving reliability at producing the desired output next is correcting failures to follow complex prompts handling many edge cases in specific ways so I guess that’s good if there’s like a specific prompts or specific situations where the assistant is failing you can just include those in your training data set and it’ll learn(33:11) how to not do that and then performing a new skill or task that’s hard to articulate in a prompt and so I like how they put it here the high Lev way to think about it is you want to find tune when it’s easier to show not tell kind of like how I was experiencing in the initial no code GPT solution it was asking me these questions and I wasn’t necessarily sure how to give the best instructions that’s another downside of these no code Solutions is that even though it doesn’t require us to write(33:39) python code it may not be obvious how to give good instructions using natural language showing not telling is just a better way to convey the desired behavior of the assistant going back to the training data prep we have all these comments and responses in the CSV file how however the fine-tuning API doesn’t take CSV files as input the data need to be in a very specific format which they talk about here data preparation and Analysis for chat model fine tuning and basically it wants the examples the inputs and outputs in a Json L format so(34:17) basically it wants it in a text file here’s an example it wants it in this format where each row is an example and it consists of a system message a user message and then an assistant message and each line is in the Json format which you can essentially think of as a python dictionary what we need to do is take the CSV file from here and translate it into the proper format in order to use it for fine tuning the way that works here is I initialize these lists to hold all the comments and all the responses then I open the CSV file(34:52) this is a CSV file YouTube comments. CSV in read mode this like a common syntax tax for reading text files in Python not entirely sure how it actually works but it’s something I’ve used a lot so maybe I should figure out how it works but anyway my understanding is that it handles the opening and closing of the text file but if someone knows better than me drop it in the comments below I’m curious so the first thing we do is use this CSV do reader method to read the file and then what that allows us to(35:21) do is go through the CSV file line by line and just grab each chunk of text so we’re doing that in this for Loop and then we’re actually going to skip the first line cuz as you can see here the first line says comment and response so what I’m doing is if the first element of the line is equal to comment to just skip that line that’s how I’m skipping the first line here but this will probably be different if your first line isn’t comment and response with that first line out of the way the for Loop(35:48) will go to the next line which is this comment and this response and then what I’m going to do is append the comment to the comment list and then append the response to the response list but also appending this Shaw GPT sign off to the response so we get that desired behavior and of course you can do whatever string manipulation you like here to ensure that the output format of the assistant is whatever you like so here I just have like a simple thing that I’m adding to it but if you want it to be in like a(36:20) Json format or you want it to have like a particular format you can do whatever string manipulation you like to ensure that it has that format so this will just go all the way through the CSV file just grabbing each comment and each response so we do that it’ll save all the comments to the comment list so that’s what this is and then it’ll have all my responses here or should I say all of sha gpt’s responses here and so that’s what that looks like you can see emojis and all okay so all we did right(36:50) now is just put all the comments and responses from the CSV file in a list we haven’t actually made it into this Json L format so in order to do that we go to this next cell where we Define our instruction string and we generate each example line by line so the way that works is again going back to this example here each example is a dictionary so it’s this key value pair where the key is always messages for whatever reason and then the value is a list which is this list here however the list is a list of dictionaries so it(37:23) contains three dictionaries the first one corresponding to the system message the second dictionary corresponding to the user’s message and the third dictionary corresponding to the assistance response that’s the goal that’s what we’re trying to do here and so the way I do it is Define the system message just once then I just go like index by index through the comment list we go 0 to 58 through the comment list because there are 59 elements and we just generate each of these three dictionaries we generate the system(37:55) message we generate the user message and then we generate the assistant response and so that’s what we’re doing here so roll system content is always going to be this instructions string then we have roll user the content is going to be the I element of the comment list and then we have the assistant role and its content is the I element of the response list okay and then we just take these three dictionaries put them into a list and then we create a dictionary I kind of do two things in one step here so(38:29) sorry if it’s unclear but we take this list and create a dictionary with the key is messages and the value is this messages list and then we append this dictionary to the example list so it’s matching this format here where each line of this text file is a Json format so it’s essentially a dictionary and in the same way each element of this example list is a dictionary okay so there was a issue with with this line of code which I just fixed but what we’re doing here is we’re going to create our(39:01) train test split we’re going to designate a set of examples for training and then we’re going to designate another set of examples for the validation data set and so the way I do that is I randomly generate nine integers between zero and the number of examples minus one and then I use those indices to create a new list for the validation data set so this is just creating a list here where it’s going to go through each index in this index list here and it’s going to copy that example from example list and put it into Data(39:34) validation list and since these examples will still be an example list we can go through and iteratively remove them in this for Loop so now at this stage we’ve got two lists one is called example list which is a list of dictionaries that have the data in the proper format this should have 50 elements it does and then we have have validation data list which is just all the examples in our validation data set and so this is also a list of dictionaries and then we write these examples to two different files so(40:08) the first one is for the training data and the second one is for the validation data and so kind of in a similar way as before we’re opening this text file specifically a Json L file with the right flag on and then we’ll just go through each element of example list and dump it into this Json file and then we’ll do a new line character so it creates a new line for each example and then we’ll do the same exact thing for the validation data set and so once we do that it should create these files(40:37) here training data. Json L validation data. Json L and then we can upload these files to the open aai API for fine-tuning the way you do that is very similar to what we saw in the previous notebook when we’re uploading the article for rag purposes for retrieval purposes so we just use this files. create method meod we open the file which we just saved but now we set the purpose as fine-tune as opposed to assistance do that for both the training and validation data I just put that there so it didn’t run the job(41:08) prematurely and then we can just run the fine-tuning job so the way it works here is we specify the training file the validation file ID which is just a property of these two objects we can set a suffix which is basically some unique identifier or some identifier we can put into the name of the fine-tuning job or name of the model so we can identify it and then we can specify what model we want to use so gp4 is not available for fine tuning the most state-of-the-art advanced model for fine tuning available is gbt 3.5 turbo so that’s what we’re(41:42) going to use here and then we can just run this and it’ll create this fine-tuning job and then if we go back to our open AI account click on this fine-tuning tab we can see this fine tuning job is running and so this actually kind of takes 15 or 20 minutes it’s not something that runs like immediately fast but I did this yesterday so we can just use this model it’s kind of like the cooking shows like we already cooked the pasta last night and we’re going to eat it in front of you and so yeah it’s already set up here(42:09) so fine tuning jobs running once it finishes running after like 20 minutes or so we’ll be able to use it another thing about fine-tuning is that your fine-tune models don’t integrate into the assistance API they only work with the chat completions API which means those tools that we could include like retrieval like the code interpreter like the functions which we could just easily add to our system by specifying some keyword argument that’s not available natively through the chat completions API so if you want to add rag you want(42:39) to add tools you got to build out that functionality yourself using python code and of course you can use like Lang chain or llama index or some python library to do that but it’s not integrated into chat completions I’m sure at some point the fine-tuning models will be incorporated to the assistance API but at this point it’s not available okay so we’re going to throw into test comment so we specify the model and then we have to define the messages like this so we Define the system message using the instruction(43:06) string from before then we pass the user comment just like that then we can run that and then Sean GPT just says thanks it’s more concise I’ll say that for sure and I don’t know if I’ve ever just said thanks to a comment but this is a pretty brief response let’s see what another response looks like and so this is a comment that came in recently it wasn’t in the training data set so let’s see what the response to this looks like so the comment was I’m typing this after watching half of the video as I’m(43:34) already amazed with the clarity of explanation exceptional and then sha GPT says wow thanks for the compliment that’s pretty good that that is something that I would write I was playing around with this yesterday and it gave like a pretty cheeky answer I don’t know if it’ll generate that again but I found that funny it was something like thanks for commenting I hope the second half of the video is good and it had some emojis in there and I thought it was funny but now it’s like generating responses that I would say oh(43:59) yeah here we go this is something similar to what I saw yesterday so appreciate the compliment hope the second half doesn’t disappoint so now let’s see how it handles a technical question again the fine tune model doesn’t have retrieval built into it if I wanted to add retrieval to this model I need to use like llama index or Lang chain or something like that to add it in there so let’s see how it handles this of course it’s not going to describe fat tilted how I did in the articles but it is giving a much more(44:25) concise and closer response to what I would say compared to what we were seeing before like this monstrosity without the fine tuning there you go that’s how you do fine tuning it takes a little bit more effort up front to get together your training data set but honestly it took me about maybe 20 to 30 minutes to manually copy paste those comments into the CSV file maybe another like 20 minutes to write this script to prepare the training data but you don’t have to write the script you can just bring in your CSV file and it should be(44:53) ready to go and then again I’ll delete the training in valid ation files hopefully that doesn’t ruin the fine-tuning job but it doesn’t really matter and then more resources here I’ll drop these in the description below so you can take a closer look at it so open AI guide for fine-tuning that was pretty helpful all of open ai’s documentation is super clean and easy to understand let’s see they have their API reference on fine tuning and then how to prepare your data for fine tuning is also(45:19) available here so overall impressions of fine tuning I’m honestly pretty impressed I didn’t expect it to work as well as it did especially because it only used 50 training examples in grad school if I wanted to train a good model I needed 100,000 examples to train a good predictive model but because of the power of GPT 3.(45:40) 5 turbo it’s already a powerful model even fine tuning with just 50 examples results in really good results that’s why personally I feel like fine-tuning is probably the biggest Innovation that we’ve seen in machine learning because if you can find the right base model for your use case and you can just tweak it with just a handful of examples you can get really good results in a fraction of the time that it would have taken you to develop that model from scratch okay so that’s basically it we talked about three different ways to build a custom AI(46:10) assistant using open AI we first talked about the no code solution which allowed you to build an assistant pretty quickly without any python coding next we looked at the assistance API which gave us a pretty straightforward way of creating assistance using python code which we can take and Port over to an application or a website and then finally we saw how we can fine tune an assistant to dramatically change its style and really Come Away with something that has good performance if you want to play around with the no Cod shot GPT I’ll provide a(46:38) link in the description below but if you want to interact with the fine tuned version of sha GPT drop a comment in the comment section below and I’ll be sure to respond with shot GPT unless you explicitly don’t want me to respond with shbt just let me know and I’ll respond as myself so I hope this tutorial was helpful and gave you an idea of which approach might be best for your particular use case of building an AI assistant while open AI does currently have the state of the art large language models for building these things a(47:07) natural question is how can we build these types of AI assistants using open-source Solutions so that’s exactly what I’m going to cover in future videos of this series where we’re going to explore both retrieval augmented generation or rag and model fine-tuning using open-source models and if you enjoyed this content this is just one video in a much larger series on using large language models in practice so if you want to learn more check out the series playlist Linked In the description below and in the comment(47:36) section and as always thank you so much for your time and thanks for watching" }, { "title": "Build an LLM from scratch", "url": "/posts/build-llm-from-scratch/", "categories": "AI, LLM", "tags": "ai, build, llm, scratch", "date": "2024-09-07 06:44:46 -0400", "snippet": "(00:00) hey everyone I’m sha and this is the sixth video in the larger series on how to use large language models in practice in this video I’m going to review key aspects and considerations for bu...", "content": "(00:00) hey everyone I’m sha and this is the sixth video in the larger series on how to use large language models in practice in this video I’m going to review key aspects and considerations for building a large language model from scratch if you Googled this topic even just one year ago you’d probably see something very different than we see today building large language models was a very esoteric and specialized activity reserved mainly for Cutting Edge AI research but today if you Google how to build an llm from scratch or should I(00:35) build a large language model you’ll see a much different story with all the excitement surrounding large language models post chat GPT we now have an environment where a lot of businesses and Enterprises and other organizations have an interest in building these models perhaps one of the most notable examples comes from Bloomberg in Bloomberg GPT which is a large language model that was specifically built to handle tasks in the space of Finance however the way I see it building a large language model from scratch is(01:08) often not necessary for the vast majority of llm use cases using something like prompt engineering or fine-tuning in existing model is going to be much better suited than building a large language model from scratch with that being said it is valuable to better understand what it takes to build one of these models from scratch and when it might make sense to do it before diving into the technical aspects of building a large language model let’s do some back the napkin math to get a sense of the financial costs that we’re talking about(01:41) here taking as a baseline llama 2 the relatively recent large language model put out by meta these were the computational costs associated with the 7 billion parameter version and 70 billion parameter versions of the model so you can see for llama 27b it took about 180,000 th000 GPU hours to train that model while for 70b a model 10 times as large it required 10 times as much compute so this required 1.(02:10) 7 million GPU hours so if we just do what physicists love to do we can just take orders of magnitude and based on the Llama 2 numbers we’ll say a 10 billion parameter model takes on the order of 100,000 GPU hours to train while 100 billion parameter model takes about a million GPU hours to train so how can we trans at this into a dollar amount here we have two options option one is we can rent the gpus and compute that we need to train our model via any of the big cloud providers out there a Nvidia a100 what was used to train llama 2 is going(02:46) to be on the order of $1 to $2 per GPU per hour so just doing some simple multiplication here that means the 10 billion parameter model is going to be on the order of1 15 $50,000 just to train and the 100 billion parameter model will be on the order of $1.5 million to train alternatively instead of renting the compute you can always buy the hardware in that case we just have to take into consideration the price of these gpus so let’s say an a100 is about $110,000 and you want to form a GPU cluster which is about 1,000 gpus(03:24) the hardware costs alone are going to be on the order of like $10 million but that’s not the only cost when you’re running a cluster like this for weeks it consumes a tremendous amount of energy and so you also have to take into account the energy cost so let’s say training a 100 billion parameter model consumes about 1,000 megawatt hours of energy and let’s just say the price of energy is about $100 per megawatt hour then that means the marginal cost of training a 100 billion parameter model(03:53) is going to be on the order of $100,000 okay so now that you’ve realized you probably won’t be training a large language model anytime soon or maybe you are I don’t know let’s dive into the technical aspects of building one of these models I’m going to break the process down into four steps one is data curation two is the model architecture three is training the model at scale and four is evaluating the model okay so starting with data curation I would assert that this is the most important(04:23) and perhaps most time consuming part of the process and this comes from the basic principle of machine learning of garbage in garbage out put another way the quality of your model is driven by the quality of your data so it’s super important that you get the training data right especially if you’re going to be investing millions of dollars in this model but this presents a problem large language models require large training data sets and so just to get a sense of this gpt3 was trained on half a trillion(04:53) tokens llama 2 was trained on two trillion tokens and the more recent Falcon 180b was trained on 3.5 trillion tokens and if you’re not familiar with tokens you can check out the previous video in the series where I talk more about what tokens are and why they’re important but here we can say that as far as training data go we’re talking about a trillion words of text or in other words about a million novels or a billion news articles so we’re talking about a tremendous amount of data going through a trillion words of text and(05:26) ensuring data quality is a tremendous effort and undertaking and so a natural question is where do we even get all this text the most common place is the internet the internet consist of web pages Wikipedia forums books scientific articles code bases you name it post J GPT there’s a lot more controversy around this and copyright laws the risk with web scraping yourself is that you might grab data that you’re not supposed to grab or you don’t have the rights to grab and then using it in a model for(05:57) potentially commercial use could come back and cause some trouble down the line alternatively there are many public data sets out there one of the most popular is common crawl which is a huge Corpus of text from the internet and then there are some more refined versions such as colossal clean crawled Corpus also called C4 there’s also Falcon refined web which was used to train Falcon 180b mentioned on the previous slide another popular data set is the pile which tries to bring together a wide variety of diverse data(06:30) sources into the training data set which we’ll talk a bit more about in the next slide and then we have hugging face which has really emerged as a big player in the generative Ai and large language model space who houses a ton of Open Access Data sources on their platform another place are private data sources so a great example of this is fin pile which was used to train Bloomberg GPD and the key upside of private data sources is you own the rights to it and and it’s data that no one else has which(07:02) can give you a strategic Advantage if you’re trying to build a model for some business application or for some other application where there’s some competition or environment of other players that are also making their own large language models finally and perhaps the most interesting is using an llm to generate the training data a notable example of this comes from the alpaca model put out by researchers at Stanford and what they did was they trained an llm alpaca using structured text generated by gpt3 this is my(07:38) cartoon version of it you pass on the prompt make me training data into your large language model and it spits out the training data for you turning to the point of data set diversity that I mentioned briefly with the pile one aspect of a good training data set seems to be data set diversity and the idea here is that a diverse data set translates to to a model that can perform well in a wide variety of tasks essentially it translates into a good general purpose model here I’ve listed out a few different models and the(08:11) composition of their training data sets so you can see gpt3 is mainly web pages but also some books you see gopher is also mainly web pages but they got more books and then they also have some code in there llama is mainly web pages but they also have books code and scientific articles and then Palm is mainly built on conversational data but then you see it’s trained on web pages books and code how you curate your training data set is going to drive the types of tasks the large language model will be good at and(08:44) while we’re far away from an exact science or theory of this particular data set composition translates to this type of model or like adding an additional 3% code in your trading data set will have this quantifiable outcome in the downstream model while we’re far away from that diversity does seem to be an important consideration when making your training data sets another thing that’s important to ask ourselves is how do we prepare the data again the quality of our model is driven by the quality of(09:14) our data so one needs to be thoughtful with the text that they use to generate a large language model and here I’m going to talk about four key data preparation steps the first is quality filtering this is removing text which is not helpful to the large language model this could be just a bunch of random gibberish from some corner of the internet this could be toxic language or hate speech found on some Forum this could be things that are objectively false like 2 + 2al 5 which you’ll see in the book 1984 while that text exists out(09:48) there it is not a true statement there’s a really nice paper it’s called survey of large language models I think and in that paper they distinguish two types of quality filtering the first is classifier based and this this is where you take a small highquality data set and use it to train a text classification model that allows you to automatically score text as either good or bad low quality or high quality so that precludes the need for a human to read a trillion words of text to assess its quality it can kind of be offloaded(10:21) to this classifier the other type of approach they Define is heuristic based this is using various rules of thumb to filter the text text this could be removing specific words like explicit text this could be if a word repeats more than two times in a sentence you remove it or using various statistical properties of the text to do the filtering and of course you can do a combination of the two you can use the classifier based method to distill down your data set and then on top of that you can do some heuristics or vice versa(10:52) you can use heuristics to distill down the data set and then apply your classifier there’s no one- siiz fits-all recipe for doing quality filter in rather there’s a menu of many different options and approaches that one can take next is D duplication this is removing several instances of the same or very similar text and the reason this is important is that duplicate texts can bias the model and disrupt training namely if you have some web page that exists on two different domains one ends up in the training data set one ends up(11:24) in the testing data set this causes some trouble trying to get a fair assessment of model performance during training another key step is privacy redaction especially for text grab from the internet it might include sensitive or confidential information it’s important to remove this text because if sensitive information makes its way into the training data set it could be inadvertently learned by the language model and be exposed in unexpected ways finally we have the tokenization step which is essentially translating text(11:55) into numbers and the reason this is important is because neural networks do not understand text directly they understand numbers so anytime you feed something into a neural network it needs to come in numerical form while there are many ways to do this mapping one of the most popular ways is via the bite pair encoding algorithm which essentially takes a corpus of text and deres from it an efficient subword vocabulary it figures out the best choice of subwords or character sequences to define a vocabulary from(12:29) which the entire Corpus can be represented for example maybe the word efficient gets mapped to a integer and exists in the vocabulary maybe sub with a dash gets mapped to its own integer word gets mapped to its own integer vocab gets mapped to its own integer and UL gets mapped to its own integer so this string of text here efficient subword vocabulary might be translated into five tokens each with their own numerical representation so one two three four five there are python libraries out there that implement this(13:03) algorithm so you don’t have to do it from scratch namely there’s the sentence piece python Library there’s also the tokenizer library coming from hugging face here the citation numbers and I provide the link in the description and comment section below moving on to step two model architecture so in this step we need to define the architecture of the language model and as far as large language models go Transformers have emerged merged as the state-of-the-art architecture and a Transformer is a neural network architecture that(13:34) strictly uses attention mechanisms to map inputs to outputs so you might ask what is an attention mechanism and here I Define it as something that learns dependencies between different elements of a sequence based on position and content this is based on the intuition that when you’re talking about language the context matters and so let’s look at a couple examples so if we see the sentence I hit the base baseball with a bat the appearance of baseball implies that bat is probably a baseball bat and not a nocturnal mammal this is the(14:09) picture that we have in our minds this is an example of the content of the context of the word bat so bat exists in this larger context of this sentence and the content is the words making up this context the the content of the context drives what word is going to come next and the meaning of this word here but content isn’t enough the positioning of these words is also important so to see that consider another example I hit the bat with a baseball now there’s a bit more ambiguity of what bat means it(14:46) could still mean a baseball bat but people don’t really hit baseball bats with baseballs they hit baseballs with baseball bats one might reasonably think bad here means the nocturnal mammal and so an attention mechanism captures both these aspects of language more specifically it will use both the content of the sequence and the positions of each element in the sequence to help infer what the next word should be well at first it might seem that Transformers are a constrained in particular architecture we actually(15:19) have an incredible amount of freedom and choices we can make as developers making a Transformer model so at a high level there are actually three types of Transformers which follows from the two modules that exist in the Transformer architecture namely we have the encoder and decoder so we can have an encoder by itself that can be the architecture we can have a decoder by itself that’s another architecture and then we can have the encoder and decoder working together and that’s the third type of Transformer so let’s take a look at(15:51) these One By One The encoder only Transformer translates tokens into a semantically mean meaningful representation and these are typically good for Tech classification tasks or if you’re just trying to generate a embedding for some text next we have the decoder only Transformer which is similar to an encoder because it translates text into a semantically meaningful internal representation but decoders are trying to predict the next word they’re trying to predict future tokens and for this decoders do not(16:25) allow self attention with future elements which makes it great for text generation tasks and so just to get a bit more intuition of the difference between the encoder self attention mechanism and the decoder self attention mechanism the encoder any part of the sequence can interact with any other part of the sequence if we were to zoom in on the weight matrices that are generating these internal representations in the encoder you’ll see that none of the weights are zero on the other hand for a decoder it uses(16:58) so-called masked self attention so any weights that would connect a token to a token in the future is going to be set to zero it doesn’t make sense for the decoder to see into the future if it’s trying to predict the future that would kind of be like cheating and then finally we can combine the encoder and decoder together to create another choice of model architecture this was actually the original design of the Transformer model kind of what’s depicted here and so what you can do with the encoder decoder model that you(17:29) can’t do with the others is the so-called cross attention so instead of just being restricted to self attention with the encoder or mask self attention with the decoder the encoder decoder model allows for cross attention where the embeddings from the encoder so this will generate a sequence and the internal embeddings of the decoder which will be another sequence will have this attention weight Matrix so that the encoders representations can communicate with the decoder representations and this tends to be good for tasks such as(18:00) translation which was the original application of this Transformers model while we do have three options to choose from when it comes to making a Transformer the most popular by far is this decoder only architecture where you’re only using this part of the Transformer to do the language modeling and this is also called causal language modeling which basically means given a sequence of text you want to predict future text Beyond just this highlevel choice of model architecture there are actually a lot of other design choices(18:32) and details that one needs to take into consideration first is the use of residual connections which are just Connections in your model architecture that allow intermediate training values to bypass various hidden layers and so to make this more concrete this is from reference number 18 Linked In the description and comment section below what this looks like is you have some input and instead of strictly feeding the input into your hidden layer which is this stack of things here you allow it to go to both the hidden layer and to(19:03) bypass the hidden layer then you can aggregate the original input and the output of the Hidden layer in some way to generate the input for the next layer and of course there are many different ways one can do this with all the different details that can go into a hidden layer you can have the input and the output of the Hidden layer be added together and then have an activation applied to the addition you can have the input and the output of the Hidden layer be added and then you can do some kind of normalization and then you can add(19:32) the activation or you can have the original input and the output of the Hidden layer just be added together you really have a tremendous amount of flexibility and design Choice when it comes to these residual Connections in the original Transformers architecture the way they did it was something similar to this where the input bypasses this multiheaded attention layer and is added and normalized with the output of this multi attention layer and then the same thing happens for this layer same thing happens for this layer same thing(20:03) happens for this layer and same thing happens for this layer next is layer normalization which is rescaling values between layers based on their mean and standard deviation and so when it comes to layer normalization there are two considerations that we can make one is where you normalize so there are generally two options here you can normalize before the layer also called pre-layer normalization or you can normalize after the layer also called post layer normalization another consideration is how you normalize one(20:34) of the most common ways is via layer norm and this is the equation here this is your input X you subtract the mean of the input and then you divide it by the variance plus some noise term then you multiply it by some gain factor and then you can have some bias term as well an alternative to this is the root mean Square Norm or RMS Norm which is very similar it just doesn’t have the mean term in the numerator and then it replaces this denominator with just the RMS while you have a few different options on how you do layer(21:06) normalization the most common based on that survey of large language models I mentioned earlier reference number eight pre-layer normalization seems to be most common combined with this vanilla layer Norm approach next we have activation functions and these are non-linear functions that we can include in the model which in principle allow it to capture comp Lex mappings between inputs and outputs here there are several common choices for large language models namely gelu relo swish swish Glu G Glu and I’m sure there are more but glus(21:42) seem to be the most common for large language models another design Choice Is How We Do position embeddings position embeddings capture information about token positions the way that this was done in the original Transformers paper was using these sign and cosine basic functions which added a unique value to each token position to represent its position and you can see in the original Transformers architecture you had your tokenized input and the positional encodings were just added to the tokenized input for both the encoder(22:16) input and the decoder input more recently there’s this idea of relative positional encodings so instead of just adding some fixed positional encoding before the input is passed into the model the idea with relative positional encodings is to bake positional encodings into the attention mechanism and so I won’t dive into the details of that here but I will provide this reference self attention with relative position representations also citation number 20 the last consideration that I’ll talk about when it comes to model(22:48) architecture is how big do I make it and the reason this is important is because if a model is too big or train too long it can overfit on the other hand if a model is too small or not trained long enough it can underperform and these are both in the context of the training data and so there’s this relationship between the number of parameters the number of computations or training time and the size of the training data set there’s a nice paper by Hoffman at all where they do an analysis of optimal compute(23:21) considerations when it comes to large language models I’ve just grabbed a table from that paper that summarizes their key findings what this is saying is that a 400 million parameter model should undergo on the order of let’s say like 2 to the 19 floating Point operations and have a training data consisting of 8 billion tokens and then a parameter with 1 billion models should have 10 times as many floating Point operations and be trained on 20 billion parameters and so on and so forth my kind of summarization takeaway from this(23:54) is that you should have about 20 tokens per model mod parameter it’s not going to be very precise but might be a good rule of thumb and then we have for every 10x increase in model parameters there’s about a 100x increase in floating Point operations so if you’re curious about this check out the paper Linked In the description below even if this isn’t an optimal approach in all cases it may be a good starting place and rule of thumb for training these models so now we come to step three which is training these(24:23) models at scale so again the central challenge of these large language models is is their scale when you’re training on trillions of tokens and you’re talking about billions tens of billions hundreds of billions of parameters there’s a lot of computational cost associated with these things and it is basically impossible to train one of these models without employing some computational tricks and techniques to speed up the training process here I’m going to talk about three popular training techniques the first is mixed(24:53) Precision training which is essentially when you use both 32bit and 16 bit floating Point numbers during model training such that you use the 16bit floating Point numbers whenever possible and 32bit numbers only when you have to more on mixed Precision training in that survey of large language models and then there’s also a nice documentation by Nvidia linked below next is this approach of 3D parallelism which is actually the combination of three different parallelization strategies which are all listed here and I’ll just(25:27) go through them one by one first is pipeline parallelism which is Distributing the Transformer layers across multiple gpus and it actually does an additional optimization where it puts adjacent layers on the same GPU to reduce the amount of cross GPU communication that has to take place the next is model parallelism which basically decomposes The Matrix multiplications that make up the model into smaller Matrix multiplies and then distributes those Matrix multiplies across multiple gpus and then and then finally there’s data parallelism which(26:00) distributes training data across multiple gpus but one of the challenges with parallelization is that redundancies start to emerge because model parameters and Optimizer States need to be copied across multiple gpus so you’re having some portion of the gpu’s precious memory devoted to storing information that’s copied in multiple places this is where zero redundancy Optimizer or zero is helpful which essentially reduces data redundancy regarding the optimizer State the gradient and parameter partitioning and(26:32) so this was just like a surface level survey of these three training techniques these techniques and many more are implemented by the deepe speed python library and of course deep speed isn’t the only Library out there there are a few other ones such as colossal AI Alpa and some more which I talk about in the blog associated with this video another consideration when training these massive models is training stability and it turns out there are a few things that we can do to help ensure that the training process goes smoothly(27:03) the first is checkpointing which takes a snapshot of model artifacts so training can resume from that point this is helpful because let’s say you’re training loss is going down it’s great but then you just have this spike in loss after training for a week and it just blows up training and you don’t know what happened checkpointing allows you to go back to when everything was okay and debug what could have gone wrong and maybe make some adjustments to the learning rate or other hyperparameters so that you can try to(27:31) avoid that spike in the loss function that came up later another strategy is weight Decay which is essentially a regularization strategy that penalizes large parameter values I’ve seen two ways of doing this one is either by adding a term to the objective function which is like regular regularization regular regularization or changing the parameter update Rule and then finally we have gradient clipping which rescales the gradient of the objective function if it exceeds a pre-specified value so this helps avoid the exploding gradient(28:03) problem which may blow up your training process and then the last thing I want to talk about when it comes to training are hyperparameters while these aren’t specific to large language models my goal here is to just lay out some common choices when it comes to these values so first we have batch size which can be either static or dynamic and if it’s static batch sizes are usually pretty big so on the order of like 16 million tokens but it can also be dynamic for example in GPT 3 what they did is they(28:29) gradually increased the batch size from 32,000 tokens to 3.2 million tokens next we have the learning rate and so this can also be static or dynamic but it seems that Dynamic learning rates are much more common for these models a common strategy seems to go as follows you have a learning rate that increases linearly until reaching some specified maximum value and then it’ll reduce via a cosine Decay until the learning rate is about 10% % of its max value next we have the optimizer atom or atom based optimizers are most commonly used for(29:04) large language models and then finally we have Dropout typical values for Dropout are between 0.2 and 0.5 from the original Dropout paper by Hinton at all finally step four is model evaluation so just cuz you’ve trained your model and you’ve spent millions of dollars and weeks of your time if not more it’s still not over typically when you have a model in hand that’s really just the starting place in many ways next you got to see what this thing actually does how it works in the context of the desired(29:34) use case the desired application of it this is where model evaluation becomes important for this there are many Benchmark data sets out there here I’m going to restrict the discussion to the open llm leaderboard which is a public llm Benchmark that is continually updated with new models un hugging faces models platform and the four benchmarks that is used in the open El M leaderboard are Arc H swag MML and truthful QA while these are only four of many possible Benchmark data sets the evaluation strategies that we can use(30:11) for these Benchmark data sets can easily port to other benchmarks so first I want to start with just Arc helis swagen MML U which are multiple choice tasks so a bit more about these Ark and MML U are essentially great school questions on subjects like math math history common knowledge you know whatever and it’ll be like a question with a multiple choice response A B C or D so an example is which technology was developed most recently a a cell phone B a microwave c a refrigerator and D an airplane H swag(30:44) is a little bit different these are specifically questions that computers tend to struggle with so an example of this is in the blog associated with this video which goes like this a woman is outside with a bucket ET and a dog the dog is running around trying to avoid a bath she dot dot dot a rinses the bucket off with soap and blow dries the dog’s head B uses a hose to keep it from getting soapy C gets the dog wet then it runs away again D gets into a bathtub with a dog and so this is a very strange question but intuitively humans tend to(31:20) do very well on these tasks and computers do not so while these are multiple choice tasks and we might think it should be pretty straight forward to evaluate model performance on them there is one hiccup namely these large language models are typically text generation models so they’ll take some input text and they’ll output more text they’re not classifiers they don’t generate responses like ABC or D or class one class 2 class 3 class 4 they just generate text completions and so you have to do a little trick to get(31:51) these large language models to perform multiple choice tasks and this is essentially through prompt templates for example if you have the question which technology was developed most recently instead of just passing in this question and the choices to the large language model and hopefully it figures out to do a BC or D you can use a prompt template like this and additionally prend the prompt template with a few shot examples so the language model will pick up that I should return just a single token that is one of these four tokens here so if(32:26) you pass this into to the model you’ll get a distribution of probabilities for each possible token and what you can do then is just evaluate of all the tens of thousands of tokens that are possible you just pick the four tokens associated with a B C or D and see which one is most likely and you take that to be the predicted answer from the large language model while there is this like extra step of creating a prompt template you can still evaluate a large language model on these multiple choice tasks and(32:57) in a relatively straightforward way however this is a bit more tricky when you have open-ended tasks such as for truthful QA for truthful QA or other open-ended tasks where there isn’t a specific one right answer but rather a wide range of possible right answers there are a few different evaluation strategies we can take the first is human evaluation so a person scores the completion based on some ground truth some guidelines or both while this is the most labor int ensive this may provide the highest quality assessment(33:30) of model completions another strategy is we could use NLP metrics so this is trying to quantify the completion quality using metrics such as perplexity blue score row score Etc so just using the statistical properties of the completion as a way to quantify its quality while this is a lot less labor intensive it’s not always clear what the mapping between a completions statistical properties is to the quality of that that completion and then the third approach which might capture The Best of Both Worlds is to use an(34:03) auxiliary fine-tuned model to rate the quality of the completions and this was actually used in the truthful QA paper should be reference 30 where they created an auxiliary model called GPT judge which would take model completions and classify it as either truthful or not truthful and then that would help reduce the burden of human evaluation when evaluating model outputs okay so what’s next so you’ve created your large language model from scratch what do you do next often this isn’t the end of the(34:38) story as the name base models might suggest base models are typically a starting point not the final solution they are really just a starting place for you to build something more practical on top of and there are generally two directions here one is via prompt engineering and prompt engineering is just feeding things into the language model and harvesting their completions for some particular use case another Direction one can go is via model fine-tuning which is where you take the pre-trained model and you adapt(35:11) it for a particular use case prompt engineering and model fine tuning both have their pros and cons to them if you want to learn more check out the previous two videos of this series where I do a deep dive into each of these approaches if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please drop those in the comment section below and as always thank you so much for your time and thanks for watching" }, { "title": "Estimated cost associated with memory and GPU devices while considering the RTX 4090's pricing and capabilities.", "url": "/posts/budget-own-llm-build/", "categories": "AI, LLM", "tags": "build, llm, budget, estimation", "date": "2024-09-07 04:00:24 -0400", "snippet": "Building a large language model (LLM) with a focus on using lower-cost GPUs, such as the NVIDIA RTX 4090, involves significant computational resources. Below is a detailed breakdown of the estimate...", "content": "Building a large language model (LLM) with a focus on using lower-cost GPUs, such as the NVIDIA RTX 4090, involves significant computational resources. Below is a detailed breakdown of the estimated costs associated with memory and GPU devices while considering the RTX 4090’s pricing and capabilities.1. Understanding Model Parameters and Memory RequirementsMemory RequirementsThe memory required for training a model is primarily determined by the number of parameters and the precision used to store them. Here’s a general guideline: Full Precision (float32): Each parameter requires 4 bytes. Half Precision (float16): Each parameter requires 2 bytes.To calculate the total memory requirement for a model, you can use the formula:\\[\\text{Memory (in GB)} = \\frac{\\text{Number of Parameters} \\times \\text{Bytes per Parameter}}{1,073,741,824}\\]For example, for a 1 billion parameter model in full precision:\\(\\text{Memory} = \\frac{1,000,000,000 \\times 4}{1,073,741,824} \\approx 3.73 \\text{ GB}\\)2. GPU Options: NVIDIA RTX 4090NVIDIA RTX 4090 Specifications Memory: 24 GB GDDR6X Price: Approximately $1,600 - $2,000 (depending on the retailer and model)The RTX 4090 is a powerful GPU that can handle substantial workloads, making it suitable for training LLMs.Example Calculation for Model Training1 Billion Parameters Memory Requirement: Full Precision: \\(1,000,000,000 \\times 4 \\text{ bytes} = 4 \\text{ GB}\\) GPU Selection: A single NVIDIA RTX 4090 (24 GB) can easily handle this model. Total GPU Cost: Approximately $1,600. 7 Billion Parameters Memory Requirement: Full Precision: \\(7,000,000,000 \\times 4 \\text{ bytes} = 28 \\text{ GB}\\) GPU Selection: A single NVIDIA RTX 4090 can accommodate this model as well. Total GPU Cost: Approximately $1,600. 175 Billion Parameters Memory Requirement: Full Precision: \\(175,000,000,000 \\times 4 \\text{ bytes} = 700 \\text{ GB}\\) GPU Selection: This model size exceeds the memory capacity of a single RTX 4090, requiring multiple GPUs. You would need at least 30 RTX 4090 GPUs to accommodate this model, costing approximately $48,000. 3. Estimating Training Time and CostsTraining TimeTraining time can vary widely based on several factors, including the architecture of the model, the efficiency of the training code, and the number of epochs. A rough estimate for training time can be derived from the following: Batch Size: Larger batch sizes can speed up training but require more memory. Epochs: The total number of passes through the training dataset.For example, if you have: A dataset with 1 million samples A batch size of 256 10 epochsThe number of iterations would be:\\(\\text{Iterations} = \\frac{1,000,000}{256} \\times 10 \\approx 39,062 \\text{ iterations}\\)If each iteration takes approximately 0.1 seconds on an RTX 4090, the total training time would be:\\(\\text{Total Time} = 39,062 \\times 0.1 \\approx 3,906 \\text{ seconds} \\approx 1.08 \\text{ hours}\\)4. Total Cost EstimationTo summarize the total costs associated with building an LLM using the NVIDIA RTX 4090: 1 Billion Parameters: GPU Cost: Approximately $1,600 for 1 RTX 4090. Total Memory Cost: Minimal additional costs for memory since it fits on one GPU. 7 Billion Parameters: GPU Cost: Approximately $1,600 for 1 RTX 4090. Total Memory Cost: Still manageable within the budget. 175 Billion Parameters: GPU Cost: Approximately $48,000 for 30 RTX 4090 GPUs. Total Memory Cost: Additional costs for high-capacity storage and memory may apply, but the primary cost is the GPUs. GPU PricesHere is a table with additional GPU devices like the RTX 4080 and A100: GPU Memory Memory Type Memory Bus Width Memory Bandwidth CUDA Cores Tensor Cores RT Cores Base Clock Boost Clock TDP Price RTX 4090 24 GB GDDR6X 384-bit 1,008 GB/s 16,384 512 128 2.23 GHz 2.52 GHz 450W $1,599 RTX 4080 16 GB GDDR6X 256-bit 716 GB/s 9,728 304 76 2.21 GHz 2.51 GHz 320W $1,199 RTX 4070 Ti 12 GB GDDR6X 192-bit 504 GB/s 7,680 240 60 2.31 GHz 2.61 GHz 285W $799 RTX 4070 12 GB GDDR6 192-bit 504 GB/s 5,888 184 46 1.92 GHz 2.46 GHz 200W $599 RTX 4060 Ti 8 GB GDDR6 128-bit 288 GB/s 4,352 136 34 1.87 GHz 2.37 GHz 200W $399 RTX 4060 8 GB GDDR6 128-bit 288 GB/s 3,584 112 28 1.82 GHz 2.42 GHz 170W $299 RX 7900 XTX 24 GB GDDR6 384-bit 960 GB/s 6,144 - 96 1.86 GHz 2.3 GHz 355W $999 RX 7900 XT 20 GB GDDR6 320-bit 800 GB/s 5,376 - 84 1.86 GHz 2.25 GHz 300W $899 RX 7800 XT 16 GB GDDR6 256-bit 560 GB/s 4,096 - 64 1.81 GHz 2.2 GHz 255W $599 A100 40 GB HBM2 5120-bit 1.6 TB/s 6,912 432 - 1.41 GHz 1.71 GHz 400W $10,000 This table includes the latest NVIDIA RTX 40-series and AMD RX 7000-series GPUs, as well as the NVIDIA A100 for comparison. Key specifications like memory size, type, bus width, bandwidth, CUDA/Tensor/RT cores, clock speeds, TDP, and pricing are provided for each model.The RTX 4090 leads with its impressive 24 GB of GDDR6X memory and 1 TB/s of bandwidth, along with the highest core counts. The RTX 4080 and 4070 Ti offer great performance at lower prices. AMD’s RX 7900 XTX and XT compete well with the RTX 4080 and 4070 Ti respectively.The NVIDIA A100 is included as a high-end data center GPU with its massive 40 GB HBM2 memory and 1.6 TB/s bandwidth, but at a much higher price point of $10,000.This table allows for easy comparison of the key specifications and pricing across the latest desktop and data center GPUs from NVIDIA and AMD. The information is sourced from official product pages and reviews.ConclusionBuilding an LLM from scratch using the NVIDIA RTX 4090 is feasible for smaller models, such as those with 1 billion or 7 billion parameters, with a total cost of approximately $1,600. However, for larger models (e.g., 175 billion parameters), the costs escalate significantly, requiring a substantial investment in multiple high-performance GPUs. Understanding these requirements will help you plan the computational resources needed for your LLM project effectively." }, { "title": "Prompt Engineering", "url": "/posts/prompt-engineering/", "categories": "AI, LLM", "tags": "prompt", "date": "2024-09-06 22:53:16 -0400", "snippet": "(00:00) hey everyone I’m Shaw and this is the fourth video in the larger series on using large language models in practice today I’m going to be talking about prompt engineering and now before all ...", "content": "(00:00) hey everyone I’m Shaw and this is the fourth video in the larger series on using large language models in practice today I’m going to be talking about prompt engineering and now before all the technical folks come after me with their pitchforks let’s just address the elephant in the room so if you’re a technical person like Tony Stark here you might be rolling your eyes at the idea of prompt engineering you might say prompt engineering is not engineering or prompt engineering is way overhyped or(00:29) even prompt engineering is just a complete waste of time and when I first heard about the concept I had a similar attitude it didn’t seem like something worth my time I was more concerned with the model development side of things like how can I fine-tune a large language model but after spending more time with it my perspective on prompt engineering has changed my goal with this dog is to give a sober and practical overview of prompt engineering and the technical people out there who are rolling their eyes like this version(00:56) of Tony Stark maybe by the end of this you’ll be more like this version of Tony Stark oh wow imprompt engineering will be another tool in your AI data science and software development Arsenal so since this is kind of a long session I apologize in advance first I’ll talk about what is prompt engineering then I’ll talk about two different levels of problem engineering what I call the easy way and the less easy way next we’re going to talk about how you can build AI apps with prompt engineering then I’ll(01:39) talk about seven tricks for prompt engineering and then finally we will walk through a concrete example of how to create an automatic grader using Python and Lang chain what is prompt engineering the way I like to Define it is it’s any use of an llm out of the box but there’s a lot more that can be said about prompt engineering here are a few comments on prompt engineering that have stood out to me the first comes from the paper by white at all which defines prompt engineering as the means by which llms are programmed with prompts and(02:13) this raises this idea that prompt engineering is a new way to program computers and this was something that was really eye-opening for me when I first saw tragedy PT and heard this idea of prompt engineering it felt like oh this is just like a chat bot kind of thing but as I dove deeper into it and I read this paper and consumed other resources out there the deeper picture here is that large language models provide a path to making programming and computation as easy as asking a computer what you want in natural language(02:47) another definition comes from the paper by Hugh at all which defines prompt engineering as an empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task the reason this one stood out to me is because it highlights this aspect of prompt engineering then it’s at this point it’s not really a science it’s a collection of heuristics and people throwing things against the wall and accidentally stumbling across techniques and then through that messy process it seems like some tricks and(03:18) heuristics are starting to emerge and this might be part of the reason why people are so put off by prompt engineering because it doesn’t seem like a serious science and that’s because it’s not a serious science it’s still way too early in this new paradigm of large language models that we’re operating in it’s going to take a while for us to understand what these models are actually doing why they actually work and I think with that we’ll have a better understanding of how to manipulate them how to throw stuff at(03:45) them and get desired results out and the final comment that I really liked about prompt engineering comes from Andre carpathy in his state of GPT talk from Microsoft build 2023 where he said language models want to complete documents and so you can trick them into performing tasks just by arranging fake documents I feel like this captures the essence of prompt engineering language models are not explicitly trained to do the vast majority of tasks we ask them to do all these language model want to do is to predict the next token and then(04:17) predict the next one and the next one and the next one and so I love this concept of tricking the AI into solving your problems and that’s essentially all prompt engineering is constructing some text that generates the desired outcome from the large language model and so the way I like to think about it is that there are two levels of prompt engineering the first level is what I call the easy way which is essentially chat GPT or something similar so now Google has barred out there Microsoft has Bing chat all these different(04:44) applications provide a very user-friendly and intuitive interface for interacting with these large language models and so while this is the easiest and cheapest way to interact with large language models it is a bit restrictive in that you can’t really use chat GPT to build an app maybe it’ll help you write some code but you can’t integrate chat gbt into some piece of software or some larger application that you want to build out that’s where the less easy way come comes in the less easy way is to interact with these large(05:16) language models programmatically and so you could use Python for this you could use JavaScript or whatever programming language the key upside of the less easy way here is that you can fully customize how a large language model fits into a larger piece of software this in many ways unlocks a new paradigm for programming and software development and that brings us to building AI apps with prompt engineering like I just said the less easy way unlocks a new paradigm of software development and to demonstrate this let’s just look at a specific use(05:52) case suppose we wanted to make an automatic grader for a high school history class and while this might be easy enough if the questions are multiple choice or true false this becomes a bit more difficult when the answers are short form or even long form text responses and so an example of this is as follows consider the question who was the 35th president of of the United States well you might think that there’s only one answer John F Kennedy there are many answers that are reasonable and could be considered correct and so(06:22) here’s a list of a few examples so there’s John F Kennedy but JFK a very common abbreviation of his name could also be considered correct there’s also Jack Kennedy which is a common nickname used for JFK there’s John Fitzgerald Kennedy which is his full name and someone probably trying to get extra credit and then there’s John F Kennedy where the student may have just forgotten to put one of the ends in his last name let’s see how we can go about making a piece of software that can do(06:48) this grading process automatically first we have the traditional Paradigm this is how programming has always been done and here it’s on the developer to figure out the logic to handle all the variations and all the edge cases this is the hard part of programming it’s like writing a robust piece of software that can handle all the different edge cases so this might require the user to input a list of all possible correct answers and then that might be hard you know with homework with a bunch of questions you(07:15) can’t anticipate every possible answer that a student is going to write down and traditionally if you’re trying to evaluate texts against some like Target text you probably would be using some kind of like exact or fuzzy string matching algorithm but now let’s look at this new paradigm where we can incorporate large language models into the logic of our software and here you can use an olm to handle all the logic of this automatic grading task using prompt engineering instead of coming in with some code that does exact matching(07:47) or fuzzy matching and figuring out the logic that gives you the desired outcome you could just write a prompt and so what this might look like is you write the prompt you are a high school history teacher grading homework assignments based on the homework question indicated by q and the correct answer indicated by a your task is to determine whether the student’s answer is correct grading is binary therefore student answers can be correct or wrong simple misspellings are okay then we have this template here(08:14) where we have q and a as indicated by The Prompt and these curly brackets are indicating where a question is going to be placed in and where the single correct answer is going to be placed in and then we also have a place for the student answer all this can be fed to a large language model and the language model will generate a completion that says the student answer is correct or the student answer is wrong and maybe it’ll give some reasoning behind why this student answer is wrong taking a step back and comparing these two(08:44) approaches to this problem approach one was to manually sit down think and write out a string matching algorithm that tried to handle all the different edge cases and variations of potentially correct answers I’m an okay programmer at best so it would probably take me a week or so to get a piece of software that did an okay job at doing that comparing that to how long it took me to write this prompt which is about two minutes think of the time saving here I could have spent a week trying to use string matching to solve this problem or(09:18) I could have spent a couple minutes writing a prop this is just like the core logic of the application this isn’t including all the peripherals the user interfaces the boilerplate code and stuff like that but that’s the cost savings we’re talking about here we’re talking about minutes versus days or weeks of software development and so that’s the power of prompt engineering and this kind of new way of thinking of programming and software development so now let’s talk about best practices for(09:45) prompt engineering here I’m going to talk about seven tricks you can use to write better prompts and this is definitely not a complete or comprehensive list this is just a set of tricks that I’ve extracted from comparing and contrasting a few resources if you want to dive deeper into any one of these tricks check out the blog published and towards data science where I talk more about these tricks and different resources you can refer to to learn more about any of these so just running through this the(10:11) first trick is to be descriptive even though so in a lot of writing tasks less is more when doing prompt engineering it’s kind of the opposite more is better trick twos give examples and so this is the idea of few shot learning you give a few demonstrations of questions and answers for example in your prompt and that tends to improve the llm’s performance trick three is to use structured text which we’ll see what that looks like later trick four is Chain of Thought which is essentially having the llm think step by step trick(10:41) five is using chatbot personas so basically assigning a role or expertise to the large language model trick six is this flipped approach where instead of you are asking the large language model questions you prompted to ask you questions so it can extract information from you to generate a more helpful completion finally trick 7 is what I summarize as reflect review and refine which is essentially having the large language model reflect on its past responses and refine them either by improving it or or identifying errors in(11:15) past responses okay so let’s see what this looks like via a demo here I’m going to use Chad GPT and it’s important to know what large language model you’re using because optimal prompting strategies are dependent on the large language model that you’re using Chachi PT is a fine-tuned model so you don’t really have to break your back too much on the prompt engineering to get reasonable responses but if you’re working with a base model like gpt3 you’re going to have to do a lot more(11:42) work on the prompt engineering side to get useful responses and that’s because gpg3 is not a fine-tuned model it only does word prediction while chat GPT is a fine-tuned model it was trained to take instructions and then on top of that they did this reinforcement learning with human feedback to refine those responses even further trick one is to be descriptive so let’s compare and contrast an example with and without this trick so let’s say I want to use chatgpt to help me write a birthday message for my dad the naive thing to do(12:12) would be to type been to chat GPT the following prompt write me a birthday message for my dad and so it’s gonna do that and so while this might be fine for some use cases I don’t write messages that are verbose like this and the response is a bit generic you know like Dad you’ve been my rock my guide my source of inspiration throughout my life your wisdom kindness and unwavering support has shaped me into the person I am today for that I am eternally grateful oh that’s very nice I tend to be a bit more cheeky when it comes to(12:40) these kinds of birthday messages and whatnot another thing we can do is to employ this trick of being descriptive and getting a good response from chat you PT what that might look like is you type in write me a birthday message for my dad no longer than 200 characters okay so now we don’t want it to be as verbose this is a big birthday because he’s turning 50 so now we’re giving more context to celebrate I booked us a boy’s trip to Cancun more context and then be sure to include some cheeky humor he(13:06) loves that so I’m giving jack gbt more to work with to tailor the response to something closer that I would actually write so let’s see what this response looks like okay so it’s a lot more concise which I like it says happy 50th dad time to Fiesta like you’re 21 again in Cancun cheers to endless Adventures ahead hashtag dad and Cancun that’s actually pretty funny maybe I want to use this exactly but I could see it as like a starting point for actually writing a birthday message so the second(13:32) trick is to give examples let’s compare prompts without and with this trick without giving examples we might prompt chat gbt as follows given the title of a torch data Science Blog article write a subtitle for it here we’re putting in the title as prompt engineering how to trick AI into solving your problems which is the title of the blog associated with this video and then we leave the subtitle area blank so the completion that it spits out is Unleash the Power of clever prompts for more effective AI problem solving yeah pretty(14:06) nifty let’s see what this looks like if we give a few more examples to try to capture the style of the subtitle that we’re looking for and so here the prompt is pretty similar but now I’m putting in the title and subtitle for preceding blogs in this larger Series so here put a practical introduction to llms three levels of using llms in practice then we have cracking open the openai python API a complete beginner friendly introduction with example code and then finally we have the same prompt as we(14:35) saw before so let’s see what it spits out now mastering the art of crafting effective prompts for AI driven Solutions well at face value this might not seem much different than the completion that we saw before I kind of prefer this one over this one here and the only reason is because again I don’t like verbose text and this is more concise than this previous one here so I think maybe that’s what Chad GPT picked up on it’s like oh these subtitles here have these number of tokens let’s make(15:04) sure that the next subtitle has about the same number of tokens just speculating but regardless that’s how you can incorporate examples into your prompt the next trick is to use structured text let’s see what this looks like in action so I suppose this is our prompt for tragedy BT we don’t have any structured text here we’re just putting in prompt without structured text so we’re asking it to write me a recipe for chocolate chip cookies gives a pretty good response gives us ingredients gives us instructions and(15:33) gives us some tips if Chachi PT was not fine-tuned it may not have spit out this very neat structure for a chocolate chip cookie recipe and so this is another indication of why what large language model you’re working with matters because I could be happy with this response here there may not even be a need to use structured text here but still let’s see what this could look like if we did use structured text in our prompt here the prompt is a little different create a well organized recipe for chocolate chip cookies use the(16:04) following formatting elements the key difference here is we’re now asking it specifically to follow this specific format and we’re giving it kind of of a description of each section that we want so let’s see what this looks like so one subtle difference here is that in the completion where we use structured text you notice that it just kind of gives the title and the ingredients and so on this is something that you could easily just copy paste onto like a web page without any alterations well if we go(16:33) here there’s no title which could be fine but you have this certainly here’s a classic chocolate chip cookie recipe for you so now it’s trying to be more conversational and may have required some extra steps if this is fitting into a larger like automated pipeline but other than that it doesn’t seem like there’s much difference between the other aspects of the completion one interesting thing is that here the tips are a bit more clear and bolded well here there’s just some like quick bullet(16:59) points next we have trick four which is Chain of Thought and the basic idea with Chain of Thought is to give the llm time to think and this is achieved by breaking down a complex task into smaller pieces so that it’s a bit easier for the large language model to give good completions without using Chain of Thought this is what the prompt might look like write me a LinkedIn post based on the following medium blog and then we just copy paste the medium blog text here through some text in here so it does a pretty good job again this feels(17:31) way too long for LinkedIn post and it feels like it’s just summarizing the text that I threw in there but I mean it’s not bad this could be a really good starting place but now let’s see what this can look like using Chain of Thought instead of just having it write the LinkedIn post based on the text here I’m trying to explicitly list out my personal process for turning a Blog into a LinkedIn post and trying to get the llm to mimic that so here I put write me a LinkedIn post based on the step-by-step process and medium blog(17:57) given below so here step one come up with a one line hook relevant to the blog step two extract three key points from the article step three compress each point to less than 50 characters step four combine the hook compress key points from step three and add a con to action to generate the final output and then we put the medium text here okay looking at this this seems a lot more reasonable for a LinkedIn post each line is just one sentence it’s not way too much text no one likes reading a wall of text or at least I don’t like reading a(18:27) wall of text so this is much more helpful to me in making a LinkedIn post okay trick five is to use these chatbot personas the idea here is to prompt the llm to take on a particular Persona so let’s see a concrete example of this without the trick let’s just say we want chat gbt to make me a travel itinerary for a weekend in New York city so it spits out something that looks pretty good so now let’s see what this could look like with a Persona so here instead of just asking it straight up for an(18:54) itinerary I say act as an NYC native and cabbie who knows everything about the city please make me a travel itinerary for a weekend in New York City based on your experience don’t forget to include your Charming New York accent in your response okay so let’s see what this does comparing this response with the other response there seems to be a lot of overlap and maybe there’s not a practical difference between these two but it does feel like there are things here that you don’t get here start your(19:23) day with the classic New York breakfast at a local dinner Cafe well this one will just say start with the bagel Central Park stroll Museum and grab a bagel again yeah it’s just eat Bagels every single day oh that’s funny I like how it injected a bit of humor here yep you guessed it another Bagel fuel of your final day maybe you really have to like read through these to get a sense of the subtle differences but maybe just from this Bagel example this just gives you two different flavors of itineraries and maybe one matches your(19:53) interests a bit more than the other trick number six the flipped approach and so here instead of you asking all the questions to the chat bot you prompt the chatbot to ask you questions to better help you with whatever you’re trying to do so let’s see this without the trick let’s say you just want an idea for an llm based application you give it that prompt and it’s just gonna generate some idea for you here’s generating a idea for us edu bot pros and intelligent educational platform that harnesses the power of llms to(20:21) offer personalized learning and tutoring experience for students of all ages and levels so this could be a great product idea the problem is maybe this isn’t something that you’re passionate about or that you really care about or this idea is not tailored to your interests and skill set as someone that that wants to build an app let’s see how the flipped approach can help us with this so here instead of asking for an idea just straight up we can say I want you to ask me questions to help me come up(20:46) with an llm based application idea ask me one question at a time to keep things conversational you can see right off the bat what are your areas of expertise and interest that you’d like to incorporate into your llm based application idea I didn’t think to say oh yeah maybe I should tell the chat bot what I know and what I’m interested in so we can better serve me and maybe there are a bunch of other questions that are critical to making a good recommendation on an app idea that I just wouldn’t think of and(21:12) that’s where the flip approach is helpful because the chatbot will ask you what it needs to know in order to give a good response and those questions may or may not be something that you can think of all up front the seventh and final trick is reflect review and refine and so this is essentially where we prompt the chat bot to look back at previous responses and evaluate them whether we’re asking it for improvements or to to identifying potential mistakes so what this might look like is here we have the edu bot Pro response from(21:44) before let’s see what happens when we prompt it to review the previous response so here I’m saying review your previous response pinpoint areas for enhancement and offer an improved version then explain your reasoning for how you improved the response so I haven’t tried this so we’re both seeing this for the first time it looks pretty similar but since we asked it to explain how it improved the responses it gave us this extra section here so reasoning for enhancements Clarity and conciseness(22:10) emphasizing personalization enhanced language and then monetization strategies the monetization section provides more detail on viable strategies okay cool well I’m not going to read through this but this prompt or something like it you can basically copy paste this as needed to potentially improve any chat completion so I know that was a ton of content and I flew through that but if you want to dive into any particular trick a bit more check out the Torches data Science Blog where I talk about each of of these a(22:36) bit more insight resources where you can learn more everything we have just talked about is applicable to both the easy way and the less easy way of prompt engineering but now I want to focus more on the less easy way and I’m going to try to demonstrate the power of prompt engineering the less Easy Way by building out this automatic greater example we were talking about before using the langchain python Library first as always we’re going to do some imports so here we’re just importing everything(23:04) from langchain and then here we’re going to be using the openai API so that requires a secret key if you haven’t worked with the open AI API before check out the previous video that talks all about that there I talk about what an API is talk about open ai’s API and give some example python code of how you can use it here we’re just importing our secret key which allows us to make API calls here we’re going to make our first chain the main utility of Lang chain is that it provides a ton of boilerplate(23:34) code that makes it easy to incorporate calls to large language models within your python code or some larger piece of software that you’re developing and it does this through these things called chains which is essentially a set of steps which you can modularize into these so-called chains so let’s see what that looks like the first thing we need is our chat model so here we’re going to incorporate open ai’s GPT 3.(24:02) 5 turbo the next thing we need is a prompt template so essentially this is going to be a chunk of text that we can actually pass in inputs and dynamically update with new information so for example this is the same prompt we saw from the previous slide for the automatic grader we’ll be able to pass in these arguments question correct answer and student answer into our chain and it’ll dynamically update this prompt template send it to the chat bot and get back the response to put this chain together it’s super simple(24:33) the syntax looks like this you have llm chain you define what your llm is which is chat model which is the open AI model we instantiated earlier The Prompt is prompt which is the prompt template we created on the previous slide and you combine it all together into this llm chain and we Define it as chain what this looks like in action is as follows we Define the inputs so here we’re going to define the question who was the 35th President of the United States of America we Define the correct answer John F Kennedy and we Define the(25:01) student’s answer FDR and so we can pass all these inputs to the chain as a dictionary so we have this questions correct answer student answer keywords and then we plug in these values that we Define up here and then this is what the large language model spits out students answer is wrong so it correctly grades the student answer as wrong because FDR was not the 35th President of the United States however there’s a small problem with our chain right now namely the output from this chain is a piece of(25:30) text which may or may not fit nicely into our larger data pipeline or software pipe line that we’re putting together it might make a lot more sense instead of outputting a piece of text the chain will output like a true or false indicating whether the student’s answer was correct or not with that numerical or Boolean output it’ll be much easier to process that information with some Downstream task maybe you want to sum up all the correct and incorrect answers of the homework and generate the final grade of the entire worksheet we(26:02) can do this via output parsers so this is another thing we can include in our chains that will take the output text of the large language model we’ll format in a certain way extract some piece of information or convert it into some other format as we’ll see here here I’m defining our output parser to determine whether the grade was correct or wrong and I just use a simple piece of logic here I have it returned a Boolean of whether or not the word wrong is in the text completion as an example before the(26:34) completion was the student answer is wrong so this word wrong appears in the text completion this parser here will return false because wrong is in the completion and so this knot will flip that and it’ll make it false so as you can see like we haven’t automated all the logic out of programming you still need to have some problem solving skills and programming skills here but then once we have our parser defined we can just add it into our chain like this so we have our llm same as before our prompt template same as before and then(27:05) we add this output parser which is the grade output parser that we defined right here and then we can apply this chain so let’s see what this looks like in for Loop so we have the same question and correct answer as before who’s the 35th President of the United States and then the correct answer is John F Kennedy and now we’re defining a list of student questions that we may have received which are John F Kennedy JFK FDR John F Kennedy only one n John Kennedy Jack Kennedy Jacqueline Kennedy and Robert F Kennedy also with one end(27:37) we’ll run through this list in a for Loop we’ll run our chain just like we did before and we’ll print the result and so here we can see that John F Kennedy is true indicating a correct response JFK is true FDR is false John F Kennedy spelled incorrectly is true because we specifically said misspellings are okay John Kennedy is true because we’re just dropping the middle initial Jack Kennedy’s true it’s a common nickname Jacqueline Kennedy is false that was his wife and then Robert F Kennedy is false because that’s his(28:05) brother and as always the code is available at the GitHub repo for this video series which is linked down here feel free to take this code adopt it or maybe just give you some ideas of what’s possible with prompt Engineering in this way I would be remiss if I did not talk about the limitations of prompt engineering which are as follows like I said in before optimal prompt strategies are model dependent what is the optimal prompt for chat GPT it’s going to be completely different than what’s a optimal prompt for gpt3 another downside(28:35) is that not all pertinent information may fit into the context window because only so much information can be passed into a large language model and if you’re talking about a significantly large knowledge base that’s not something that prompt engineering may be able to do most effectively another limitation is that typically the models we use to do prompt engineering are these like huge general purpose models and if you’re talking about a particular use case this might be cost inefficient or even overkill for the problem you’re(29:04) trying to solve and another version of this is that smaller specialized models can outperform a larger general purpose models an example of this was demonstrated by open AI When comparing their smaller instruct GPT model to a much larger version of gpt3 so this brings up the idea of model fine tuning and that’s going to be the topic of the next video in this series there we’re going to break down some key fine-tuning Concepts and then I’m going to share some concrete example code of how you can fine tune your very own large(29:36) language model using the hugging face software ecosystem so I hope this video was helpful to you if you enjoyed it please consider liking subscribing and sharing with others if you have any questions or suggestions for future content please feel free to drop those in the comments section below and as always thank you so much for your time and thanks for watching" }, { "title": "understanding-paramters-neurons-in-llm", "url": "/posts/understanding-paramters-neurons-in-llm/", "categories": "AI, LLM", "tags": "llm, parameters, neurons", "date": "2024-09-06 22:50:18 -0400", "snippet": "The query about whether the number of model parameters in a large language model (LLM) corresponds to the number of neurons can be clarified as follows:Understanding Model Parameters and Neurons in...", "content": "The query about whether the number of model parameters in a large language model (LLM) corresponds to the number of neurons can be clarified as follows:Understanding Model Parameters and Neurons in LLMs Model Parameters: In the context of LLMs, parameters refer to the adjustable weights and biases within the neural network that are learned during the training process. These parameters are crucial as they determine how the model processes input data and generates output. The total number of parameters in a model, often expressed in billions (e.g., 70B parameters), indicates the model’s capacity to learn complex patterns from the training data. More parameters typically allow for more nuanced understanding and generation of language, but they also increase the computational resources needed for training and inference[1][2][5]. Neurons: Neurons are the fundamental units of a neural network. Each neuron receives input, processes it, and passes on its output to subsequent layers. In a neural network, neurons are organized into layers (input, hidden, and output layers). The number of neurons in a model is related to its architecture, including the number of layers and the size of each layer. Each neuron can be associated with multiple parameters (weights and biases) that define its behavior. Relationship Between Parameters and Neurons The number of parameters is not directly equal to the number of neurons. Instead, each neuron typically has multiple parameters associated with it: For example, in a fully connected layer, each neuron receives inputs from all neurons in the previous layer, resulting in a weight for each connection, plus a bias term. Therefore, the total number of parameters in a layer is the product of the number of neurons in that layer and the number of neurons in the previous layer, plus the biases. A model with a large number of parameters may have many neurons, but the exact relationship depends on the specific architecture of the model. For instance, a model could have a few neurons with many parameters (if each neuron has many connections) or many neurons with fewer parameters each.ConclusionIn summary, while the number of parameters in an LLM gives an indication of its complexity and capacity, it does not directly equate to the number of neurons. The architecture of the model, including how neurons are connected and the number of layers, plays a significant role in determining the total number of parameters. Understanding this distinction is crucial for evaluating the capabilities and limitations of different LLMs." }, { "title": "weights-and-parameters-llms", "url": "/posts/weights-and-parameters-llms/", "categories": "AI, LLM", "tags": "weights, parameters, model", "date": "2024-09-06 22:21:11 -0400", "snippet": "In the context of large language models (LLMs), model weights and parameters are closely related but distinct concepts:Model ParametersModel parameters are the variables that are learned during the...", "content": "In the context of large language models (LLMs), model weights and parameters are closely related but distinct concepts:Model ParametersModel parameters are the variables that are learned during the training process of the LLM. They define the behavior and capabilities of the model. Examples of model parameters in LLMs include: Weights of connections between neurons in the neural network architecture Biases added to the inputs of neurons Embeddings that map input tokens to dense vector representationsThe values of these parameters are optimized during training to minimize the model’s loss on the training data. The final values of the parameters determine how the model will perform on unseen data.Model WeightsModel weights are a specific type of model parameter that determine the strength of connections between neurons in the neural network. They are the coefficients that scale the input signals to each neuron.In other words, weights are the numerical values that are multiplied with the inputs to compute the weighted sum that goes into each neuron. The weights are the primary parameters that are learned and updated during training to capture the patterns in the data.So in summary: Model parameters are all the variables in the LLM that are learned during training, including weights, biases, embeddings, etc. Model weights are a subset of the model parameters that specifically refer to the coefficients that scale the inputs to each neuron in the neural network.Both model parameters and weights are crucial for the performance of LLMs. The training process aims to find the optimal values of all the parameters, especially the weights, to enable the model to generate high-quality text outputs. The final values of the weights and other parameters encapsulate the knowledge acquired by the LLM during training." }, { "title": "Fine tuning LLMs", "url": "/posts/fine-tuning-llms/", "categories": "AI, LLM", "tags": "fine-tuning, yt", "date": "2024-09-06 22:07:58 -0400", "snippet": "(00:00) hey everyone I’m Shaw and this is the fifth video in the larger series on how to use large language models in practice in the previous video we talked about prompt engineering which is conc...", "content": "(00:00) hey everyone I’m Shaw and this is the fifth video in the larger series on how to use large language models in practice in the previous video we talked about prompt engineering which is concerned with using large language models out of the box while prompt engineering is a very powerful approach and can handle a lot of llm use cases in practice for some applications prompt engineering just doesn’t cut it and for those cases we can go one step further and fine-tune a existing large language model for a(00:31) specific use case so Navy’s question is what is model fine tuning the way I like to Define it is taking a pre-trained model and training at least one internal model parameter and here I mean the internal weights or biases inside the neural network what this typically looks like is taking a pre-trained existing model like gpt3 and fine-tuning it for a particular use case for example chatgypt to use an analogy here gpt3 is like a raw diamond right out of the earth it’s a diamond but it’s a bit rough around(01:07) the edges fine tuning is taking this raw diamond and transforming it into something a bit more practical something that you can put on a diamond ring for example so the process of taking the raw base model of gpt3 and transforming it into the fine-tuned model of gbt 3.5 turbo for example is what gives us applications like chat GPT or any of the other incredible applications of large language models we’re seeing these days to get a more concrete sense of the difference between a base model link gpt3 and a fine-tuned model let’s look(01:41) at this particular example we have to keep in mind that these Foundation large language models like gpg3 llama 2 or whatever your favorite large language model is these models are strictly trained to do word prediction given a sequence of words predicting the next word so when you train one of these launch language models on huge Corpus of text and documents and web pages what it essentially becomes is a document completer what that translates to in practice is if you plug into a lot of these base models like gpt3 the prompt(02:13) tell me how to find tune a model a typical completion might look something like this where it’s just listing out questions like you might see in a Google search or maybe like a homework assignment or something here when I prompted gpt3 to tell me how to fine-tune a model the completion was as follows how can I control the complexity of a model how do I know when my model is done how do I test a model well this might be reasonable for gpt3 to do based on the data that it was trained on this isn’t something that’s very practical(02:43) now let’s look at the fine-tuned model completion so now we have text DaVinci zero zero three which is just one of the many fine-tuned models based on gpt3 coming from open AI we give it the same prompt tell me how to fine tune a model and this is the completion fine-tuning a model involves a adjusting the parameters of a pre-trained model in order to make it better suited for a given task there are generally three steps involved to fine-tuning a model select a base model adjust parameters train the model while this completion(03:11) may not be perfect it’s much more aligned what we were hoping to get out of the language model compared to the base model’s completion so if you want to learn more about model fine tuning and how open AI did their fine tuning their alignment tuning and instruction tuning check out the references in the description and comment section below so as we saw when comparing a base model to a fine-tuned model we see that the fine-tune model can generate completions that are much more aligned and desirable for our particular use case Beyond this(03:40) performance there’s actually a deeper reason why you might want to fine tune and that is the observation that a smaller fine-tuned model can often outperform a larger base model this was demonstrated by open AI in their instruct GPT model where they’re small 1.3 billion parameter fine tuned instruct GPT model generated to completions that were preferred to gpt3 completions even though gpt3 had about 100 times as many internal parameters this is one of the biggest upsides of fine tuning you don’t have to rely on(04:17) some massive general purpose large language model to have good performance in a particular use case or application now that we have a better understanding of what fine-tuning is and why it’s so great let’s look at three possible ways one can fine tune an existing large language model the first is via self-supervised learning this is the same way these base models and Foundation large language models are trained so in other words you get your training Corpus of text and you train the model in a self-supervised way in(04:47) other words you take a sequence of text like listen to your you feed it into the model and you have it predict a completion if we feed in listen to your it might spit out hard what would differentiate fine tuning with self-supervised learning versus just training a base model through self-supervised learning is that you can curate your training Corpus to align with whatever application that you’re going to use the fine-tuned model for for example if I wanted to fine-tune gpt3 to write text in the likeness of Me(05:21) Maybe I would feed it a bunch of my torch data science blogs and then that resulting fine-tuned model might be able to generate completions that are more like my style the second way we can fine tune a model is via supervised learning this is where we have a training data set consisting of inputs and Associated outputs or targets for example if we have a set of question answer pairs such as who was the 35th President of the United States and then the answer is John F Kennedy we can use this question answer pair to fine-tune an existing(05:53) model to learn how to better answer questions so the reason this might be helpful as we saw before if we were to just feed in who was the 35th President of the United States into a base model the completion that it might generate is who was the 36th president of the United States who was the 40th President of the United States who is the speaker of the house so on and so forth but through having these question answered pairs we can fine tune the model to essentially learn how to answer questions but there’s a little trick here these(06:22) language models are again document completers so we actually have to massage these input output pairs a bit before we can feed it into our large language model for training one simple way we can do this is via prompt templates for example we could generate a template please answer the following question where we input the question here so our input would go here and then we input the target here so the answer would go here and then through this process we can translate our training data set to a set of prompts and(06:54) generate a training Corpus and then go back to the self-supervised approach and the final way one one can fine tune an existing model is via reinforcement learning while there are many ways one could do this I’m going to just focus on the approach outlined by open Ai and generating their instruct GPT models which consisted of three steps the first was supervised fine tuning so essentially what we were talking about in this second way to fine-tune a model this consists of two steps one curating your training data set and then two(07:27) fine-tuning the model The Next Step was to train a reward model and all this is is essentially a model that can generate a score for a language model’s completion so if it generates a good completion it’ll have a high score it generates a bad completion it’ll generate a low score so what this looked like for the instruct GPT case was as follows you start with a prompt and you pass it in to your supervised fine-tuned model from here but you don’t just do it once you actually do it many times so(07:56) you generate multiple completions for the same prompt then you get human labelers to rank the responses from worst to best and then you can use that ranking to train the reward model which is indicated by this Square here and then the final step is to do reinforcement learning with your favorite reinforcement learning algorithm in the case of instruct GPT they used proximal policy optimization or PPO for short what this looks like is you take the prompt you pass it in to your supervised fine-tuned model and then you pass that completion to the(08:34) reward model and then the reward model will essentially give feedback to the fine-tuned model and this is how you can update the model parameters and eventually end up with a model that’s fine-tuned even further I know this was a ton of information but if you want to dive deeper into any one of these approaches check out the blog in towards data science where I go into a bit more detail on each of these approaches okay so to keep things relatively simple for the remainder of the video we’ll be focused just on the supervised learning(09:02) approach to model fine tuning here I break that process down into five steps first choose your fine tuning task so this could be text summarization it could be text generation it could be binary classification text classification whatever it is you want to do next you prepare your training data set if you’re trying to do text summarization for example you would want to have input output pairs of text and the desired summarization and then you take those input Alpha Pairs and generate a training Corpus using prompt(09:33) templates for example next you want to choose your base model there are many Foundation large language models out there or there are many existing fine-tuned large language models out there and you can choose either of these as your starting place next we can fine tune the model via supervised learning and then finally we evaluate model performance there’s certainly a lot of details into each of these steps but here I’m just going to focus on step number four the fine tuning the model with supervise learning and here I want(10:00) to talk about three different options we have when it comes to updating the model parameters the first option is to retrain all the parameters given our neural network given our language model we go in and we tweak all the parameters but perhaps obviously this comes with the downside when you’re talking about billions tens of billions hundreds of billions of internal model parameters the computational cost for training explodes even if you’re doing the most efficient tricks to speed up the training process retraining a billion(10:33) parameters is going to be expensive another option we can do is transfer learning and this is essentially where we take our language model and instead of retraining all the parameters we freeze most of the parameters and only fine tune the head namely we fine tune the last few layers of the model where the model embeddings or internal representations are translated into the Target or the output layer and while transfer learning is a lot cheaper than retraining all parameters there is still another approach that we can do which is(11:06) the so-called parameter efficient fine tuning this is where we take our language model and instead of just phrasing a subset of the weights we freeze all of the weights we don’t change any internal model parameters instead what we do is we augment the model with additional parameters which are trainable and the reason why this is advantageous is that it turns out that we can fine tune a model with a relatively small set of new parameters as can be seen by this beautiful picture here one of the most popular ways to do(11:38) this is the so-called low rank adaptation approach or low raw for short like I mentioned in the previous slide this fine tunes a model by adding new trainable parameters here we have a cartoon of a neural network but let’s just consider one layer the mapping from these inputs to this hidden layer here we can call our inputs X and then we can call the hidden layer essentially some function of X and to make this a bit more concrete we can write this as an equation h of X is just equal to X we can think of it as a vector to keep(12:13) things simple and some white Matrix which is just some two-dimensional Matrix to see this a bit more visually we have our weight Matrix which is some d by K Matrix we have X which will just take to be a vector in this case the multiplication of these two things will generate our hidden layer and for the mathesonatos here the spaces that these objects live in and so this is what the situation looks like without Lora where if we’re going to do the full parameter fine-tuning what this will look like is all the parameters in this weight Matrix(12:46) are trainable so here W naught is a d by K Matrix and let’s just say d is a thousand K is a thousand this would translate to one million trainable parameters which may not be a big number but when you have a lot of layers this number of triangle parameters can really explode now let’s see how low raw can help us reduce the number of trainable parameters again we’re just going to look at one of the layers but now we’re going to add some additional parameters to the model what that looks like mathematically is we have the W naught(13:19) times x is equal to H of X like we saw in the previous slide but now we’re adding this additional term here which is Delta W Times X this is going to be another weight Matrix the same shape as W naught and looking at this you might think sha how does this help us we just doubled the number of parameters yeah sure if we keep W naught Frozen we still have Delta W with the same number of parameters to deal with but let’s say that we Define delta W to be the multiplication of two matrices b and a in this case our hidden layer becomes W(13:53) naught times X Plus ba times x looking at this more visually we have W naught which is the same same weight Matrix we saw in the previous slide but now we have b and a which have far fewer terms than W naught does and then what we can do is through matrix multiplication generate a matrix of the proper size namely Delta W add it to W naught multiply all that by X and generate our h of X looking at the dimensionality of these things W naught and Delta W live in the same space they’re matrices of d by K B is going to be a matrix of d by R(14:30) A is going to be a matrix of R by K and then h of X is going to be D by 1. the key thing here is this R number what the authors of this method called the intrinsic rank of the model the reason that this works and we get the efficiency gains is that this R is a lot smaller than D and K to see how this plays out unlike before where W naught was trainable now these parameters are going to be Frozen and B and a are trainable and maybe as you can just tell visually from the area of this rectangle versus the areas of these two rectangles(15:04) b and a contain far fewer terms than W naught to make this a bit more concrete let’s say d is equal to a thousand K is equal to a thousand and our intrinsic rank is equal to two what this translates to is 4 000 trainable parameters as opposed to the million trainable parameters we saw in the previous slide this is the power of low raw it allows you to fine tune a model with far fewer trainable parameters if you want to learn more about Laura check out the paper Linked In the description below or if you want something that’s a(15:35) bit more accessible check out the blog and towards data science where I talk about this a bit more let’s dive into some example code and how we can use low raw to fine tune a large language model here I’m going to use the hugging face ecosystem namely pulling from libraries like data sets Transformers p e f t and evaluate which are all hugging face python libraries also importing in pi torch and numpy for some extra things with our Imports the next step is to choose our base model here I use distill(16:07) burnt uncased which is a base model available on hugging faces model repository this is what the model card looks like we can see that it only has 67 million parameters in it and then there’s a lot more information about it on the model card here we’re gonna take distilbert uncased and we’re gonna fine tune it to do sentiment analysis we’re going to have it take in some text and generate a label of either positive or negative based on the sentiment of the input text so to do that we need to Define some label Maps so here we’re(16:41) just defining that 0 is going to be negative and one is going to mean positive and vice versa that negative means zero and positive means one now we can take these label maps and we can take our model checkpoint and we can plug it into this Nifty Auto model for sequence classification and class available from the Transformers library and very easily we import this base model specifically ready to do binary classification the way this works is that hugging face has all these base models and has many versions of them(17:15) where they replace the head of the model for many different tasks and we can get a better sense of this from the Transformers documentation as shown here you can see that this Auto model for sequence classification has a lot of Base models that it can build on top of here we’re using distilber which is a smaller version of bird here but there are several models you can choose from the reason I went with distillbird is because it only has 67 million parameters and it can actually run on my machine the next step is to load the(17:45) data set so here I’ve actually just made the data set available on the hugging face data set repository so you should be able to load it pretty easily it’s called IMDb truncated it’s a data set of IMDb movie reviews with an Associated positive or negative label if we print the data set it looks something like this there are two parts to it there’s this train part and then there’s this validation part and then you can see that both the training and validation data sets have 1000 rows in them this is(18:13) another great thing about model fine tuning is that while training a large language model from scratch may require trillions of tokens or a trillion words in your training Corpus fine-tuning a model requires far fewer examples here we’re only going to be using a thousand examples for model fine tuning the next step is to pre-process the data here the most important thing is we need to create a tokenizer if you’ve been keeping up with this series you know that tokenization is a critical step when working with large language models(18:43) because learner networks do not understand text they understand numbers and so we need to convert the text that we pass into the large language model into a numerical form so that it can actually understand it so here we can use the auto tokenizer class from Transformers to grab the tokenizer for the particular base model that we’re working with next we can create a tokenization function this is a function that defines how we will take each example from our training data set and translate it from text to numbers this(19:16) will take in examples which is coming from our training data set and you see we’re extracting the text so going back to the previous slide you can see that our training data set has two features it has a label and a piece of text so you can imagine each row of this training data set has text and it has a label associated with that text so when we go over here the examples is just like a row from this data set and we’re grabbing the text from that example and then what we do is we Define the side that we want to truncate truncation is(19:48) important because the examples that we pass into the model for training need to be the same length we can either achieve this by truncating long sequences or padding short sequences to like a predetermined fixed length or a combination of the two so we’re just choosing the current truncation side to be left and here we’re tokenizing the text here’s our tokenizer that we defined up here passing in the text we’re returning numpy tensors we’re doing the truncation and we defined how to do that here and then we’re defining(20:17) our max length and this will return our tokenized inputs since the tokenizer does not have a pad token this is a special token that you can add to a sequence which will essentially be ignored by the large language model here we’re adding a pad token and then we’re updating the model to handle this additional token that we just created finally we’ve applied this tokenize function to all the data in our data set using this map method here we have our data set and we plan this map method we pass in our tokenized function and it’ll(20:49) output a tokenized version of our data set to see what the output looks like we have another data set dictionary we have the training and validation data sets but now you see we have have these additional features we don’t only have the text in the label but we also have input IDs and we also have this attention mask one other thing we can do at this point is to create a data collator this is essentially something that will dynamically pad examples in a given batch to be as long as the longest sequence in that batch for example if we(21:20) have four examples in our batch the longest sequence has 500 but the others have shorter ones it’ll dynamically pad the shorter sequences to match the longer one and the reason why this is helpful is because if you pad your sequences dynamically like this with a collater it’s a lot more computationally efficient than padding all your examples across all 1000 training examples because you might just have one very long sequence at 512 that is creating unnecessary data that you have to process next we want to define a(21:51) valuation metrics so this is how we will monitor the performance of the model during training so here I just did something simple I’m going to import the accurac see from the evaluate python Library so we can package our evaluation strategy into a function that here I’m going to call compute metrics and so here we’re not restricted to just using one evaluation metric or even just using accuracy as an evaluation metric but just to keep things simple here I just stick with accuracy here we take a model(22:18) output and we unpack it into a prediction and label the predictions here are the logits and so it’s going to have two elements one associated with the negative class and one associated with the Positive class and all this is doing is evaluating which element is larger and whichever one is larger is going to become the label so if the zeroth element is larger the ARG Max will return zero and that’ll become the model prediction and vice versa if the first element is the largest this will return a one and then that will become(22:50) the model prediction and then here we just compute accuracy by comparing the model prediction to the ground truth label so before training our find tuned model we can evaluate the performance of the base model out of the box so let’s see what that looks like here we’re going to generate a list of examples such as it was good not a fan don’t recommend the better than the first one this is not worth watching even once and then this one is a pass then what we do is for each piece of text in this list(23:18) we’re gonna tokenize it compute the logits so basically we’re going to pass it into the model and take the logits out then we’re going to convert the logits to a label either a zero or one and so the output looks like this we have the untrained model predictions it was good the model says this has a negative sentiment not a fan don’t recommend the model says this as a negative sentiment so that’s correct better than the first one the model says this has a negative sentiment even though that’s probably positive this is(23:45) not worth watching even once model says it’s a negative sentiment which is correct and then this one is a pass and the model signs a negative sentiment to them as you can see it got two out of five correctly essentially this model is as good as chance as flipping a coin it’s right about half the time which is what we would expect from this unfine-tuned based model so now let’s see how we can use Lora to fine-tune this model and hopefully get some better performance the first thing we need to do is Define our low configuration(24:14) parameters first is the task type we’re saying we’re going to be doing sequence classification next we Define the intrinsic rank of the trainable weight matrices so that was that smaller number that allowed b and a to have far fewer parameters than just W naught next we Define the lower Alpha value which is essentially a parameter that’s like the learning rate when using the atom Optimizer then we Define the low raw Dropout which is just the probability of Dropout and that’s where we randomly(24:43) zero internal parameters during training finally we Define which modules we want to apply low raw to and so here we’re only going to apply it to the query layers and then we can use these configuration settings and update our model to get another model but one that is ready to be fine-tuned using low raw and so that’s pretty easy we just use this get p e f t model by passing in our original model and then our config from above then we can easily print the number of trainable parameters in our model and we can see it’s about a(25:13) million out of this 67 million that are in the base model so you can see that we’re going to be fine-tuning less than two percent of the model parameters so it’s just a huge cost savings like 50 times fewer model parameters than if we were to do the full parameter fine tuning next we’re going to Define our hyper parameters and training arguments so here we put the learning rate as .(25:37) 001 we put the batch size as four and the number of epochs as 10. next we say where we want the model to be saved here I dynamically create a name so it’ll be the model checkpoint Dash low raw text classification learning rates what we defined before batch size is what we put before find weight Decay as 0.(25:57) 01 then we set the evaluation strategy as Epoch so every Epoch is going to compute those evaluation metrics the safe strategy is every Epoch is going to save the model parameters and then load best model at the end so at the end of training it’s going to return us the best version of the model then we just plug everything to this trainer class trainer takes in the model and takes in the learning arguments it takes in our training and validation data sets it takes in our tokenizer it takes in our data collator and it takes in our evaluation metrics(26:25) put that all into this trainer class and then we train the model using this dot train method so during training these metrics will be generated so we can see the epochs the training loss the validation loss and the accuracy so as you can see the training loss is decreasing which is good and the accuracy is increasing which is good but you can see that the validation loss is increasing so this is a sign of overfitting which I’ll comment on in a bit here now that we have our fine-tuned model in hand we can evaluate its(26:54) performance on those same five examples that we evaluated before 5 fine tuning basically same code copy pasted but here’s the different output the text it was good is now correctly being classified as positive not a fan don’t recommend is correctly classified as negative better than the first one correctly classified as positive and then this is not worth watching even one correctly classified as negative and then this one this one is a pass it’s classified as positive but this one’s a little tricky even though we don’t get(27:22) perfect performance on these five like baby examples we do see that the model is performing a little bit better and so returning back to the overfitting problem this example is meant to be more instructive than practical in practice before jumping to low raw one thing we might have tried is to Simply do transfer learning to see how close we can get to something that does sentiment analysis well after doing the transfer learning then maybe we would use low Rod to fine tune the model even further either way I hope this example was(27:52) instructed and gave you an idea of how you can start fine-tuning your very own large language models if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please feel free to drop those in the comment section below and as always thank you so much for your time and thanks for watching" }, { "title": "Compressing LLMs with Python", "url": "/posts/compressing-llm-with-python/", "categories": "AI, LLM", "tags": "ai, llm, compression, yt", "date": "2024-09-06 19:33:36 -0400", "snippet": "(00:00) large language models have demonstrated impressive performance across a wide range of use cases while this is largely due to their immense scale deploying these massive models to solve real...", "content": "(00:00) large language models have demonstrated impressive performance across a wide range of use cases while this is largely due to their immense scale deploying these massive models to solve real world problems can be challenging in this video I’m going to discuss how we can overcome these challenges by compressing llms I’ll start with a highlevel overview of key Concepts and then dive into a specific example with python code and if you’re new here welcome I’m I make videos about data science and Entrepreneurship if you enjoy this(00:34) content please consider subscribing that’s a great no cost way you can support me in all the videos that I make last year the Mantra and AI seemed to be bigger is better where the equation for creating better models was more data plus more parameters plus more compute and we can see over time large language models just kept getting larger and larger this is a figure from reference 11 down here and we can see over time models kept getting bigger and bigger so in 2018 large meant something around 100 million parameters in 2019 with gpt2 we(01:11) were up to 1 billion parameters then came gpt3 which was around 100 billion parameters then we have more recent language models which have a trillion parameters or more there’s no doubt that this equation actually works GPT 4 is objectively better than gpt3 and everything that came before it however there’s a problem with creating bigger and bigger models simply put bigger models come with higher costs so just to put this into computational terms a 100 billion parameter model is going to take up 200 GB of storage and then if you(01:48) want to use this model you got to fit this massive thing into the memory of your machine so needless to say this comes with high compute costs so it’s probably not something that’ll run on your laptop you’re going to need a lot more compute than that it comes with higher Financial costs and then of course it comes with a higher environmental cost but what if there was a way we could make these Large Scale Models a lot smaller this is the motivation of model compression which aims to reduce the size of a machine(02:19) learning model without sacrificing its performance so if we’re able to pull this off by taking a massive model and shrinking it down to a smaller model this means we could run one of these models on our laptop or even on other devices like cell phones or SmartWatches or other types of devices not only does this Foster greater accessibility for this technology it also promotes user privacy because these models can be run on device and user information does not need to be sent to a remote server for inference this also means less Financial(02:55) cost and of course the negative environmental impact can be a lot smaller here I’m going to talk about three different ways we can compress these models the first is quantization the second is pruning and the third approach is called knowledge distillation starting with quantization although this might sound like a scary and sophisticated word it’s a very simple idea quantization consists of lowering the Precision of model parameters and you can think of this like taking a high resolution photo and converting it to a lower resolution one(03:29) that still captures the main features of the image and to put this into computational terms you might take a model whose model parameters are represented in fp32 so using 32 bits and translating the parameters into int 8 just to get a sense of this the number seven represented in fp32 looks something like this so the computer has to keep track of all these binary digits just to encode a single parameter however if that same parameter is represented in in eight that’ll look something like this it’s a fourth of the(04:03) memory footprint if you want more details on how quantization Works under the hood I talked more about it in a previous video of this series on Q Laura I’ll link that here for those who are interested when it comes to implementing quantization on large language models there are two popular categories of approaches the first is called post trining quantization and the basic idea here is you train your model and then quantize it the key upside of this is that this allows you to take models that other people have trained and then you(04:35) can take them and quantize them without any additional training or data curation using this approach you can take these off-the-shelf models that might be encoded in fp32 and convert the parameters to 8bit or even 4bit however if you want to compress Beyond 4bit post-training quantization typically leads to a degradation in model performance for situations where even more compression is needed we can turn to another set of approaches known as quantization aware training this essentially flips the order where one(05:10) quantizes the model first and then trains IT training models in lower Precision is a powerful way to get compact models that still have good performance this means parameters can be encoded with even fewer than four bits for example in reference number six the authors were able to create a one bit model that mat matched the performance of the original llama model but of course the downside of quantization aware training is that it is significantly more sophisticated than post-training quantization because one(05:41) has to train the quantized model from scratch the second compression approach is pruning which consists of removing unnecessary components from a model so an analogy here is that pruning is like clipping off dead branches from a tree it reduces the tree’s size without harming it to put this in terms of parameters we might have a 100 billion parameter model but then through pruning we might reduce it down to 60 billion parameters while there are a wide range of pruning approaches out there they can be broadly classified into two(06:14) categories the first is called unstructured pruning which consists of removing individual weights or parameters from the model showing that visually if this is our original model here unstructured pruning might consist of zeroing out the these two weights in the model the key benefit of unstructured pruning is that since it’s operating on this granular scale of individual weights it can result in a significant reduction of non-trivial model parameters however there’s a key caveat here since we’re just taking(06:47) model weights and turning them into zeros this is going to result in sparse Matrix operations to get predictions from our model in other words The Matrix multiplications involved in generating a prediction from our model will consist of a lot of zeros and this isn’t something that normal Hardware can do any faster than nonsparse Matrix operations which means that one needs specialized Hardware that’s designed to optimize these sparse Matrix operations in order to realize the benefits of unstructured pruning on the other hand(07:21) we have structured pruning which instead of removing individual weights it removes entire structures from the model this can be think things like attention heads neurons or even entire layers so visually what this might look like is if this is our original model we might remove this entire neuron from the model which does not result in the sparse Matrix operations that we see in unstructured pruning while this does result in less opportunities for model reduction it allows one to completely remove parameters from the model if you(07:56) want to explore specific unstructured and structured pruning techniques check out out reference number five which provides a nice survey of these approaches the final way we can compress an llm is via knowledge distillation this is where we transfer Knowledge from a larger model into a smaller one this is just like how we all learn at school where we have a teacher who has much more experience in a particular subject transferring their knowledge to the students in the context of large language models the teacher model might(08:28) have a 100 billion parameters which are then distilled to a student model which might have just 50 billion parameters again there are a couple of ways that we can achieve this the first is using soft targets which consists of training the student model using the logits from the teacher model what that means is let’s say we have our teacher model here and let’s say it performs sentiment analysis so given a chunk of text it’ll label that text as either positive sentiment or negative sentiment the way these model models work is that(09:00) the raw outputs are not just a positive or negative prediction but rather there’s a prediction for each class known as a logit for example let’s say the logit for the positive class is 0.85 and the logit for the negative class is minus 0.85 what this is indicating is that the input text is more likely to be positive sentiment than negative sentiment and this is exactly how text generation models like l 3.(09:32) 1 or gp4 work under the hood however instead of having two output logits these models will have tens of thousands of output lits corresponding to each token in its vocabulary these lits are then converted into probabilities and then these probabilities can be sampled to generate text one token at a time so we can actually use these loits to do knowledge distillation so the way that works is we’ll take our smaller student model have it generate predictions and then we’ll compare those predictions to the teacher model’s predictions for the same(10:07) input text and the reason these are called Soft targets is because the predictions of the student model aren’t compared to a zero or one ground truth but rather a softer fuzzier probability distribution this turns out to be an effective strategy because using all the output logits from the teacher model provides richer information to the student model to learn from another another way to achieve knowledge distillation is instead of using logits to train the student model one can use synthetic data generated by the teacher(10:39) model a popular example of this was the alpaca model which took synthetic data generated from the original chat GPT and used it to perform instruction tuning on llama 7B in other words chat GPT was used to generate these input output pairs of input prompts from users and out put responses from the model which were then used to endow this llama 7B model with the ability to follow instructions and follow user prompts so now with a basic understanding of the key Concepts behind model compression let’s see what this looks like in code(11:17) as always the example code here is freely available on the GitHub additionally all the models derived here and the data set used for training is also freely available on the hugging face Hub and we’ll be using python for this example as well as pytorch the example here is we’re going to take a text classifier and compress it using knowledge distillation and quantization so that’s one thing I actually forgot to mention is that these three different compression approaches of quantization pruning and knowledge distillation are(11:49) often independent of one another which means that we can combine multiple approaches to achieve maximum model compression so here I’m going to combine knowledge distillation with quantization to achieve a 7x reduction in model size the first step here is to do some imports many of these are hugging face libraries so data sets is from hugging face we’ll import some things from Transformers we’re going to import some things from pytorch and then finally I’m going to import some evaluation metrics(12:20) from psyit learn then I’ll import the data set with one line of code and this is something I’ve made available on the hugging face Hub it consists of a training testing and validation data set with a 70515 split so it’s 2100 examples of data in the training data set and then 450 examples in the testing and validation and the data set consists of two columns the First Column are website URLs and the second column is a binary label of whether that URL is a fishing website or not a fishing website so this(12:54) is actually a very practical use case used by email providers or cyber security folks that may want to ensure that links are safe before presenting them to end users with the data loaded in we’ll load in our teacher model here to speed things up I used the freely available GPU on Google collab so I’m importing that GPU as a device here next I’m going to load in the teacher model which is a model I ftuned on This fishing classification task we can load in the model’s tokenizer and the model itself using these two lines of code(13:28) here and then using this two method I’m loading the model onto the GPU then we can load in our student model so here we’re going to create a model from scratch I’m going to copy the architecture of distill bir to initialize the model however I’m going to drop four of the attention heads from each layer additionally I’m going to drop two of the layers from the model in other words each attention layer has 12 attention heads so I’m going to reduce that to eight and the original architecture has six layers and I’m(13:57) going to reduce that down to four then I’m going to use this distill BT for sequence classification object what that does is it’ll load in this distill BT architecture with these modifications and then slap on top of it a classification head in other words the model instead of generating text is going to perform text classification we’re also going to load the student model onto the GPU and just to get a sense of the scale here the teacher model has 109 million parameters and takes up 438 megab of memory while the(14:28) student model here here consists of 52.8 million parameters and takes up 211 MB of memory the reason I’m using relatively small models by today’s standard is that this is what I can easily run on the free GPU on collab but if you have beefier gpus or more compute at your disposal you can take this code and just plug in bigger models and it should work just fine so the data set that we loaded in consists of plain text with a label so before we can actually use the this data we’ll need to tokenize(15:01) it here I defined a simple pre-processing strategy so what’s happening here is each URL is being converted into a sequence of Tokens The Tokens are being truncated so they’re not too long and then within each batch of examples the shorter sequences are going to be padded so all the examples have the same length and this is important so we can convert it into a pytorch tensor and efficiently do the computation with the gpus this the pre-processing function the the actual transformation happens in this line of(15:31) code so we take the data set and then we map it into tokenized Data making sure that we are using batches and then we’re converting it into a pytorch format where we have columns for the tokens the attention mask and the target labels another thing we need to do is Define an evaluation function so this will allow us to compute evaluation metrics during model training and so there’s a lot happening here so I’m going to go through it line by line first we’re putting the model into eval mode instead(16:00) of training mode we’re initializing two lists one list is for model predictions another list is for labels here we’re going to disable gradient calculations then batch by batch we’re going to do the following first we’re going to load all the data onto the GPU so that’s the input tokens the attention mask and the labels then we’re going to perform the forward pass so we’re going to compute model outputs and then we’re going to extract the logits from the outputs this lits variable here will actually consist(16:28) of two numbers one one corresponding to the probability that the URL is fishing and another corresponding to the probability that the URL is not fishing so in order to turn this into a binary classification in other words the URL is fishing or is not fishing we can take the ARG Max of this logits variable and then that’ll be our prediction and then we can append the predictions and the ground truth label to these lists we initialized earlier once we do that for all the batches we can compute the accuracy precision recall and F1 score(17:00) for all the data in one go next we’re going to define a custom loss function and the way we’re going to do that is we’re going to use both soft Targets in other words the logits from the teacher model and the ground truth labels and so the way we’re doing that here is we’re going to compute a distillation loss as well as a hard loss and then we’re going to combine those into a final loss so to get the distillation loss we’ll first compute the soft targets so these are the teachers logits and then we’re going(17:29) to convert those logits into probabilities in order to generate probabilities from the teacher models logits we can use the soft Max function and it’s common practice to divide the teacher logits by a temperature parameter which will increase the entropy of the probability distribution so we generate a probability distribution corresponding to the teacher’s prediction and then a probability distribution corresponding to the students prediction and now that we have two probability distributions one from the teacher model one from the(18:01) student model we can compare their differences using the KL Divergence pytorch has a built-in method that does that so we can just easily compute the difference between these two probability distributions using this line of code here and then we can compute the hard loss so instead of comparing the student model’s predictions to the teacher model’s predictions we’re going to compare them to the ground truth label and then we’ll use the cross entropy loss to compare those probabilities distributions and then finally we can(18:31) combine these losses by adding them together and adding this Alpha parameter which controls how much weight we’re giving to the distillation loss versus the hard loss next we’ll Define the hyperparameter so here I use a badge size of 32 put the learning rate as .(18:51) 001 we’ll do five Epoch we’ll set the temperature that we use in our loss function at two and then we’ll set the alpha so the relative weights of the distillation loss versus the hard loss as 0.5 so we’ll give equal weight to both types of losses then we’ll Define our Optimizer so we’ll use atom then we’ll create two data loaders we’ll have a data loader to control the flow of batches for the training data as well as the testing data then we’ll train the model using pytorch so we put the student model into train mode and then(19:21) train it we have two for Loops here so we have one for the epoch one for the batches and it’s a similar thing as to what we saw in the evaluation function so we’ll load each batch onto the GPU we’ll compute the outputs of the teacher model and then since we’re not training the teacher model there’s no need to calculate gradients so we can avoid that using this syntax here then we’ll pass through the student model to generate its outputs and extract its logits we’ll compute the loss value using our(19:47) distillation loss that we defined earlier and then we’ll perform the back propagation sorry the script was too long so I have to extend it like this but once we make it through every single batch we can print the performance metric tricks after each Epoch so we’ll print the accuracy precision recall F1 score for the teacher model and then the accuracy precision recall F1 score for the student model and then we’ll be sure to put the student model back into train mode because this evaluate model function that we defined earlier puts it(20:16) into eval mode so I know this was a ton of code maybe way more code than you were hoping to get into but here are the results of the training so we have five Epoch here and we can see the loss is going down which is a good sign so it bumped up in Epoch 4 but then it dropped back down in Epoch 5 which is very normal and then we can compare the performance of the teacher and student models so of course since we’re not updating the teacher model its accuracy is going to stay the same across all Epoch cuz it’s not changing but we can(20:46) see the student model performance get better and better across each Epoch and then once we get to Epoch number five the student model is actually performing better than the teacher across all evaluation metrics next we can evaluate the performance of the teacher and student models using the independent validation data set so the training set is used to update model parameters the testing data set is used in tuning the hyperparameters of the model and the validation set wasn’t touched so this will give us a fair evaluation of each(21:18) model and for that we again see that the student model is performing better than the teacher model across all evaluation metrics this is one of the other upsides of model compression if your base model if your teacher model is overparameterized meaning that it has way too many internal parameters relative to the task that it’s trying to achieve actually compressing the model not only reduces the memory footprint but also it can lead to better performance because it’s removing a lot of the noisy and redundant structures(21:51) within the model but we can go one step further so we did knowledge distillation let’s see how we can quantize this model first I’ll push the student model to the hugging face Hub and then we’ll load it back in using the bits and bytes integration in the Transformers Library so we’ll use the bits and bytes config so we’ll load it in for bit we’ll use the normal float data type described in the Cur paper and all this is is a clever way of doing the quantization to take advantage that model parameters(22:21) tend to be normally distributed so you can be a bit more clever in how you quantize the values and I talk more about that in the Cur video that I mentioned earlier next we’ll set the compute data type as brain float 16 and then finally we’ll do double quantization which is another thing described in the Cur paper once we set up this config we can simply just load in our student model from the hugging face hub using this config file so the result of that is we have still the same number of parameters 52.8 million but(22:54) we’ve reduced the memory footprint so we went from 21 megab down to 62.7 megab then comparing that to our original teacher model we started with we cut the number of model parameters in half and then we reduced the memory footprint by about 7x but we’re not done yet so just cuz we reduced the model size that doesn’t mean that we still maintain the performance so now let’s evaluate the performance of the quantize model here we see we actually get another performance game post quantization so intuitively we can understand this(23:25) through the aam’s razor principle which says that simpler models are better so this might be indicating that there’s even more opportunity in knowledge distillation for this specific task all right so that brings us to the end if you enjoyed this video and you want to learn more check out the blog in towards data science and although this is a member only story like all my other videos you can access it completely for free using the friend Link in the description below additionally if you enjoyed this video you may enjoy the(23:55) other videos in my llm series and you can check those out by clicking on on the playlist linked here and as always thank you so much for your time and thanks for watchingImage List" }, { "title": "tasks-in-speech-audio-processing", "url": "/posts/tasks-in-speech-audio-processing/", "categories": "AI, Audio", "tags": "ai, speech, audio, processing, tasks", "date": "2024-09-06 10:47:14 -0400", "snippet": "Here are explanations for each of the tasks you mentioned related to speech and audio processing:Text-to-Speech (TTS)Text-to-Speech is the process of converting written text into spoken audio outpu...", "content": "Here are explanations for each of the tasks you mentioned related to speech and audio processing:Text-to-Speech (TTS)Text-to-Speech is the process of converting written text into spoken audio output. It involves generating natural-sounding speech from text input. TTS systems use various techniques like concatenative synthesis, formant synthesis, and neural networks to produce the audio. Applications include audiobook creation, voice assistants, and accessibility for visually impaired users.Text-to-AudioText-to-Audio is similar to Text-to-Speech, but the output is not necessarily human-like speech. It refers to generating any type of audio output from text input. This could include creating sound effects, background music, or other non-speech audio based on textual descriptions. It requires mapping text to appropriate audio samples or synthesizing the audio from scratch.Automatic Speech Recognition (ASR)Automatic Speech Recognition, also known as Speech-to-Text (STT), converts spoken audio into written text. ASR systems use acoustic and language models to transcribe speech. They analyze the acoustic features of the audio and match them to language units like phonemes, words and phrases. ASR enables applications like voice commands, transcription, and voice search.Audio-to-AudioAudio-to-Audio refers to transforming one type of audio into another. This could include: Audio enhancement: Improving audio quality by reducing noise, echoes, etc. Voice conversion: Converting one speaker’s voice into another’s Audio style transfer: Changing audio characteristics like emotion, accent, etc. Audio translation: Translating speech from one language to another Audio synthesis: Generating new audio from scratch based on input audioAudio ClassificationAudio Classification involves identifying the category or type of audio content. It can classify audio at different levels: Acoustic event classification: Recognizing sounds like dog barks, car horns, etc. Environmental sound classification: Identifying soundscapes like city streets, forests, etc. Music genre classification: Categorizing music by genre like rock, pop, jazz, etc. Speaker identification: Recognizing who is speaking based on their voice Emotion recognition: Detecting emotions like happy, sad, angry, etc. from speechVoice Activity Detection (VAD)Voice Activity Detection is the process of identifying the presence of human speech in an audio stream. It distinguishes speech regions from non-speech regions like silence, music or noise. VAD is used as a pre-processing step in many speech applications to improve performance. It helps focus processing only on the speech segments and ignore irrelevant audio.In summary, these tasks cover the key areas of speech and audio processing, from converting text to speech, recognizing speech, modifying audio characteristics, classifying audio content, and detecting speech regions in audio streams. They enable a wide range of applications in voice interfaces, audio analysis, and multimedia processing." }, { "title": "evaluation-ai-tasks", "url": "/posts/evaluation-ai-tasks/", "categories": "AI, Evaluation", "tags": "ai, evaluation, tasks", "date": "2024-09-06 10:27:21 -0400", "snippet": "The evaluation of AI tasks, particularly for large language models (LLMs), encompasses several benchmarks designed to assess different aspects of model performance. Here are key evaluation tasks in...", "content": "The evaluation of AI tasks, particularly for large language models (LLMs), encompasses several benchmarks designed to assess different aspects of model performance. Here are key evaluation tasks including ARC, HellaSwag, MMLU, Truthful QA, and CommonGEN V2:1. AI2 Reasoning Challenge (ARC)ARC is a multiple-choice test aimed at evaluating scientific reasoning and understanding. It measures a model’s ability to solve scientific problems, focusing on complex reasoning and problem-solving skills. The primary metric is the accuracy of the model’s answers, reflecting its capability to apply scientific principles effectively.2. HellaSwagHellaSwag is a benchmark for commonsense reasoning, designed to evaluate how well models can predict likely scenarios based on given contexts. It presents a challenging task where models must choose the correct completion from several options, with human performance significantly outpacing that of initial models. HellaSwag has evolved to push the boundaries of commonsense reasoning in LLMs, with recent models like GPT-4 achieving human-level performance under specific conditions.3. Massive Multitask Language Understanding (MMLU)MMLU assesses language comprehension across a wide range of topics in a multiple-choice format. This benchmark is crucial for evaluating the versatility of LLMs and their ability to perform across various domains. The evaluation focuses on overall accuracy and domain-specific performance, highlighting strengths and weaknesses in different areas of knowledge.4. Truthful QATruthful QA is a benchmark that evaluates the truthfulness and factual accuracy of model responses. In this multiple-choice format, models must select the most accurate answer from provided options, emphasizing their ability to discern truthfulness within a constrained framework. The primary metric is the accuracy of the model’s selections, assessing consistency with known facts.5. CommonGEN V2CommonGEN V2 is a benchmark that tests LLMs on their ability to generate contextually and culturally relevant outputs based on given conditions. This task is particularly focused on commonsense generation, where models must create coherent sentences that reflect common knowledge and understanding within a specific cultural context.These benchmarks are integral to advancing the capabilities of LLMs, providing a structured approach to evaluate their performance across various tasks and ensuring continuous improvement in AI development." }, { "title": "Open Source projects for TTS", "url": "/posts/opensource-tts/", "categories": "AI, TTS", "tags": "tts, open source, audio, speech", "date": "2024-09-05 23:56:17 -0400", "snippet": "Here’s an overview of some of the best open-source text-to-speech (TTS) AI projects available on GitHub and Hugging Face, including their pros, cons, benchmark scores, and hardware requirements.1. ...", "content": "Here’s an overview of some of the best open-source text-to-speech (TTS) AI projects available on GitHub and Hugging Face, including their pros, cons, benchmark scores, and hardware requirements.1. Coqui TTS Overview: Coqui TTS supports multiple languages and features like voice cloning and fine-tuning. Pros: High performance with deep learning models. Supports over 30 languages. Flexible training options and pre-trained models available. Cons: Voice cloning capabilities may not match the latest proprietary solutions. Requires significant data for training custom voices. Benchmark Score: Generally high-quality outputs, but specific scores vary based on model and dataset used. Hardware Requirements: Minimum: NVIDIA GPU (CUDA support recommended) for optimal performance. Recommended: 8GB RAM or more, with higher-end GPUs for faster training and synthesis. 2. Mozilla TTS Overview: A deep learning-based TTS engine that creates realistic speech. Pros: Fully customizable and supports multiple languages. Strong community support and documentation. Cons: May require extensive training data for high-quality results. Setup can be complex for beginners. Benchmark Score: Produces high-quality speech synthesis, competitive with commercial offerings. Hardware Requirements: Minimum: 8GB RAM, NVIDIA GPU recommended for training. Recommended: More than 16GB RAM for larger datasets. 3. ESPnet TTS Overview: Part of the broader ESPnet project, it offers state-of-the-art models. Pros: Supports multiple advanced models like Tacotron and FastSpeech. Good for research and experimentation. Cons: Complexity in setup and usage. Requires significant computational resources for training. Benchmark Score: High-quality outputs, especially with Tacotron and FastSpeech models. Hardware Requirements: Minimum: 16GB RAM and a decent GPU. Recommended: High-end GPUs (NVIDIA RTX series) for efficient training. 4. NVIDIA NeMo Overview: A collection of pre-trained models and pipelines for TTS. Pros: High-quality speech synthesis with FastPitch and HiFi-GAN. Optimized for NVIDIA hardware, offering GPU acceleration. Cons: Primarily designed for NVIDIA hardware, limiting compatibility. Can be complex for users unfamiliar with NVIDIA’s ecosystem. Benchmark Score: Excellent quality, often leading in benchmarks for speech synthesis. Hardware Requirements: Minimum: NVIDIA GPU (CUDA support required). Recommended: 16GB RAM or more, with high-end NVIDIA GPUs for best performance. 5. Tacotron 2 Overview: A well-known model for generating high-quality human-like speech. Pros: Produces natural-sounding speech. Can be paired with various vocoders for enhanced audio quality. Cons: Older model, with newer alternatives available. Requires significant training data for optimal results. Benchmark Score: High-quality outputs, but newer models may outperform it. Hardware Requirements: Minimum: 8GB RAM, NVIDIA GPU recommended. Recommended: 16GB RAM for larger datasets. 6. OpenTTS Overview: A lightweight TTS engine that supports multiple backends. Pros: Easy to deploy and set up. Supports various TTS backends, providing flexibility. Cons: Limited features compared to more advanced models. Quality may vary depending on the chosen backend. Benchmark Score: Generally lower than more specialized models. Hardware Requirements: Minimum: Basic CPU and RAM (4GB). Recommended: 8GB RAM for better performance. 7. VITS (Conditional Variational Autoencoder TTS) Overview: A newer model that combines variational autoencoders for fast TTS. Pros: High-quality, fast synthesis. Suitable for real-time applications. Cons: Newer and may have less community support. Requires good training data for best results. Benchmark Score: Competitive with the best in the field, especially for real-time applications. Hardware Requirements: Minimum: 8GB RAM, NVIDIA GPU recommended. Recommended: High-end GPU for optimal performance. These open-source TTS projects provide a variety of options for developers looking to implement text-to-speech capabilities in their applications. Depending on your specific needs, Coqui TTS and Tacotron 2 are strong candidates for high-quality speech synthesis." }, { "title": "Understanding valid parameters of ollama", "url": "/posts/valid-parameters-ollama/", "categories": "AI, LLM", "tags": "ollama, parameters, basics", "date": "2024-09-05 23:37:31 -0400", "snippet": "Detailed Parameter Descriptions and Use Cases1. mirostat Description: Enables Mirostat sampling, which helps control the perplexity of the generated text. This is important for maintaining the qua...", "content": "Detailed Parameter Descriptions and Use Cases1. mirostat Description: Enables Mirostat sampling, which helps control the perplexity of the generated text. This is important for maintaining the quality and coherence of the output. Value Type: Integer Possible Values: 0: Disabled 1: Mirostat 2: Mirostat 2.0 Example Usage: mirostat 1 Use Case: Use Mirostat when you want to dynamically adjust the generation process based on the complexity of the text, ensuring that the output remains coherent while still being creative.2. mirostat_eta Description: Influences the learning rate of the Mirostat algorithm. A lower value results in slower adjustments to the generated text, while a higher value makes the algorithm more responsive to feedback. Value Type: Float Default Value: 0.1 Example Usage: mirostat_eta 0.1 Use Case: Set a higher mirostat_eta if you want the model to quickly adapt to the style and tone of the text being generated, which is useful in interactive applications like chatbots.3. mirostat_tau Description: Controls the balance between coherence and diversity in the generated output. A lower value leads to more focused and coherent text, while a higher value allows for more diverse responses. Value Type: Float Default Value: 5.0 Example Usage: mirostat_tau 5.0 Use Case: Use a lower mirostat_tau in applications where clarity and coherence are critical, such as summarization tasks, and a higher value for creative writing tasks where diversity is desired.4. num_ctx Description: Sets the size of the context window used to generate the next token. A larger context window allows the model to consider more previous tokens, improving coherence. Value Type: Integer Default Value: 2048 Example Usage: num_ctx 4096 Use Case: Increase num_ctx for complex tasks that require understanding of longer contexts, such as story generation or dialogue systems.5. repeat_last_n Description: Determines how far back the model looks to prevent repetition in the output. A value of 0 disables this feature, while -1 sets it to the context size. Value Type: Integer Default Value: 64 Example Usage: repeat_last_n 64 Use Case: Use this parameter to avoid repetitive phrases in creative writing or dialogue generation, ensuring more engaging and varied output.6. repeat_penalty Description: Sets the strength of the penalty applied to repeated tokens. A higher value penalizes repetitions more strongly, while a lower value is more lenient. Value Type: Float Default Value: 1.1 Example Usage: repeat_penalty 1.1 Use Case: Increase repeat_penalty in scenarios where you want to ensure unique and diverse responses, such as in creative writing or generating dialogue for characters.7. temperature Description: Controls the randomness of the model’s output. A higher temperature leads to more creative and varied responses, while a lower temperature yields more deterministic outputs. Value Type: Float Default Value: 0.8 Example Usage: temperature 0.7 Use Case: Use a higher temperature for brainstorming sessions or creative writing, and a lower temperature for tasks requiring precision, like coding assistance or factual responses.8. seed Description: Sets the random number seed for generation. Using a specific seed allows for reproducible results, meaning the same input will yield the same output each time. Value Type: Integer Default Value: 0 Example Usage: seed 42 Use Case: Set a specific seed when testing or debugging to ensure consistent results across runs, which is particularly useful in research and development.9. stop Description: Defines stop sequences that, when encountered, will halt the text generation process. Multiple stop patterns can be specified. Value Type: String Example Usage: stop \"AI assistant:\" Use Case: Use stop sequences in interactive applications to control when the model should stop generating text, such as in chatbots or interactive storytelling.10. tfs_z Description: Tail-free sampling reduces the influence of less probable tokens in the output. A higher value reduces this impact, while a value of 1.0 disables the feature. Value Type: Float Default Value: 1 Example Usage: tfs_z 1.5 Use Case: Adjust tfs_z when you want to focus on generating higher-quality text by filtering out less likely tokens, which can be beneficial in formal writing or technical documentation.11. num_predict Description: Specifies the maximum number of tokens to predict during text generation. A value of -1 allows for infinite generation, while -2 fills the context. Value Type: Integer Default Value: 128 Example Usage: num_predict 42 Use Case: Set this parameter to control the length of generated responses, such as limiting answers in a Q&amp;A system or extending responses in creative writing.12. top_k Description: Reduces the probability of generating nonsensical outputs by limiting the number of tokens considered. A higher value allows for more diverse answers, while a lower value is more conservative. Value Type: Integer Default Value: 40 Example Usage: top_k 100 Use Case: Use a higher top_k for creative tasks where diversity is desired, and a lower value for tasks requiring more focused and coherent responses.13. top_p Description: Works in conjunction with top_k to control the diversity of the text generated. A higher value results in more varied outputs, while a lower value yields more focused responses. Value Type: Float Default Value: 0.9 Example Usage: top_p 0.95 Use Case: Adjust top_p for applications needing a balance between creativity and coherence, such as dialogue generation or storytelling.14. min_p Description: An alternative to top_p, this parameter ensures a balance between quality and variety by filtering out tokens below a certain probability threshold. Value Type: Float Default Value: 0.0 Example Usage: min_p 0.05 Use Case: Use min_p when you want to ensure that only high-probability tokens are considered, which can be beneficial in applications requiring high-quality outputs, like formal writing or technical documentation.ConclusionThese parameters provide fine control over the behavior of language models, allowing users to tailor the output to their specific needs. Adjusting these settings can significantly impact the quality, coherence, and creativity of the generated text. For further insights, you may want to check the video you referenced, which likely provides additional context and examples on how these parameters affect model performance." }, { "title": "Small LLM Comparison in 2024", "url": "/posts/llm-comparison-2024/", "categories": "AI, LLM", "tags": "ai, llm, comparison, 2024", "date": "2024-09-04 06:21:40 -0400", "snippet": "Timeline of major LLM developments from 2019 to 2024, here are the top 3 Large Language Models (LLMs) in various domains, their key specifications, and performance suitability: Domain ...", "content": "Timeline of major LLM developments from 2019 to 2024, here are the top 3 Large Language Models (LLMs) in various domains, their key specifications, and performance suitability: Domain Model Name Developer General Q&amp;A (Text Generation) Mistral 7B Mistral AI (2023)   LLaMA 2 (7B) Meta (2023)   Phi-3-Mini Microsoft (2024) Coding Phi-3-Small Microsoft (2024)   DeciCoder-1B Deci (2023)   GPT-2 Small OpenAI (2020) Math Dolly-v2-3B Databricks (2023)   Phi-1.5 Microsoft (2024)   LLaMA 2 (7B) Meta (2023) Image Understanding Dolly-v2-3B Databricks (2023)   LLaMA 2 (7B) Meta (2023)   Phi-3-Mini Microsoft (2024) Translation Mistral 7B Mistral AI (2023)   Phi-3-Small Microsoft (2024)   LLaMA 2 (7B) Meta (2023) 1. General Q&amp;A (Text Generation)Mistral 7B (2023 by Mistral AI) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 4GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 20GB Benchmark: Outperforms Llama 2 13B on all benchmarks; limited context window of 8,000 tokens.Phi-3-Mini (2024 by Microsoft) Technical Specs and Hardware Requirements: Parameters: 3 billion RAM: 4GB (smartphones), 6GB (PCs) CPU: Snapdragon 855 or higher Storage: 10GB Benchmark: Fast response times, limited factual recall.LLaMA 2 (7B) (2023 by Meta) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 6GB (smartphones) CPU: Snapdragon 855 or higher Storage: 8GB Benchmark: Slower on-device performance compared to Mistral 7B, efficient for basic tasks.2. CodingDeciCoder-1B (2023 by Deci) Technical Specs and Hardware Requirements: Parameters: 1 billion RAM: 2GB (smartphones), 4GB (PCs) CPU: Snapdragon 855 or higher Storage: 2GB Benchmark: Great for basic coding tasks, runs well on low resources.Phi-3-Small (2024 by Microsoft) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 4GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 20GB Benchmark: Fast and efficient in code understanding.GPT-2 Small (2020 by OpenAI) Technical Specs and Hardware Requirements: Parameters: 117 million RAM: 2GB (smartphones), 4GB (PCs) CPU: Snapdragon 660 or higher Storage: 1GB Benchmark: Basic coding capabilities, low-resource requirements.3. MathPhi-1.5 (2024 by Microsoft) Technical Specs and Hardware Requirements: Parameters: 1.5 billion RAM: 3GB (smartphones), 6GB (PCs) CPU: Snapdragon 845 or higher Storage: 15GB Benchmark: Fast performance, struggles with complex math problems.Dolly-v2-3B (2023 by Databricks) Technical Specs and Hardware Requirements: Parameters: 3 billion RAM: 4GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 25GB Benchmark: High accuracy but higher RAM usage.LLaMA 2 (7B) (2023 by Meta) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 6GB (smartphones) CPU: Snapdragon 855 or higher Storage: 8GB Benchmark: Efficient for basic math, struggles with advanced calculations.4. Image UnderstandingLLaMA 2 (7B) (2023 by Meta) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 6GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 10GB Benchmark: Basic image understanding tasks, works on mid-tier smartphones.Dolly-v2-3B (2023 by Databricks) Technical Specs and Hardware Requirements: Parameters: 3 billion RAM: 4GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 20GB Benchmark: Decent performance in image-to-text tasks but requires more RAM.Phi-3-Mini (2024 by Microsoft) Technical Specs and Hardware Requirements: Parameters: 3 billion RAM: 4GB (smartphones), 6GB (PCs) CPU: Snapdragon 855 or higher Storage: 10GB Benchmark: Works well with basic image recognition but limited context handling.5. Translation (Chinese, Japanese)LLaMA 2 (7B) (2023 by Meta) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 6GB (smartphones) CPU: Snapdragon 855 or higher Storage: 10GB Benchmark: Efficient for on-device translations but limited language precision.Phi-3-Small (2024 by Microsoft) Technical Specs and Hardware Requirements: Parameters: 3 billion RAM: 4GB (smartphones), 6GB (PCs) CPU: Snapdragon 855 or higher Storage: 8GB Benchmark: Good for basic translations but slower in complex phrases.Mistral 7B (2023 by Mistral AI) Technical Specs and Hardware Requirements: Parameters: 7 billion RAM: 4GB (smartphones), 8GB (PCs) CPU: Snapdragon 855 or higher Storage: 20GB Benchmark: Outperforms Llama 2 13B on all benchmarks; limited context window of 8,000 tokens; good for basic translation tasks, handles multiple languages decently." }, { "title": "Timeline of LLM Development", "url": "/posts/timeline-llm-development/", "categories": "AI, LLM", "tags": "timeline, llm", "date": "2024-09-04 05:02:30 -0400", "snippet": "Here is a detailed timeline of major developments in Large Language Models (LLMs) from 2019 to 2025, including specific model names, their release dates, specifications, abilities, best suitable do...", "content": "Here is a detailed timeline of major developments in Large Language Models (LLMs) from 2019 to 2025, including specific model names, their release dates, specifications, abilities, best suitable domains, and hardware requirements. This timeline covers over 30 popular models across text, image, and video modalities.Timeline of Major LLM Developments (2019-2024)2019 GPT-2 Developer: OpenAI Release Date: February 2019 Parameters: 1.5 billion Abilities: Text generation, summarization, translation. Best Suitable Domain: Content creation, chatbots. Hardware Specs: Requires high-end CPUs/GPUs; 16GB RAM recommended. Support Languages: English 2020 GPT-3 Developer: OpenAI Release Date: June 2020 Parameters: 175 billion Abilities: Advanced text generation, conversation, code generation. Best Suitable Domain: Creative writing, programming assistance. Hardware Specs: High-performance GPUs (NVIDIA A100 recommended); 32GB+ RAM. Support Languages: English 2021 Codex Developer: OpenAI Release Date: August 2021 Parameters: 12 billion (based on GPT-3) Abilities: Code generation, code completion. Best Suitable Domain: Software development, IDE integration. Hardware Specs: High-end CPUs/GPUs; 16GB RAM recommended. Support Languages: English 2022 ChatGPT Developer: OpenAI Release Date: November 2022 Parameters: Based on GPT-3.5 Abilities: Conversational AI, text generation. Best Suitable Domain: Customer support, personal assistants. Hardware Specs: Cloud-based; requires high-performance GPUs. Support Languages: English LLaMA Developer: Meta Release Date: February 2022 Parameters: 7B, 13B, 30B, 65B Abilities: Text generation and understanding. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 16GB+ RAM recommended. Support Languages: Multilingual (including English) DALL-E 2 Developer: OpenAI Release Date: April 2022 Parameters: Unknown Abilities: Image generation from text prompts. Best Suitable Domain: Art creation, marketing. Hardware Specs: High-performance GPUs; 32GB RAM recommended. Support Languages: English 2023 Claude Developer: Anthropic Release Date: March 2023 Parameters: Unknown Abilities: Conversational AI with a focus on safety. Best Suitable Domain: Customer service, chatbots. Hardware Specs: Cloud-based; requires high-performance GPUs. Support Languages: English Toolformer Developer: Unknown Release Date: February 2023 Parameters: Unknown Abilities: Generates its own training data. Best Suitable Domain: Research and model training. Hardware Specs: Standard server configurations. Support Languages: English Jurassic-2 Developer: AI21 Labs Release Date: March 2023 Parameters: 178 billion Abilities: Text generation and code generation. Best Suitable Domain: Creative writing and programming assistance. Hardware Specs: High-performance GPUs; 32GB RAM recommended. Support Languages: English Falcon 40B Developer: Technology Innovation Institute Release Date: March 2023 Parameters: 40 billion Abilities: Text generation and understanding. Best Suitable Domain: General NLP tasks. Hardware Specs: High-end GPUs; 24GB RAM recommended. Support Languages: English Tongyi Qianwen Developer: Alibaba Release Date: September 2023 Parameters: Unknown Abilities: Multilingual text generation. Best Suitable Domain: E-commerce and customer service. Hardware Specs: Cloud-based; requires high-performance GPUs. Support Languages: English and Chinese Gemini Developer: Google DeepMind Release Date: September 2023 Parameters: 7-10 trillion Abilities: Multimodal processing (text and images). Best Suitable Domain: AI applications across various industries. Hardware Specs: Requires advanced GPUs and substantial memory. Support Languages: English Mistral 7B Developer: Mistral AI Release Date: September 2023 Parameters: 7 billion Abilities: Text generation and understanding. Best Suitable Domain: Research and academic applications. Hardware Specs: Standard server configurations. Support Languages: English 2024 GPT-4o Developer: OpenAI Release Date: May 2024 Parameters: Unknown Abilities: Enhanced natural language understanding and generation. Best Suitable Domain: General NLP tasks. Hardware Specs: High-performance GPUs; 32GB+ RAM recommended. Support Languages: English Claude 3 Developer: Anthropic Release Date: March 2024 Parameters: Unknown Abilities: Improved conversational AI. Best Suitable Domain: Customer service and chatbots. Hardware Specs: Cloud-based; requires high-performance GPUs. Support Languages: English Llama 3 Developer: Meta Release Date: April 2024 Parameters: 8 billion, 70 billion Abilities: Text generation and understanding with improved reasoning capabilities. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 16GB+ RAM recommended. Support Languages: Multilingual (including English) Additional Notable ModelsLLaMA Models LLaMA Developer: Meta AI Release Date: February 2023 Parameters: 7B, 13B, 30B, 65B Abilities: Text generation and understanding. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 16GB+ RAM recommended. Support Languages: Multilingual (including English) LLaMA 2 Developer: Meta AI Release Date: July 2023 Parameters: 7B, 13B, 70B Abilities: Text generation and understanding with improved performance. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 16GB+ RAM recommended. Support Languages: Multilingual (including English) LLaMA 3 Developer: Meta AI Release Date: April 2024 Parameters: 8B, 70B Abilities: Text generation and understanding with improved reasoning capabilities. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 16GB+ RAM recommended. Support Languages: Multilingual (including English) Other Notable Models Mistral MoE Developer: Mistral AI Release Date: September 2023 Parameters: Unknown Abilities: Efficient text generation using MoE architecture. Best Suitable Domain: General NLP tasks. Hardware Specs: High-performance GPUs; 24GB RAM recommended. Support Languages: English DBRX Developer: Unknown Release Date: Unknown Parameters: Unknown Abilities: Text generation and understanding with advanced reasoning capabilities. Best Suitable Domain: Research and academic applications. Hardware Specs: High-end GPUs; 32GB+ RAM recommended. Support Languages: English Falcon Developer: Technology Innovation Institute Release Date: March 2023 Parameters: 40 billion Abilities: Text generation and understanding. Best Suitable Domain: General NLP tasks. Hardware Specs: High-end GPUs; 24GB RAM recommended. Support Languages: English Chinese LLaMA / Alpaca Developer: Alibaba Release Date: September 2023 Parameters: Unknown Abilities: Multilingual text generation in Chinese. Best Suitable Domain: E-commerce and customer service in Chinese-speaking regions. Hardware Specs: Cloud-based; requires high-performance GPUs. Support Languages: Chinese Vigogne (French) Developer: Unknown Release Date: Unknown Parameters: Unknown Abilities: Text generation and understanding in French. Best Suitable Domain: General NLP tasks in French-speaking regions. Hardware Specs: Standard server configurations. Support Languages: French Image and Video Models DALL-E 2 Developer: OpenAI Release Date: April 2022 Parameters: Unknown Abilities: Image generation from text prompts. Best Suitable Domain: Art creation, marketing. Hardware Specs: High-performance GPUs; 32GB RAM recommended. Support Languages: English Multimodal Models Gemini Developer: Google DeepMind Release Date: September 2023 Parameters: 7-10 trillion Abilities: Multimodal processing (text and images). Best Suitable Domain: AI applications across various industries. Hardware Specs: Requires advanced GPUs and substantial memory. Support Languages: English ConclusionThis timeline highlights significant advancements in LLMs from 2019 to 2025, showcasing key models and their impact on various domains. As the technology continues to evolve, we can expect further innovations and applications across text, image, and video processing. The future holds promise for even more sophisticated models that can generalize better and integrate multiple modalities seamlessly.Additional ConsiderationsHardware Requirements CPU/GPU Requirements: Most LLMs require high-performance CPUs/GPUs like NVIDIA A100 or AMD Radeon Instinct. Memory Requirements: Models typically require substantial memory ranging from 16GB to 32GB or more depending on the complexity of the model. Storage Requirements: Large datasets used for training these models require significant storage capacity.Ethical Considerations Safety Features: Many models include safety features such as Llama Guard 2 which uses MLCommons taxonomy for prompt and response safety. Responsible Use Guide: Developers are encouraged to follow responsible use guidelines to mitigate potential harms associated with AI deployment.ConclusionThe landscape of LLMs has evolved dramatically from 2019 to 2025, driven by advancements in architecture and applications across various industries. As we move forward, the focus will likely be on improving efficiency, enhancing multimodal capabilities, and addressing ethical considerations in AI deployment. The future holds promise for even more sophisticated models that can generalize better and integrate multiple modalities seamlessly." }, { "title": "Ollama vs llama.cpp", "url": "/posts/ollama-vs-llama.cpp/", "categories": "AI, LLM", "tags": "ollama, llama.cpp, comparison", "date": "2024-09-04 04:41:48 -0400", "snippet": "When deciding between llama.cpp and ollama for running large language models (LLMs) locally, several factors should be considered. Here’s a detailed comparison of the two tools:Performance Speed C...", "content": "When deciding between llama.cpp and ollama for running large language models (LLMs) locally, several factors should be considered. Here’s a detailed comparison of the two tools:Performance Speed Comparison: llama.cpp is generally faster than ollama. In one benchmark, llama.cpp ran 1.8 times faster than ollama when processing the same quantized model on a GPU[1]. This difference is attributed to various factors, including memory calculations and layer offloading.Ease of Use User-Friendliness: ollama is designed to be more user-friendly, automating many aspects of model management and deployment. It simplifies tasks like chat request templating, dynamic model loading, and caching[2][3]. Documentation and Support: While ollama is easier to use, the documentation for both tools is limited compared to commercial solutions. However, ollama provides a more straightforward interface for beginners[3].Customization and Control Granular Control: llama.cpp offers more granular control over AI models, appealing to users who prioritize detailed customization and deep technical engagement. It allows for interactive mode, prompt files, and customizable parameters for token prediction length and repeat penalty[2][3]. Model Support: Both tools support multiple AI models, but llama.cpp currently supports 37 different models, including LLaMA, Vicuna, and Alpaca[3].Hardware Support Hardware Agnosticism: Both tools can run on a variety of hardware configurations, from CPUs to GPUs. However, llama.cpp leverages various quantization techniques to reduce the model size and memory footprint while maintaining acceptable performance[2][3]. GPU Optimization: llama.cpp can offload layers entirely to the GPU, which can significantly improve performance. In contrast, ollama might err on the conservative side with GPU offloading, potentially leading to slower performance[1].Integration and Maintenance Integration Challenges: Integrating either tool into existing enterprise systems and workflows may require significant development effort and customization. However, llama.cpp can be hooked up directly to OpenAI-compatible plugins and applications without needing wrappers[1]. Maintenance and Updates: As community-driven projects, both tools rely on community support. Enterprises may need to invest in in-house expertise or rely on community resources for troubleshooting and maintenance[3].ConclusionWhen to Use Each Tool Use llama.cpp: If you need granular control over your AI models. If you prioritize detailed customization and technical engagement. If you require efficient, hardware-agnostic solutions for running LLMs. Use ollama: If you seek a more user-friendly experience with automated model management and deployment. If you want to simplify tasks related to chat requests, dynamic model loading, and caching. If you prefer a straightforward interface without needing extensive technical expertise. Ultimately, the choice between llama.cpp and ollama depends on your specific needs—whether you prioritize performance, ease of use, or customization. Both tools offer unique strengths that can be leveraged depending on your project requirements." }, { "title": "Understanding RISC-V and Manufacture", "url": "/posts/understanding-riscv-and-manufacture/", "categories": "CS, CPU", "tags": "riscv, architecture, cpu, basics", "date": "2024-09-04 02:15:22 -0400", "snippet": "Jim Keller, a prominent figure in CPU design, has been vocal about the potential of RISC-V architecture, especially in comparison to proprietary architectures like Apple’s Silicon. Here’s an overvi...", "content": "Jim Keller, a prominent figure in CPU design, has been vocal about the potential of RISC-V architecture, especially in comparison to proprietary architectures like Apple’s Silicon. Here’s an overview of the key differences and considerations regarding RISC-V and Apple Silicon CPUs.Jim Keller’s Advocacy for RISC-V1. Open-Source Architecture RISC-V is an open-source instruction set architecture (ISA) that allows for greater flexibility and customization. Keller believes that this openness fosters innovation and reduces dependency on proprietary technologies, which can stifle progress and lead to dead ends in development[2][3].2. Future Potential Keller predicts that RISC-V could dominate data centers within the next 5 to 10 years, especially for high-performance computing (HPC) and scientific applications. He emphasizes that RISC-V’s simplicity and the ability to optimize for specific applications make it an attractive option for future computing needs[1][6].3. Customization and Licensing Companies can build their own RISC-V processors tailored to their specific needs, which is a significant advantage for businesses looking to innovate without being locked into a single vendor’s ecosystem. This flexibility is appealing to organizations that want to maintain control over their technology stack[2][3].Apple Silicon1. Proprietary Architecture Apple Silicon, which includes chips like the M1 and M2, is based on ARM architecture and is proprietary. Apple designs these chips to optimize performance and efficiency specifically for its ecosystem of products, including Macs, iPads, and iPhones. This tight integration allows for enhanced performance, battery life, and user experience[1].2. Performance and Efficiency Apple Silicon has been praised for its performance, particularly in tasks that require high computational power. The architecture is designed to deliver superior performance per watt, making it ideal for mobile and portable devices. This efficiency has allowed Apple to create devices that perform exceptionally well while maintaining long battery life[1].3. Vertical Integration By controlling both hardware and software, Apple can ensure that its operating systems (like macOS and iOS) are finely tuned to work with its chips. This vertical integration leads to a seamless user experience and allows Apple to introduce features that leverage the capabilities of its hardware[1].ConclusionWhile Jim Keller advocates for RISC-V as a promising alternative that could reshape the computing landscape through its openness and customization potential, Apple Silicon has established itself as a powerful and efficient proprietary solution tailored for Apple’s ecosystem. The choice between RISC-V and Apple Silicon ultimately depends on the specific needs of developers and companies—whether they prioritize flexibility and innovation (RISC-V) or performance and integration (Apple Silicon). Both architectures represent significant advancements in CPU design, each catering to different segments of the market.The RISC-V architecture, being open-source, allows for a wide range of countries and companies to develop their own CPUs based on it. Here are some key points regarding which entities can build RISC-V CPUs and what capabilities are required:Countries Developing RISC-V CPUs China has been a major proponent of RISC-V, with companies like Alibaba, Huawei, and Tencent developing a range of domestic RISC-V processors. Over 50% of the 10 billion RISC-V cores manufactured in 2022 came from China[1]. India is also investing in RISC-V, with the Centre for Development of Advanced Computing (C-DAC) developing a series of RISC-V processors under the VEGA Microprocessors project[3]. European Union is providing 270 million euros to support a RISC-V CPU development project aimed at servers and data centers, to reduce dependence on foreign architectures[3]. Key Companies Driving RISC-V SiFive, a U.S. company, is a major RISC-V IP provider. Chinese companies like SophGo are licensing SiFive’s RISC-V cores to develop high-performance processors[2]. Alibaba’s T-Head Semiconductor has played a significant role in promoting RISC-V through open-source projects and innovative products like the XuanTie series processors[4]. Espressif Systems, a Chinese company, focuses on wireless SoC chips based on RISC-V, such as the ESP32-C series with Wi-Fi 6 and Bluetooth 5[4]. Technical Capabilities Required Chip design expertise: Developing RISC-V CPUs requires strong chip design capabilities, including knowledge of RTL design, verification, and physical implementation. Process technology: Fabricating RISC-V chips requires access to advanced semiconductor process nodes, typically provided by foundries like TSMC. Both SophGo and Alibaba are using TSMC’s 12nm process for their RISC-V designs[2][4]. EDA tools: Companies need access to electronic design automation (EDA) tools for design, simulation, and verification. Major EDA providers like Synopsys and Cadence support RISC-V[4]. IP blocks: While RISC-V is open-source, companies can license additional IP blocks like memory controllers, interconnects, and accelerators to enhance their RISC-V designs. SiFive and others provide such IP[2][3]. In summary, the open-source nature of RISC-V allows a diverse set of countries and companies to develop their own CPUs, provided they have the necessary chip design expertise, process technology access, EDA tools, and supporting IP. China and India are actively investing in RISC-V, while the EU aims to leverage it for digital sovereignty. However, significant technical capabilities are still required to successfully design and manufacture RISC-V chips.If Jim Keller were to open-source the RISC-V architecture, he would indeed lose direct control over its development and future direction. However, there are significant benefits to making it an open-source project:Benefits of Open-Sourcing RISC-V Accelerated Innovation: By opening up the architecture to a wider community, RISC-V would benefit from contributions, ideas, and optimizations from developers around the world. This collaborative approach often leads to faster innovation and improvements to the core architecture. Ecosystem Growth: An open-source RISC-V would attract more companies and individuals to adopt and build upon the platform. This would expand the ecosystem of tools, libraries, and applications available for RISC-V, making it more attractive for broader adoption. Democratization of Chip Design: Open-sourcing RISC-V would lower the barriers to entry for chip design, allowing startups and smaller companies to develop custom processors tailored to their needs without the burden of licensing fees. This democratization of chip design could spur more innovation. Avoidance of Fragmentation: By maintaining RISC-V as an open standard, Keller could help prevent fragmentation of the architecture into incompatible variants. An open governance model with clear specifications would ensure interoperability and a cohesive ecosystem. Reputational Benefits: Keller would be seen as a pioneer in open-sourcing a major processor architecture, which could enhance his reputation and influence in the industry. He could also maintain leadership by guiding the open-source project’s direction. Potential Drawbacks Loss of Direct Control: As mentioned, Keller would lose the ability to unilaterally decide the future of RISC-V. The project would be guided by a community of contributors. Potential for Forks: There is always a risk of the project forking into incompatible variants if the open-source community disagrees on the direction. However, a well-designed governance model can mitigate this risk. Reduced Monetization Opportunities: Keller would forego opportunities to directly monetize RISC-V through licensing fees. However, he could still benefit indirectly through consulting, support services, and other business models built around the open-source project. In summary, while open-sourcing RISC-V would mean relinquishing direct control, the benefits of accelerated innovation, ecosystem growth, democratization of chip design, and enhanced reputation could outweigh the drawbacks. Keller could still play a leadership role in guiding the open-source project’s direction and benefit from the success of the architecture." }, { "title": "comparision-arm64-amd64", "url": "/posts/comparision-arm64-amd64/", "categories": "CS, CPU", "tags": "arm64, amd64, architecture, basics", "date": "2024-09-04 01:54:43 -0400", "snippet": "The comparison between AArch64 (also known as ARM64) and AMD64 (also referred to as x86-64) highlights significant differences in architecture, design philosophy, and application use cases. Here’s ...", "content": "The comparison between AArch64 (also known as ARM64) and AMD64 (also referred to as x86-64) highlights significant differences in architecture, design philosophy, and application use cases. Here’s an overview based on the search results.Architecture TypeAArch64 (ARM64) Design: AArch64 is part of the ARMv8 architecture and represents a Reduced Instruction Set Computing (RISC) design. This approach emphasizes a smaller set of instructions that are simple and execute in a single cycle, leading to greater efficiency and lower power consumption. Usage: Primarily used in mobile devices, embedded systems, and increasingly in servers and high-performance computing environments. It is known for its power efficiency, making it suitable for battery-operated devices and applications where thermal management is crucial. Instruction Set: Supports both 64-bit (AArch64) and 32-bit (AArch32) instruction sets, allowing for backward compatibility with older ARM architectures.AMD64 (x86-64) Design: AMD64 is an extension of the x86 architecture, which is based on Complex Instruction Set Computing (CISC). CISC architectures have a more extensive set of instructions, allowing for complex operations to be executed with fewer lines of code, but often at the cost of increased power consumption and complexity. Usage: Dominant in desktop and server markets, used in most personal computers and enterprise servers. It is optimized for high-performance computing tasks and supports a wide range of operating systems and applications. Instruction Set: Fully backward compatible with the x86 instruction set, allowing it to run both 32-bit and 64-bit applications seamlessly.Performance and Efficiency Power Efficiency: AArch64 is generally more power-efficient than AMD64, making it ideal for mobile and embedded applications where battery life is critical. In contrast, AMD64 processors are often optimized for maximum performance, which can lead to higher power consumption. Performance: While both architectures have converged in terms of performance in recent years, AMD64 typically excels in raw processing power, particularly for tasks requiring heavy computation. AArch64 implementations have improved significantly, especially in server and high-performance computing contexts. Compatibility and Ecosystem Software Compatibility: AMD64 has a vast ecosystem of software, particularly in desktop environments, due to its long-standing presence in the market. AArch64 is gaining traction, especially with the rise of ARM-based servers and devices, but it still has a smaller software ecosystem compared to AMD64. Operating Systems: Both architectures support major operating systems, including Linux, Windows, and macOS. However, the implementation and optimization of these operating systems can differ significantly between the two architectures. ConclusionIn summary, AArch64 and AMD64 serve different markets and applications, each with its strengths. AArch64 is favored for its power efficiency and suitability for mobile and embedded systems, while AMD64 remains the go-to architecture for high-performance desktop and server computing. The choice between the two often depends on specific application requirements, power considerations, and software ecosystem needs." }, { "title": "Understanding ARM", "url": "/posts/understanding-arm/", "categories": "CS, CPU", "tags": "arm, architecture, basics", "date": "2024-09-04 01:41:10 -0400", "snippet": "Here is a summary of the history and evolution of ARM architecture:Origins at Acorn Computers In the early 1980s, British company Acorn Computers needed a processor for their new Acorn Archimedes ...", "content": "Here is a summary of the history and evolution of ARM architecture:Origins at Acorn Computers In the early 1980s, British company Acorn Computers needed a processor for their new Acorn Archimedes computer that could deliver high performance while being energy efficient. This led to the creation of the first ARM processor, the ARM1, in 1985. It was designed by Sophie Wilson and Steve Furber. The ARM architecture was based on Reduced Instruction Set Computer (RISC) principles, using a small set of instructions that could be executed quickly. This made it simpler and more power-efficient compared to Complex Instruction Set Computer (CISC) designs at the time. The ARM2, released in 1987, added a Booth multiplier and a Fast Interrupt reQuest (FIQ) mode. It offered performance comparable to contemporary workstations while drawing less power.Early Licensees and Market Share Growth In 1990, ARM was officially founded as a joint venture between Acorn, Apple, and VLSI Technology. ARM introduced the IP licensing business model, allowing partners to license the ARM architecture and produce their own ARM-based chips. Early licensees included Texas Instruments, who used ARM in Nokia’s GSM mobile phones in the 1990s. By 2005, about 98% of mobile phones used at least one ARM processor. By 2010, ARM had shipped 6.1 billion processors, with 95% in smartphones.ARMv7 ArchitectureThe ARMv7 architecture, introduced in 2006, defines three main profiles: ARMv7-A (Application): Designed for high-performance applications like smartphones, tablets, and servers. It supports a 32-bit instruction set and virtual memory management. ARMv7-R (Real-Time): Optimized for real-time systems with deterministic behavior, such as industrial control and automotive applications. It supports a 32-bit instruction set and a Memory Protection Unit (MPU). ARMv7-M (Microcontroller): Focused on low-power, cost-sensitive embedded systems. It supports a 32-bit Thumb instruction set and a simplified programmers’ model compared to ARMv7-A and ARMv7-R. Notable ARMv7 processors include the Cortex-A8, Cortex-A9, Cortex-R4, and Cortex-M3.ARMv8 ArchitectureThe ARMv8 architecture, introduced in 2011, added support for 64-bit processing. It defines three main profiles: ARMv8-A (Application): Extends the ARMv7-A profile with a new 64-bit instruction set (AArch64) and 64-bit registers. It also retains support for the 32-bit instruction set (AArch32) for backward compatibility. ARMv8-R (Real-Time): Extends the ARMv7-R profile with some ARMv8-A features, including 64-bit support. ARMv8-M (Microcontroller): Extends the ARMv7-M profile with additional security features and support for the Thumb instruction set. Notable ARMv8-A processors include the Cortex-A53, Cortex-A57, and Cortex-A72. ARMv8-M processors include the Cortex-M23 and Cortex-M33.ARM64 (AArch64)ARM64, also known as AArch64, is the 64-bit instruction set architecture introduced with ARMv8-A. It provides: 31 general-purpose 64-bit registers 32 floating-point/SIMD 128-bit registers 4 GB exception levels for hardware virtualization and security 48-bit virtual address space (can be extended to 52 bits) Improved performance and energy efficiency compared to 32-bit ARMARM64 is used in high-performance applications like servers, HPC, and flagship mobile devices. Examples include the Cortex-A57, Cortex-A72, and Neoverse N1 processors.Expansion into New Markets ARM’s architecture expanded beyond mobile into markets like automotive, IoT, servers, and supercomputers. The Neoverse family targeted server and cloud computing workloads. By 2022, 65% of embedded IoT devices used ARM-based chips.Acquisition by SoftBank In 2016, ARM was acquired by Japanese conglomerate SoftBank for $32 billion. This took ARM private after nearly 20 years as a public company.In summary, ARM’s RISC-based architecture, IP licensing model, and energy-efficient designs have enabled it to grow from a British startup to a dominant force powering a wide range of computing devices globally. Its evolution has allowed it to adapt to the changing needs of the technology industry over the past 35+ years." }, { "title": "Understanding x86 and x86_64", "url": "/posts/understanding-x86-x86_64/", "categories": "CS, CPU", "tags": "cpu, architecture, basics", "date": "2024-09-04 01:35:49 -0400", "snippet": "x86 and x86_64 (also referred to as x64 or AMD64) are terms that describe different architectures used in computer processors. Here’s a detailed explanation of the differences, their history, and t...", "content": "x86 and x86_64 (also referred to as x64 or AMD64) are terms that describe different architectures used in computer processors. Here’s a detailed explanation of the differences, their history, and their evolution across various operating systems.Key Differences Architecture: x86: Refers to a 32-bit architecture, which was originally developed by Intel and includes processors like the 80386 and later models. It can address a maximum of 4 GB of RAM due to its 32-bit address space. x86_64: This is a 64-bit architecture that extends the x86 architecture, allowing for significantly larger memory addressing (theoretically up to 16 exabytes). It supports 64-bit registers, which can process more data at once compared to 32-bit registers. Compatibility: x86: Software compiled for x86 architecture can run on 32-bit systems. Some 64-bit systems can run x86 applications through compatibility layers (e.g., WOW64 in Windows). x86_64: Software compiled for x86_64 architecture can run on 64-bit systems. However, it is not compatible with 32-bit systems unless specifically compiled for that architecture. Performance: x86_64 architectures generally offer better performance due to larger registers and improved memory management capabilities. Historical Context and Evolution Origins of x86: The x86 architecture began with the Intel 8086 processor, released in 1978. It was originally a 16-bit architecture. The architecture evolved to 32-bit with the introduction of the 80386 in 1985, which allowed for more advanced computing capabilities. Development of x86_64: In the late 1990s, as applications and operating systems required more memory and processing power, AMD developed the x86_64 architecture, also known as AMD64, as an extension of the existing x86 instruction set. The first x86_64 processor, the AMD Opteron, was released in 2003. Intel followed suit with its own x86_64 processors (branded as Intel 64) in 2004. Cross-Licensing: Intel and AMD have cross-licensed each other’s technologies, allowing both companies to produce compatible processors that support the x86_64 architecture. Operating Systems and x86/x86_64 Windows: Windows has historically supported both x86 and x86_64 architectures. Windows 32-bit (x86) versions can run on 64-bit (x86_64) systems, but the reverse is not true without specific compatibility layers. As of Windows 11, Microsoft has dropped support for 32-bit processors. Linux: Most Linux distributions support both x86 and x86_64 architectures. Many distributions are moving away from supporting 32-bit versions due to declining usage. Users can check their architecture using commands like uname -m (returns x86_64 for 64-bit systems). macOS: macOS transitioned from supporting x86 to x86_64 with the introduction of Intel-based Macs in 2006. Apple has since moved to ARM-based architecture (Apple Silicon) starting in 2020. Android: Android primarily runs on ARM architecture, but it also supports x86 and x86_64 architectures for specific devices and emulators. Use Cases and Applications Software Development: Developers must choose the correct architecture when compiling applications to ensure compatibility with the target operating system. Gaming: Many modern games are optimized for x86_64, taking advantage of the increased memory and processing capabilities. Virtualization: Virtual machines can run both architectures, allowing users to run legacy applications on newer hardware.Common Commands and ToolsFor users working with systems that utilize x86 or x86_64 architectures, here are some common commands and tools: Check Architecture: On Linux: uname -m (returns x86_64 for 64-bit systems) On Windows: Use System Information or wmic os get osarchitecture in the command prompt. Installing Software: Use package managers like apt (Debian-based) or yum (Red Hat-based) to install software specific to the architecture. ConclusionUnderstanding the differences between x86 and x86_64 architectures is essential for optimizing software performance and ensuring compatibility across various operating systems. The evolution of these architectures has played a significant role in the advancement of computing technology, allowing for greater processing power and memory management capabilities. As the industry continues to evolve, the focus is shifting towards newer architectures like ARM, but x86 and x86_64 remain foundational in many computing environments today." }, { "title": "Understanding Termux", "url": "/posts/understanding-termux/", "categories": "Android, Termux", "tags": "android, termux, basics", "date": "2024-09-04 01:24:56 -0400", "snippet": "Overview of TermuxTermux is a free and open-source terminal emulator and Linux environment for Android devices. It allows users to run a full-fledged Linux shell on their smartphones or tablets wit...", "content": "Overview of TermuxTermux is a free and open-source terminal emulator and Linux environment for Android devices. It allows users to run a full-fledged Linux shell on their smartphones or tablets without requiring root access. Termux provides a minimal base system that can be expanded with additional packages, making it a powerful tool for developers, system administrators, and tech enthusiasts.Purpose and Functionality Linux Command Line: Termux provides access to a Linux command-line interface, allowing users to execute standard Linux commands and scripts. Package Management: Users can install over a thousand packages using the built-in package manager, which is based on Debian’s APT. Programming Environment: Termux supports various programming languages, including Python, Ruby, Node.js, and C/C++. Users can write and run code directly from their devices. Add-ons: Termux offers several add-ons that enhance its functionality, such as Termux:API for accessing Android features, Termux:Boot for executing scripts at startup, and Termux:Widget for running scripts from the home screen.How to Use Termux Installation: Download Termux from F-Droid, as it is not regularly updated on the Google Play Store. Install the APK and launch the app. Basic Commands: Update package lists: apt update Upgrade installed packages: apt upgrade Install a package: apt install &lt;package-name&gt; List installed packages: dpkg --get-selections Navigate directories: cd &lt;directory-name&gt; List files: ls Clear the terminal: clear Configuration: Customize settings by editing the ~/.termux/termux.properties file for appearance and keyboard shortcuts. Use the command termux-setup-storage to grant access to shared storage. Running Scripts: Create a script file (e.g., script.sh), make it executable with chmod +x script.sh, and run it with ./script.sh. Use Cases for Termux Development: Write and run scripts in languages like Python, Ruby, or JavaScript. Install text editors like Nano or Vim for coding. Remote Access: Use SSH to connect to remote servers for administration and file management. Data Processing: Utilize Python and other tools for data analysis and processing tasks. Web Development: Set up a local web server using packages like nginx or apache. Automation: Create scripts to automate repetitive tasks and use Termux:Widget to run them from the home screen. Learning Linux: Termux is an excellent platform for learning Linux commands and shell scripting.Pros and ConsPros No Root Required: Termux operates without needing root access, making it accessible to a broader audience. Extensive Package Repository: Access to a wide variety of packages and tools for different tasks. Customizable: Users can tailor the terminal experience to their preferences, including appearance and key bindings. Active Community: A supportive community that contributes to the development and maintenance of the project.Cons Limited File System Access: Termux has restricted access to the Android file system, which may limit some functionalities. Performance Limitations: Running intensive applications may be constrained by the device’s hardware capabilities. Learning Curve: Users unfamiliar with command-line interfaces may face a steep learning curve.Common Commands to Know apt update: Update the package list. apt upgrade: Upgrade installed packages. apt install &lt;package-name&gt;: Install a specific package. pkg search &lt;keyword&gt;: Search for packages. cd &lt;directory&gt;: Change the current directory. ls: List files in the current directory. cat &lt;file&gt;: Display the contents of a file. nano &lt;file&gt;: Edit a file using the Nano text editor. ssh &lt;user&gt;@&lt;host&gt;: Connect to a remote server via SSH. python &lt;script.py&gt;: Run a Python script.To use Termux in an offline environment and install essential packages that have been pre-downloaded in an online environment, follow the steps outlined below:Using Termux in an Offline Environment Install Termux: First, install Termux from F-Droid, as it is not regularly updated on the Google Play Store. You will need an internet connection for this initial installation. Prepare for Offline Use: Before going offline, ensure that you download any necessary packages while you still have internet access. This includes essential tools and utilities that you might need. Download Packages in Online Environment: Use the following commands to update the package list and install packages while you are online: apt updateapt upgradeapt install &lt;package-name&gt; Replace &lt;package-name&gt; with the names of the packages you want to install. Download Package Files: If you want to install packages offline later, you can download the .deb files for the packages you need. Use the command: apt download &lt;package-name&gt; This will save the package files in your current directory. Installing Essential Packages Offline Transfer Packages: Move the downloaded .deb files to your offline device. You can use USB, Bluetooth, or any other file transfer method. Install Packages from Local Files: Open Termux on your offline device and navigate to the directory where you transferred the .deb files. Use the following command to install the downloaded packages: dpkg -i &lt;package-file.deb&gt; Replace &lt;package-file.deb&gt; with the name of the downloaded package file. Resolve Dependencies: If the package has dependencies that are not installed, you may need to download those dependencies in advance while online and repeat the process for each one. Pros and Cons of Using Termux OfflinePros Access to Linux Tools: Even without an internet connection, you can still use many command-line tools and scripts. Flexibility: You can customize your environment and install only the packages you need. Learning Opportunity: Termux provides a great platform for learning Linux commands and shell scripting.Cons Limited Package Availability: You must pre-download all necessary packages and their dependencies, which can be cumbersome. Manual Dependency Management: Managing dependencies can be complex, especially if you need multiple packages that rely on each other.ConclusionTermux is a versatile tool that brings the power of a Linux command line to Android devices, allowing users to perform a wide range of tasks directly from their smartphones. Its extensive package support, customization options, and active community make it a valuable resource for developers, system administrators, and tech enthusiasts alike. Whether you want to learn Linux, develop software, or automate tasks, Termux provides the necessary environment to do so effectively." }, { "title": "How to leverage computer vision technologies to assist persons with visual impairments?", "url": "/posts/ideas-computer-vision-for-people/", "categories": "Idea, CV", "tags": "computer vision, ideas, technology", "date": "2024-09-04 00:32:31 -0400", "snippet": "50 innovative ideas for leveraging computer vision technologies to assist persons with visual impairments: Text Recognition and Reading: Use computer vision to scan and read text from books, d...", "content": "50 innovative ideas for leveraging computer vision technologies to assist persons with visual impairments: Text Recognition and Reading: Use computer vision to scan and read text from books, documents, or signs aloud to users. Object Identification: Identify and describe everyday objects in the user’s environment, such as groceries or personal items. Facial Recognition: Recognize and identify faces of friends, family, or staff in public places to provide social context. Scene Description: Provide detailed descriptions of scenes, including identifying objects, people, and activities in real-time. Navigation Assistance: Create a system to help users navigate through indoor and outdoor spaces by identifying obstacles, pathways, and landmarks. Color Detection: Identify and describe colors of objects or clothing to help users with color-related tasks. Barcode Scanning: Scan barcodes on products to provide information about the item, including nutritional details or price. Currency Identification: Recognize and differentiate between different denominations of currency. Text Translation: Translate printed text into different languages and read it aloud. Speech-to-Text Conversion: Convert spoken words into text for communication or note-taking purposes. Facial Expression Recognition: Detect and describe emotions or expressions on faces, helping users understand the emotional context of interactions. Augmented Reality Navigation: Use AR to overlay navigational aids and information onto the user’s field of view. Obstacle Detection: Alert users to potential obstacles or hazards in their path, such as steps or low-hanging objects. Food Recognition: Identify and describe different types of food and their preparation methods. Package and Mail Recognition: Identify and sort packages or mail based on visual characteristics. Personalized Shopping Assistance: Provide information and recommendations about products in stores based on user preferences. Environmental Monitoring: Monitor environmental conditions like weather or air quality and provide relevant information. Interactive Voice Feedback: Offer interactive voice feedback based on visual inputs, allowing users to ask questions about their environment. Gesture Recognition: Recognize and interpret hand gestures or body movements for control and interaction with devices. Guide Dog Assistance: Integrate computer vision with guide dogs to provide additional navigation support. Reading Assistance: Help users read newspapers, magazines, or digital content by converting printed text into speech. Fitness Tracking: Track and analyze physical activities or exercises and provide feedback or guidance. Emergency Alert Systems: Detect emergencies or unsafe conditions and alert users or caregivers. Interactive Learning Tools: Create educational tools that use computer vision to teach subjects like geography or science through visual aids. Smart Home Integration: Control smart home devices through visual recognition, allowing users to interact with their home environment. Parking Assistance: Help users locate and identify parking spots, or provide assistance in parking their vehicles. Social Media Interaction: Provide access to social media content by describing images or posts. Event Recognition: Identify and describe events or activities happening around the user, such as concerts or sports games. Voice-Controlled Cameras: Use voice commands to control camera functions for capturing images or videos. Travel Assistance: Assist with travel planning and provide information about destinations, landmarks, or transportation options. Daily Task Assistance: Aid in daily tasks like cooking, cleaning, or organizing by recognizing and describing items or tasks. Safety Alerts: Detect and warn users about potential safety issues, such as gas leaks or fire alarms. Social Interaction Aids: Facilitate interactions with others by providing information about social cues and etiquette. Health Monitoring: Monitor health-related parameters, such as facial changes or symptoms, and provide alerts or recommendations. Memory Aids: Help users remember important information or events by recognizing and recalling visual cues. Customizable Alerts: Allow users to set up personalized alerts based on visual inputs or changes in their environment. Remote Assistance: Enable remote assistance through video calls and visual sharing for support or guidance. Event Planning: Assist in planning events or gatherings by identifying and organizing visual elements. Art and Entertainment: Enhance the experience of art and entertainment by describing visual content and performances. Travel Guides: Provide detailed guides and information about tourist attractions and destinations. Cultural and Historical Information: Offer information about cultural and historical landmarks based on visual recognition. Visual Storytelling: Create visual stories or narratives based on images or scenes for educational or recreational purposes. Digital Signage Interaction: Interact with digital signage and advertisements to access additional information or promotions. Sports and Recreation: Assist with sports and recreational activities by recognizing and describing game elements or instructions. Emergency Services: Provide real-time information to emergency services based on visual inputs for quicker response. Virtual Reality Integration: Enhance virtual reality experiences with real-time visual recognition and feedback. Assistive Navigation Devices: Integrate computer vision into wearable devices for navigation assistance. Daily Routine Assistance: Help users manage their daily routines by recognizing and prompting activities or tasks. Cognitive Training: Offer cognitive training exercises and games using visual recognition to support mental health. Environmental Adaptation: Adjust the environment based on visual inputs, such as changing lighting or color schemes for better accessibility. These ideas can be tailored and expanded based on specific needs and technological advancements.More ideas:Here are 50 innovative ideas for leveraging computer vision technologies to assist individuals with visual impairments:Navigation and Mobility Smart Navigation Glasses: Wearable glasses that use computer vision to provide real-time navigation assistance, identifying obstacles and guiding users through audio feedback. Object Recognition Apps: Mobile applications that help users identify everyday objects by scanning them with the camera and providing audio descriptions. Indoor Navigation Systems: Systems that use computer vision to help visually impaired individuals navigate complex indoor environments like malls or airports. Autonomous Wheelchairs: Wheelchairs equipped with computer vision to navigate autonomously, avoiding obstacles and following user commands. Smart Canes: Canes that use computer vision to detect obstacles and provide haptic feedback to guide users safely. Reading and Information Access Text-to-Speech Glasses: Glasses that read text aloud from signs, menus, or documents using optical character recognition (OCR). Scene Description Apps: Applications that describe the environment and surroundings to users, providing context about their location. Braille Translation Devices: Devices that convert printed text into Braille in real-time, allowing users to read documents on the go. Smart Reading Assistants: Devices that scan and read books or documents aloud, helping users access written content independently. Voice-Activated Information Retrieval: Systems that allow users to ask questions about their surroundings, with responses generated through computer vision analysis. Social Interaction Facial Recognition for Social Cues: Applications that identify and describe facial expressions and emotions of people, enhancing social interactions. Gesture Recognition for Communication: Systems that interpret gestures and convert them into audio or text, facilitating communication for those with visual impairments. Social Media Accessibility Tools: Tools that provide audio descriptions for images and videos on social media platforms. Safety and Security Obstacle Detection Systems: Wearable devices that alert users to nearby obstacles using audio or vibration feedback. Emergency Assistance Apps: Applications that can identify emergency situations (like a fall) and alert caregivers or emergency services. Smart Home Security: Systems that use computer vision to identify visitors and alert users about unusual activities around their homes. Education and Training Interactive Learning Tools: Educational applications that use computer vision to help visually impaired students learn through tactile and auditory feedback. Virtual Reality Training: VR simulations that allow visually impaired individuals to practice navigation and mobility skills in a safe environment. Braille Learning Apps: Applications that teach Braille using computer vision to recognize and provide feedback on user input. Daily Living Aids Smart Grocery Shopping Assistants: Apps that help users locate items in grocery stores by providing audio directions based on visual recognition. Cooking Assistance Tools: Devices that guide users through recipes using voice instructions and visual recognition of ingredients. Personal Assistant Robots: Robots that assist with daily tasks by recognizing objects and providing audio guidance. Health and Fitness Fitness Tracking Wearables: Devices that monitor physical activity and provide feedback through audio cues, encouraging exercise. Health Monitoring Systems: Wearable devices that track health metrics and alert users to potential health issues. Accessibility Enhancements Screen Readers with Enhanced Vision: Advanced screen readers that use computer vision to describe on-screen content more effectively. Smartphone Accessibility Features: Enhanced camera features that assist users in navigating apps using voice commands and visual recognition. Adaptive User Interfaces: Interfaces that adapt based on the user’s needs, providing audio descriptions and tactile feedback. Community and Support Crowdsourced Assistance Platforms: Apps that connect visually impaired users with volunteers for real-time assistance in navigating public spaces. Community Awareness Tools: Applications that educate the public about visual impairments and promote inclusivity through augmented reality experiences. Research and Development Data Annotation for AI Training: Initiatives to create annotated datasets of visual environments to improve AI models for the visually impaired. Collaborative Research Projects: Partnerships between tech companies and organizations for the visually impaired to develop new technologies. Miscellaneous Smart Contact Lenses: Lenses that use computer vision to provide augmented reality information directly to the user’s field of view. Interactive Braille Displays: Displays that convert digital text to Braille dynamically, allowing users to read on various devices. Voice-Controlled Smart Assistants: Assistants that help users manage their daily tasks through voice commands and visual recognition. Augmented Reality Navigation: AR applications that overlay navigation instructions onto the real world, guiding users with audio cues. Personalized Shopping Experiences: Apps that recognize user preferences and provide tailored shopping recommendations based on visual cues. Enhanced Public Transport Systems: Systems that provide audio announcements and visual recognition of transport schedules and routes. Smart Textiles: Clothing embedded with sensors that provide feedback about the environment, enhancing safety and awareness. Pet Assistance Devices: Wearable devices for guide dogs that help them navigate complex environments using computer vision. Assistive Gaming: Video games designed for visually impaired players that use audio cues and haptic feedback for navigation and interaction. Virtual Companions: AI-driven companions that provide emotional support and social interaction through voice and visual recognition. Remote Assistance Tools: Applications that allow sighted individuals to assist visually impaired users through video calls and shared screens. Smart Mirrors: Mirrors that provide voice feedback about the user’s appearance, clothing options, and grooming tips. Accessibility Audits: Tools that use computer vision to assess the accessibility of public spaces and provide recommendations for improvement. Interactive Storytelling Apps: Applications that narrate stories using visual recognition to enhance engagement through audio and tactile feedback. Smart City Solutions: Urban planning initiatives that incorporate computer vision to improve accessibility for visually impaired residents. Personalized Learning Environments: Educational tools that adapt to the learning styles and needs of visually impaired students. Community Engagement Platforms: Social platforms that promote interaction among visually impaired individuals and provide resources for support. Smart Assistive Devices: Devices that integrate multiple functionalities, such as navigation, object recognition, and communication. AI-Driven Personal Finance Tools: Applications that help visually impaired users manage their finances through voice commands and visual recognition. These ideas leverage computer vision technologies to enhance the independence, accessibility, and quality of life for individuals with visual impairments, addressing a wide range of daily challenges they face." }, { "title": "Key difference between RAG and embedding", "url": "/posts/key-difference-rag-embedding/", "categories": "AI, LLM", "tags": "rag, embedding, basics", "date": "2024-09-03 02:46:51 -0400", "snippet": "The key difference between RAG (Retrieval-Augmented Generation) and embeddings is: RAG is a technique that combines large language models (LLMs) with information retrieval systems to generate ...", "content": "The key difference between RAG (Retrieval-Augmented Generation) and embeddings is: RAG is a technique that combines large language models (LLMs) with information retrieval systems to generate responses. It uses embeddings as part of the retrieval process, but RAG encompasses the overall architecture and workflow. Embeddings are dense vector representations of text that capture semantic meaning and relationships. They are a crucial component in RAG, enabling the retrieval of relevant information to augment the LLM’s generation. Here are some more details on their relationship:Role of Embeddings in RAG Embeddings are used to encode both the user query and the available information (e.g. documents, passages) in a shared semantic space. This allows the RAG system to efficiently retrieve the most relevant information by finding the chunks with embeddings closest to the query embedding. The quality of the embeddings directly impacts the retrieval performance and ultimately the quality of the generated responses.RAG Architecture RAG combines an LLM for generation with a retrieval system that uses embeddings to find relevant information. The retrieved information is then used to augment the LLM’s input, providing additional context to improve the generated output. The retrieval system can use various techniques like nearest neighbor search over embeddings to efficiently find the most relevant information.Embedding Models Embeddings are produced by specialized models like BERT, GPT, or custom models trained for the specific task. The choice of embedding model is crucial and can significantly impact RAG performance. Factors like model size, training data, and architecture affect the quality of the embeddings.In summary, while embeddings are a core component, RAG is a broader concept that leverages embeddings as part of a retrieval-augmented generation workflow to enhance the capabilities of large language models. The quality of embeddings is critical for effective retrieval in RAG systems." }, { "title": "What is RAG in LLM", "url": "/posts/what-is-rag-in-llm/", "categories": "AI, LLM", "tags": "ai, llm, training, rag", "date": "2024-09-03 00:48:16 -0400", "snippet": "Retrieval-Augmented Generation (RAG) is an advanced framework that enhances the capabilities of large language models (LLMs) by integrating external information retrieval systems into the generativ...", "content": "Retrieval-Augmented Generation (RAG) is an advanced framework that enhances the capabilities of large language models (LLMs) by integrating external information retrieval systems into the generative process. Here’s a detailed overview of RAG based on the provided search results:What is RAG?RAG combines the strengths of traditional information retrieval systems with the generative capabilities of LLMs. The primary objective is to improve the relevance and accuracy of the responses generated by LLMs by providing them with real-time access to external knowledge sources.How RAG Works Information Retrieval: When a user inputs a query, RAG first retrieves relevant information from an external knowledge base, such as a database or document repository. This is typically done using vector databases that store data in a way that facilitates efficient search and retrieval based on semantic similarity. Augmenting the Input: The retrieved information is then combined with the original user query to create a richer input context for the LLM. This augmented prompt helps the model generate more accurate and contextually relevant responses. Response Generation: The LLM processes the augmented input and generates a response that is informed by both its pre-trained knowledge and the newly retrieved information. Benefits of RAG Up-to-Date Information: RAG allows LLMs to access current and authoritative information, overcoming the limitations of static training data. This is particularly useful for applications requiring accurate and timely responses, such as customer support or news reporting. Reduced Hallucinations: By grounding the model’s responses in verifiable external knowledge, RAG helps minimize the occurrence of hallucinations—instances where the model generates incorrect or fabricated information. Domain-Specific Knowledge: RAG can be tailored to specific domains by integrating relevant external data, enabling LLMs to provide contextually appropriate responses that are aligned with organizational knowledge. Cost-Effective Customization: RAG allows organizations to enhance LLM outputs without the need for extensive retraining, making it a more efficient and cost-effective approach compared to traditional fine-tuning methods. Applications of RAG Chatbots and Virtual Assistants: RAG can improve the conversational abilities of chatbots by providing them with access to external knowledge, enabling them to answer user queries more accurately. Question Answering Systems: RAG enhances the performance of systems designed to answer questions by ensuring that responses are based on the most relevant and up-to-date information. Content Generation: It can be used in content creation tools to ensure that generated text is grounded in factual information and relevant context. ConclusionRetrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of large language models by integrating real-time information retrieval. By providing LLMs with access to external knowledge sources, RAG improves the accuracy, relevance, and reliability of generated responses, making it a valuable approach for various applications in AI and natural language processing." }, { "title": "I want to become tech evangelist", "url": "/posts/become-tech-evangelist/", "categories": "Career", "tags": "career, role, evangelist", "date": "2024-09-03 00:43:09 -0400", "snippet": "The role of a technology evangelist is to promote and advocate for a particular technology, product, or service. They aim to inspire and influence individuals and businesses to adopt and use the te...", "content": "The role of a technology evangelist is to promote and advocate for a particular technology, product, or service. They aim to inspire and influence individuals and businesses to adopt and use the technology they represent. As a programmer, here are some key aspects of the technology evangelist role and the skills required to excel in this position:Key Responsibilities Educating and informing others about the benefits and potential uses of the technology Building a community of supporters and advocates for the technology Collaborating with developers and engineers to understand the technology deeply Creating compelling content such as blog posts, tutorials, and presentations Engaging with influencers, industry experts, and potential customers Organizing events, meetups, and speaking engagements Analyzing the impact and effectiveness of evangelism effortsRequired SkillsTechnical Skills Strong programming skills and hands-on experience with the technology being evangelized Understanding of software development best practices and emerging trends Ability to quickly learn and master new technologiesCommunication and Presentation Skills Excellent written and verbal communication skills Ability to explain complex technical concepts in simple terms Engaging and persuasive presentation skills Adaptability in tailoring messages to different audiencesRelationship Building and Networking Outgoing personality and ability to connect with people Proactive in initiating and maintaining relationships Collaborative mindset and willingness to work with others Networking skills to build a wide network of contactsContent Creation and Social Media Strong writing skills for blog posts, articles, and tutorials Ability to create visually appealing and informative presentations Active presence on social media platforms relevant to the target audience Understanding of content marketing and SEO best practicesAnalytical and Problem-Solving Skills Ability to gather and analyze data on evangelism efforts Adaptability in refining strategies based on feedback and results Creative problem-solving skills to overcome challengesBecoming a Technology Evangelist as a ProgrammerAs a programmer, you can leverage your technical skills and experience to transition into a technology evangelist role. Here are some steps to consider: Develop deep expertise in a specific technology or domain Start creating content such as blog posts, tutorials, and open-source projects Engage with the developer community through social media, forums, and local meetups Attend and speak at industry conferences and events Collaborate with marketing and sales teams to understand customer needs and pain points Continuously learn and stay up-to-date with the latest trends and best practicesBy combining your technical skills with strong communication abilities and a passion for sharing knowledge, you can effectively transition into a technology evangelist role and make a significant impact in promoting and advocating for the technologies you believe in." }, { "title": "What is GGUf?", "url": "/posts/what-is-gguf/", "categories": "AI, LLM", "tags": "basics, gguf", "date": "2024-09-03 00:29:59 -0400", "snippet": "What is GGUF File for LLM?The GGUF (GPT-Generated Unified Format) file is a binary file format designed for storing and loading model weights, particularly for large language models (LLMs). It is s...", "content": "What is GGUF File for LLM?The GGUF (GPT-Generated Unified Format) file is a binary file format designed for storing and loading model weights, particularly for large language models (LLMs). It is specifically tailored for use with the GGML library and other executors based on GGML. Here are the key aspects of GGUF based on the search results:Key Features of GGUF Binary Format: GGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. Model Storage: It combines model parameters (weights and biases) with additional metadata to facilitate efficient execution on consumer-grade hardware. Compatibility: GGUF supports various programming languages like Python and R, enhancing its usability across different platforms and applications. Extensibility: The format is designed to be clear and extensible, allowing for the incorporation of new features without breaking compatibility with older models. Fine-Tuning Support: GGUF allows users to fine-tune models, enabling adaptations for specialized applications. Prompt Templates: It can store prompt templates, which are useful for deploying models across different applications. Use Cases for GGUF Model Deployment: GGUF is particularly useful for deploying trained machine learning models in production settings, enabling rapid inference. Local Execution: It allows models to be run locally on user machines, which can enhance privacy and reduce latency compared to cloud-based solutions. Research and Development: Researchers can utilize GGUF to experiment with various models and configurations without extensive setup.Performance Benefits Fast Loading and Saving: The binary nature of GGUF allows for quicker model loading and saving, which is crucial for applications requiring real-time responses. Efficient Resource Use: GGUF is optimized for performance on consumer-grade hardware, making it accessible for hobbyists and researchers.Pros and Cons of GGUFPros: Efficiency: Designed for fast loading and saving, improving inference speed. Compatibility: Works with various programming languages and frameworks, enhancing integration possibilities. Extensibility: Supports future updates and features without breaking existing functionality.Cons: Complexity: Being a binary format, it may be less transparent than text-based formats, making debugging more challenging. Potential Vulnerabilities: As noted in some discussions, there may be security vulnerabilities related to insufficient validation on input files, which could lead to memory corruption issues.ConclusionThe GGUF file format represents a significant advancement in the deployment and management of large language models. Its design focuses on efficiency, compatibility, and extensibility, making it a valuable tool for developers and researchers working with AI models. As the ecosystem around GGUF continues to grow, it is likely to see broader adoption in various applications across industries." }, { "title": "What is Customization & Fine-Tuning of LLM", "url": "/posts/ollama-customization-fine-tuning/", "categories": "AI, LLM", "tags": "customization, fine-tuning, basic", "date": "2024-09-02 23:34:39 -0400", "snippet": "Customization and fine-tuning are powerful capabilities that allow users to adapt large language models (LLMs) to their specific needs and use cases. Here are some more details on how this works wi...", "content": "Customization and fine-tuning are powerful capabilities that allow users to adapt large language models (LLMs) to their specific needs and use cases. Here are some more details on how this works with Ollama:CustomizationOllama provides various ways for users to customize models to their preferences: Prompt Engineering: Users can craft prompts that guide the model’s behavior and outputs. By carefully designing prompts, users can steer the model towards generating content that aligns with their goals. Hyper-parameter Tuning: Models have various hyper-parameters like temperature, top-k, top-p that control the randomness and diversity of outputs. Users can experiment with different settings to find the right balance for their use case. Role Prompts: Users can define role descriptions that shape the model’s personality and communication style. For example, defining a “helpful assistant” role can make the model more concise and task-oriented. Persona Prompts: Similar to role prompts, persona prompts allow users to specify attributes of the model like age, gender, interests, etc. This can make interactions feel more natural and human-like. Fine-TuningFine-tuning is the process of further training a pre-trained model on a specific dataset to enhance its performance on related tasks. With Ollama, users can fine-tune models in a few key ways: Using Adapter Modules: Ollama supports adapter modules that can be trained on custom data while keeping the original model frozen. This allows for efficient fine-tuning without catastrophic forgetting. Prompt Tuning: Instead of updating model weights, prompt tuning optimizes the prompts used to condition the model. This is a parameter-efficient way to specialize the model to new domains. Full Fine-Tuning: For more extensive customization, users can fine-tune the entire model by updating all weights. This requires more data and compute but can lead to significant performance gains on the target task. Benefits of Customization and Fine-TuningCustomizing and fine-tuning models with Ollama offers several advantages: Improved Performance: Models can be optimized for specific applications, leading to better accuracy, coherence, and relevance of outputs. Personalization: Customization allows models to adapt to individual preferences and communication styles, enhancing user experience. Privacy and Security: Fine-tuning on private datasets can be done locally without exposing data to external servers. Reduced Bias: Carefully curated datasets can help mitigate biases present in general-purpose models. By providing these powerful customization and fine-tuning capabilities, Ollama empowers users to truly make large language models their own and unlock their full potential for diverse applications. The ability to adapt models to specific needs is a key driver of Ollama’s mission to democratize AI." }, { "title": "About Ollama", "url": "/posts/about-ollama/", "categories": "AI, LLM", "tags": "ollama, local, llm", "date": "2024-09-02 23:24:40 -0400", "snippet": "Ollama is a platform designed to facilitate the use of large language models (LLMs) locally on users’ machines. Here are the main use cases, core principles, performance benefits, and other relevan...", "content": "Ollama is a platform designed to facilitate the use of large language models (LLMs) locally on users’ machines. Here are the main use cases, core principles, performance benefits, and other relevant information based on the search results:Main Use Cases for Ollama LLM-Powered Web Applications: Ollama can be integrated into web applications to provide AI-driven features such as chatbots, content generation, and natural language processing. Local Note-Taking Tools: It can be used with local note-taking applications like Obsidian, allowing users to leverage LLM capabilities for summarizing notes or generating content. Creative Writing and Content Generation: Writers can use Ollama to assist in generating ideas, drafting content, or enhancing their writing with AI suggestions. Code Generation and Assistance: Developers can utilize Ollama for code completion, debugging assistance, and generating code snippets based on natural language prompts. Language Translation and Localization: Ollama can assist in translating text and localizing content for different languages, making it useful for global applications. Research and Development: Researchers can run LLMs locally for experiments, testing, and developing new AI models or applications without relying on cloud services. Customization and Fine-Tuning: Users can customize and fine-tune models to meet specific needs, enhancing the model’s performance for particular tasks. Embedding Generation: Ollama can generate vector embeddings for use in search and retrieval applications, enabling more efficient information retrieval. Tool Integration: It supports integration with various programming languages and frameworks, allowing developers to incorporate LLMs into existing workflows seamlessly. Real-Time Interaction: Ollama can serve models via a REST API, enabling real-time interactions for applications that require immediate responses. Core Principle of Its DesignThe core principle of Ollama’s design is to democratize access to large language models by allowing users to run them locally. This approach prioritizes user control, privacy, and ease of use. By bundling model weights, configurations, and datasets into a unified package (Modelfile), Ollama simplifies the deployment and management of LLMs, making it accessible for both developers and researchers.Performance BenefitsOllama offers several performance benefits: Local Execution: Running models locally reduces latency and enhances privacy compared to cloud-based solutions. Resource Management: It optimizes resource usage, allowing for efficient execution of LLMs based on the available hardware. Ease of Use: The platform simplifies the process of downloading, running, and managing models, making it easier for users to experiment and develop applications.Pros and Cons of OllamaPros: User-Friendly Interface: Simplifies the deployment of LLMs, making it accessible to non-experts. Local Control: Users have more control over their data and models, enhancing privacy. Versatile Applications: Supports a wide range of use cases, from creative writing to code generation.Cons: Resource Intensive: Running large models locally may require significant computational resources. Limited Community Support: As a newer platform, it may have less community support compared to established frameworks. Model Availability: Users may need to wait for new models to be added to the Ollama hub.ConclusionOllama provides a powerful and flexible platform for leveraging large language models locally, catering to various use cases across industries. Its design principles focus on user accessibility, privacy, and performance, making it a valuable tool for developers, researchers, and AI enthusiasts." }, { "title": "What is LLM?", "url": "/posts/what-is-llm/", "categories": "AI, LLM", "tags": "llm, basic", "date": "2024-09-02 22:43:07 -0400", "snippet": "Explanation of LLM (Large Language Models)Core PrincipleLarge Language Models (LLMs) are based on deep learning architectures, primarily using transformer networks. They are designed to understand ...", "content": "Explanation of LLM (Large Language Models)Core PrincipleLarge Language Models (LLMs) are based on deep learning architectures, primarily using transformer networks. They are designed to understand and generate human-like text by predicting the next word in a sequence based on the context provided by the preceding words. The core principle involves training on vast amounts of text data, allowing the model to learn patterns, grammar, facts, and even some reasoning abilities.AbilityLLMs have several notable capabilities: Text Generation: They can generate coherent and contextually relevant text based on prompts. Language Understanding: They can comprehend and respond to questions, summarize texts, and perform sentiment analysis. Translation: They can translate text between languages with reasonable accuracy. Conversational Agents: They can engage in dialogue, providing responses that are contextually appropriate. Text Completion: They can complete sentences or paragraphs based on initial input.LimitationDespite their capabilities, LLMs have several limitations: Context Length: They may struggle with very long contexts or maintaining coherence over extended conversations. Factual Inaccuracy: They can produce plausible-sounding but incorrect or nonsensical answers. Bias: They can reflect and propagate biases present in the training data. Lack of Understanding: They do not truly understand the text but rather generate responses based on learned patterns. Resource Intensive: Training and deploying LLMs require significant computational resources.Top 10 Popular LLMs for Various Industries OpenAI’s GPT-3: Widely used for content creation, chatbots, and coding assistance. Google’s BERT: Primarily used for improving search results and natural language understanding tasks. Facebook’s RoBERTa: An optimized version of BERT, used in various NLP applications. Microsoft’s Turing-NLG: Focused on generating human-like text for various applications. EleutherAI’s GPT-Neo: An open-source alternative to GPT-3, popular in research and development. Hugging Face’s Transformers: A library that includes various pre-trained models for different NLP tasks. DeepMind’s Gopher: Designed for knowledge-intensive tasks and question answering. Anthropic’s Claude: A conversational AI model focused on safety and ethical considerations. Cohere’s Language Models: Used for enterprise applications, such as customer support. AI21 Labs’ Jurassic-1: A large model aimed at generating human-like text for various applications.Learning PathTo study LLMs effectively, consider the following learning path: Basic Understanding of Machine Learning: Familiarize yourself with fundamental concepts. Deep Learning: Study neural networks, especially feedforward and recurrent networks. Natural Language Processing (NLP): Learn about text processing, tokenization, and embeddings. Transformers: Understand the architecture of transformers and how they work. Hands-On Practice: Use frameworks like TensorFlow or PyTorch to build and train simple models. Explore Pre-trained Models: Experiment with models available through libraries like Hugging Face’s Transformers. Advanced Topics: Study fine-tuning, transfer learning, and model evaluation.Papers to Read to Study LLM Attention is All You Need - Vaswani et al. (2017): Introduces the transformer architecture. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Devlin et al. (2018): Discusses BERT and its applications. Language Models are Few-Shot Learners - Brown et al. (2020): Introduces GPT-3 and its capabilities. RoBERTa: A Robustly Optimized BERT Pretraining Approach - Liu et al. (2019): Discusses improvements over BERT. GPT-2: Language Models are Unsupervised Multitask Learners - Radford et al. (2019): Discusses the capabilities of GPT-2. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations - Lan et al. (2019): Discusses a more efficient version of BERT. XLNet: Generalized Autoregressive Pretraining for Language Understanding - Yang et al. (2019): Introduces XLNet, which improves upon BERT. T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - Raffel et al. (2019): Discusses the T5 model and its versatility. The Power of Scale for Parameter-Efficient Prompt Tuning - Lester et al. (2021): Discusses prompt tuning techniques. Scaling Laws for Neural Language Models - Kaplan et al. (2020): Discusses the impact of scale on model performance.GitHub LinkFor practical implementations and resources related to LLMs, you can explore the following GitHub repositories: Hugging Face Transformers: A comprehensive library for working with various transformer models. OpenAI GPT-3: Access and usage guidelines for OpenAI’s GPT-3 model. EleutherAI GPT-Neo: Open-source implementations of GPT models.This overview provides a comprehensive understanding of Large Language Models, their capabilities, limitations, and resources for further study. If you have any specific questions or need more detailed information on any section, feel free to ask!" }, { "title": "The Power of Detachment - A Key to Better Decision-Making and Leadership", "url": "/posts/power-detachment/", "categories": "Leadership", "tags": "leadership", "date": "2024-01-09 21:30:54 -0500", "snippet": "Detachment is often misunderstood as indifference, but it is, in fact, a powerful tool for enhancing decision-making and leadership effectiveness. By stepping back from immediate emotional involvem...", "content": "Detachment is often misunderstood as indifference, but it is, in fact, a powerful tool for enhancing decision-making and leadership effectiveness. By stepping back from immediate emotional involvement, leaders can gain clarity and make more informed choices. Here’s a concise look at how to harness the power of detachment in your leadership journey.Understanding DetachmentDetachment involves temporarily distancing yourself from a situation to gain a broader perspective. This practice allows you to see beyond immediate concerns and assess the overall context. Jocko emphasizes that taking a step back—both physically and mentally—can lead to clearer decision-making.Practical Strategies for Practicing Detachment Physically Step Back: In tense situations, take a moment to step away. This can help you regain perspective and reduce emotional intensity. Broaden Your Perspective: Shift your focus from the details to the bigger picture. This mental detachment allows for better assessment and decision-making. Control Your Breathing: Before responding to a situation, take deep breaths. This simple act can calm your nervous system and help you think clearly. Change Your Body Language: Adopt open and relaxed body language to signal receptiveness rather than defensiveness. This can create a more constructive environment for dialogue. Listen More, Talk Less: Prioritize listening over speaking. By allowing others to express their thoughts, you can gather valuable insights that inform your decisions. Applying Detachment in LeadershipEffective leaders leverage detachment to maintain focus on overarching goals rather than getting bogged down in every detail. This approach fosters trust within teams, as leaders who detach from micromanagement empower their members to take ownership of their work.Willink’s leadership style exemplifies this; he listens actively during meetings, allowing his team to present their ideas while he observes for potential improvements. This method not only enhances decision-making but also cultivates a culture of collaboration and innovation.Conclusion: Embrace Detachment as a SuperpowerDetachment is not about being aloof; it’s about gaining the clarity needed to make thoughtful decisions. By practicing detachment—through physical distancing, mental broadening, controlled breathing, and active listening—you can navigate challenges more effectively. As leaders, embracing this skill can lead to better outcomes for both yourself and your team, transforming detachment into a true superpower in your leadership toolkit." }, { "title": "be-indistractable", "url": "/posts/be-indistractable/", "categories": "Habits", "tags": "habits, productivity, focused", "date": "2023-10-08 03:09:14 -0400", "snippet": "Break Any Habit &amp; Never Be Distracted!1. Distraction Comes from Within Internal Triggers: The majority (90%) of distractions come from internal triggers like boredom, anxiety, or discomfort. E...", "content": "Break Any Habit &amp; Never Be Distracted!1. Distraction Comes from Within Internal Triggers: The majority (90%) of distractions come from internal triggers like boredom, anxiety, or discomfort. External triggers, such as notifications, account for only 10%. Recognizing that most distractions are driven by our desire to escape discomfort is critical. Self-Reflection: To understand distractions, it’s essential to ask yourself what you’re trying to escape from when you feel the need to check your phone or procrastinate.2. Mastering Internal Triggers Emotion Management: Time management is essentially about managing discomfort. Successful people handle discomfort (stress, boredom, anxiety) by reframing it and using it to stay on track, whereas distractible people let these feelings derail their focus.3. Traction vs. Distraction Traction vs. Distraction: The opposite of distraction is not focus but traction. Both traction and distraction are actions we take, with traction being actions aligned with our goals and values, and distraction pulling us away from them.4. Control Your Attention to Avoid Regret Avoiding Regret: The biggest reason to focus is to avoid living with regret. People generally know what they need to do but get distracted because they fail to control their attention. Becoming “indistractable” allows people to live without looking back with regret.5. Four-Step Model to Become Indistractable: Master Internal Triggers: Recognize and manage emotional discomfort that leads to distraction. Make Time for Traction: Plan time to focus on important tasks, ensuring your schedule reflects your values. Hack Back External Triggers: Minimize interruptions from technology, meetings, and other external factors. Prevent Distraction with Pacts: Use precommitment devices to avoid getting distracted, like creating specific rules for work and leisure time.6. Reframing Discomfort as a Positive Force Use Discomfort for Growth: Instead of trying to avoid discomfort, use it as a motivator. High performers channel their stress and anxiety into productivity rather than escaping it through distractions.7. Scheduling and Time Management Timeboxing: Break tasks into scheduled time blocks to focus on them fully, minimizing distractions and procrastination. Complete the task within the allotted time, even if it’s just sitting and thinking about it.These principles focus on internal self-management, emotional regulation, and structured time management to overcome distractions and boost productivity." }, { "title": "Running jekyll server is not working on my Apple M2 device.", "url": "/posts/run-jekyll-on-apple-silicon-m2/", "categories": "Jekyll", "tags": "jekyll, local server, apple m2", "date": "2023-06-21 22:05:00 -0400", "snippet": "You may want to preview the site content before publishing, so just run it by:$ bundle exec jekyll sHowever, this command didn’t work on the Apple M2 devices.Here is a workaround I found:$ rm -rf v...", "content": "You may want to preview the site content before publishing, so just run it by:$ bundle exec jekyll sHowever, this command didn’t work on the Apple M2 devices.Here is a workaround I found:$ rm -rf vendor$ arch -arch x86_64 bundle install$ arch -arch x86_64 bundle exec jekyll serve" }, { "title": "How to \"Know Thyself\"", "url": "/posts/how-to-know-thyself/", "categories": "Philosophy", "tags": "philosophy, wisdom, self-awareness, self-improvement", "date": "2022-10-07 17:07:00 -0400", "snippet": "Many of my conversation include a question about knowing yourself. Something like, “Is it possible to know thyself?” or put another way, “How does one begin to know themselves?”Know Thyself?I asked...", "content": "Many of my conversation include a question about knowing yourself. Something like, “Is it possible to know thyself?” or put another way, “How does one begin to know themselves?”Know Thyself?I asked: “How about self-awareness or this maxim of Know thyself? Is that possible?”Human reality is always stretching: explained Clearly, there’s constantly shifting. “I’m a philosopher today, maybe not tomorrow. So how do I become? How do I know myself? If I’m just sticking to the label of a philosopher?” It’s important to understand who were becoming… It is important to ask - what “self” we are trying to be aware of; since we are always in flux or always changing.Who am I? “Only the shallow know themselves.” - Oscar WildeI recently read the above Oscar Wilde quote in Original Self by the psychotherapist Thomas Moore.More writes,During my years of doing therapy, it was not unusual for a client to say, “If only I could finally understand myself and figure out what I’m doing, I’d be a free person.” One of the great unconscious beliefs of our time is our trust in the mind. We try to understand every fragment…, celebrating discoverers, inventor, and researchers, and we apply same passion to ourselves.What if most of our “self-understanding” is hollow and superficial? What if we don’t really know ourselves at all? Moore observed that “I was struck by the hollowness of most self-understanding” in therapy.Usually, we think of the self as an ego contained in the skin of personality, but a deeper self, a more original self, lies outside the time and space of our personal lives. “What was your face before you were born? the Zen Master asks.” According to Moore, this is an excellent question because it has no definite answer.Thought ExperimentDo all ways of knowing require understanding? Take a few minutes to listen to a great piece of music. Stare at a beautiful painting for an extended period of time.Does one need to understand (music or art) to appreciate it truly?Ancient AphorismsThe notion of self-awareness dates all the way back to Ancient Greece. “True wisdom comes to each of us when we realize how little we understand about life, ourselves, and the world around us.”Similarly, the nineteenth-century philosopher Friedrich Nietzsche noted: “How can man know himself? He is a thing obscure and veiled. If the hare has seven skins, man can cast from him seventy times seven skins and not be able to say: Here you truly are; there is skin no more.”What if authentic self-understanding requires the courage to accept the idea that we don’t actually know ourselves?Final Thoughts “Maybe that’s enlightenment enough: to know that there is no final resting place of the mind; no moment of smug clarity. Perhaps wisdom.. is realizing how small I am, and unwise, and how far I have yet to go.”The truth is, we may never truly know ourselves, our friends, or our spouses, but we may eventually realize that it is enough to love ourselves and others without knowing what they’re all about. According to Moore, “Unconditional love means that we don’t love on the condition that we understand.”Thank you for reading." }, { "title": "Human Brain vs. Supercomputer - A Hardware Perspective", "url": "/posts/comparison-human-brain-supercomputer/", "categories": "Brain, Basics", "tags": "brain, hardware, basics", "date": "2022-02-10 01:36:14 -0500", "snippet": "When comparing the human brain to a supercomputer from a hardware perspective, several key differences and insights emerge. Understanding these distinctions can shed light on the unique capabilitie...", "content": "When comparing the human brain to a supercomputer from a hardware perspective, several key differences and insights emerge. Understanding these distinctions can shed light on the unique capabilities of the brain and the limitations of traditional computing systems.1. Architecture and Processing Brain as a Parallel Processor: The human brain operates as a massively parallel network of trillions of neurons, each connected through synapses. This allows for simultaneous processing of vast amounts of information. In contrast, traditional computers, based on the Von Neumann architecture, typically rely on a single processor executing instructions in a serial manner, even when employing multiple cores. Neurons vs. Processors: Each neuron in the brain can be part of multiple processing networks, functioning collectively rather than as distinct units. This flexibility allows the brain to adapt and rewire itself based on experiences, unlike fixed computer architectures. 2. Energy Efficiency Low Power Consumption: The human brain is remarkably energy efficient, operating on approximately 10-20 watts. In comparison, modern supercomputers require hundreds to thousands of watts to function. This efficiency is a significant advantage of biological systems over traditional computational hardware.3. Learning and Adaptation Conditioning and Learning: The brain learns through conditioning, forming new synaptic connections based on experiences and repeated actions. This process is inherently different from how computers learn, which typically involves explicit programming and data input. The brain’s ability to adapt and rewire itself allows for continual learning and improvement. Simulated Conditioning: For habits that cannot be practiced daily, simulated conditioning—such as visualization—can be employed. This approach helps reinforce the mental pathways associated with the desired behavior, similar to how the brain processes and reinforces learning. 4. Information Storage and Processing Dynamic Information Processing: Unlike computers, where hardware and software are distinct, the brain’s hardware (neurons and synapses) is fundamentally intertwined with its processing capabilities. Information is not just stored but actively shapes the brain’s structure and function. Memory and Learning: The brain’s memory system is highly complex, involving various types of memory (short-term, long-term, procedural) that interact dynamically. This contrasts with traditional computers, which rely on static memory systems. 5. Limitations of Traditional Computing Task-Specific Design: Traditional computers excel at specific tasks but struggle with tasks that require the kind of flexible, context-aware processing that the brain performs effortlessly. For example, while computers can perform calculations quickly, they lack the intuitive understanding and problem-solving abilities inherent in human cognition. Emerging Technologies: As technology advances, there is a growing interest in neuromorphic computing, which aims to mimic the brain’s architecture and processing methods. This approach seeks to bridge the gap between biological and computational systems, potentially leading to more efficient and adaptable machines. ConclusionThe human brain and supercomputers represent fundamentally different approaches to processing information. The brain’s parallel processing capabilities, energy efficiency, dynamic learning, and integrated information storage highlight its unique advantages over traditional computing systems. As we continue to explore the potential of neuromorphic computing and other advanced technologies, understanding these differences will be crucial in developing systems that more closely emulate human cognition." }, { "title": "Use multiple SSH keys for different GitHub accounts.", "url": "/posts/multi-ssh-key-use-github/", "categories": "GitHub", "tags": "github, ssh, multiple accounts", "date": "2021-11-20 16:07:00 -0500", "snippet": "Create different public keyFollow GitHub document and create a ssh key.Start the ssh-agent in the background.$ eval \"$(ssh-agent -s)\"Then, add the key as following.$ ssh-add ~/.ssh/ssh-key-fileYou ...", "content": "Create different public keyFollow GitHub document and create a ssh key.Start the ssh-agent in the background.$ eval \"$(ssh-agent -s)\"Then, add the key as following.$ ssh-add ~/.ssh/ssh-key-fileYou can check your saved keys$ ssh-add -lModify the ssh configCreate the ~/.ssh/config file.Then add as following#jstfun accountHost github.com-jstfunHostName github.comUser gitIdentityFile ~/.ssh/jstfunClone your repo and modify your ssh repo URLTell the GitHub to not fetch the repo from github.com but from github.com-jstfun.There is no impact on the github URL but tell our computer that the host for the repo is connected to the jstfun ssh key. A git clone command should be like this:before: git clone git@github.com:jstfun/project.gitafter: git clone git@github.com-jstfun:jstfun/project.git If you already have fetched the repo on your local machine, you can change the remote URL properly.Check laterSimply, set the config as following:Host github.comHostName github.comUser your_user_account_githubPreferredAuthentications publickeyIdentityFile ~/.ssh/your_user_account_github_rsaIdentitiesOnly yes" }, { "title": "OrderedDictionary in Swift", "url": "/posts/ordered-dictionary-swift/", "categories": "Swift", "tags": "swift, ordered dictionary", "date": "2021-04-11 17:02:00 -0400", "snippet": "Many iOS and macOS applications use Dictionary for internal logic. A new Swift package called swift-collections introduces OrderedDictionary, a dictionary which keeps track of insertion order. This...", "content": "Many iOS and macOS applications use Dictionary for internal logic. A new Swift package called swift-collections introduces OrderedDictionary, a dictionary which keeps track of insertion order. This post presents an overview of OrderedDictionary and usage examples:To use OrderedDictionary, first add the swift-collection Swift package to your project. Then, import the OrderedCollections module:import OrderedCollectionsOrderedDictionary ExamplesInsert Value for Keyvar oderedDic: OrderedDictionary = [ \"key0\": 0, \"key1\": 1]orderedDict[\"key2\"] = 2// orderedDic now contains, in order,// \"key0\": 0, \"key1\": 1, \"key2\": 2Get Value for KeyLike a traditional Dictionary, OrderedDictionary is a key-value store and can retrieve values using a specific key:var orderedDict: OrderedDictionary = [ \"key0\": 0, \"key1\": 1]// Return the value 1orderedDict[\"key1\"]Additionally, OrderedDictionary has an internal order and can retrieve values at a specific position in the order using the elements property:// Returns the element at index 0var element = orderedDict.elements[0]element.key // \"key0\"element.value // 0Remove Key and ValueThere are multiple ways to remove keys and values from an OrderedDictionary. One way is to remove a key and value explicitly, either specifying the key directly or the index the key and value are at:// Remove a specific keyorderedDict.removeValue(forKey: \"key1\")// Remove a key and value at a specific indexorderedDict.remove(at: 2)Another method is to remove keys and values relative to the front and back of the OrderedDictionary:// Remove keys and values from the frontorderedDict.removeFirst()orderedDict.removeFirst(2)// Remove keys and values from the backorderedDict.removeLast()orderedDict.removeLast(2)OrderedDictionary also includes methods for removing all keys and values, and removing all keys and values that meet some filter criteria:// Remove all keys and valuesorderedDict.removeAll()// Filter keys and valuesorderedDict.removeAll{ (key, value) -&gt; Bool in // Filter criteria}Dictionary vs OrderedDictionaryDictionary is an unordered collection of keys and associated values, often used as a key-value store. Like Dictionary, OrderedDictionary contains keys and associated values and can be used as a key-value store. Unlike Dictionary:OrderedDictionary Maintains Insertion OrderAs the name indicates, OrderedDictionary introduces an elements property. The elements property is an Array value, and can be used to iterate over or retrieve keys and values at a specific position in the order.This means an OrderedDictionary can efficiently retrieve a value for a specific key, like a traditional Dictionary, and also retrieve a key and value at a specific position, similar to a traditional Array.An important note, reassigning keys to different values does not change the order:var orderdDict: OrderedDictionary = [ \"key0\": 0, \"key1\": 1, \"key2\": 2]// orderedDict contains, in order,// \"key0\": 0, \"key1\": 1, \"key2\": 2orderedDict[\"key1\"] = 100// OrderedDict contains, in order,// \"key0\": 0, \"key1\": 100, \"key2\": 2When To Use OrderedDictionaryOrdered CountersA counter is often used to determine the number of occurrences unique elements have in a sequence. An ordered counter allows the occurrences of unique elements to be counted, while also preserving the first-seen order:var sequence = [ \"a\", \"b\", \"a\", \"c\", \"b\", \"b\", \"b\", \"b\", \"a\"]var orderedCounter: OrderedDictionary&lt;String, Int&gt; = [:]for item in sequence { orderedCounter[item, default: 0] += 1}// orderedCounter now contains, in order,// \"a\": 3, \"b\": 4, \"c\": 1// Accessing the key \"b\" returns 4, the number of // times \"b\" occurs in the sequenceorderedCounter[\"b\"]// Accessing the position 0 returns the element// \"a\": 3, introducing \"a\" occurred first in the sequence// and occurred a total of 3 timesvar element = orderedCounter.element[1]element.key // \"a\"element.value // 3Random Access To Unique, Ordered ElementsWhen working with unique sequences, like time-series, it is often useful to access the elements of the unique sequence in order and using a unique identifier.OrderedDictionary provides a type that can do both:var timeSeries = [ [\"id\", \"t0\", \"value\": \"0.1\"], [\"id\", \"t1\", \"value\": \"1.1\"], [\"id\", \"t2\", \"value\": \"2.1\"],]var series = OrderedDictionary&lt;String, Dictionary&lt;String,String&gt;&gt; = [:]for datapoint in timeSeries { series[datapoint[\"id\"]!] = datapoint}// Accessing the key \"t1\" returns the datapoint// associated with the id \"t1\"series[\"t1\"] // [\"id\": \"t1\", value: \"1.1\"]// Accessing the element at index 2 returns// the element associated with index 2 in the// time seriesvar element = series.elements[2]element.key // \"t2\"element.value // [\"id\": \"t2\", \"value\": \"2.1\"]That’ it!By using the swift-collection package and OrderedDictionary, you can take advantage of both Dictionary key-value store properties and element ordering in Swift." }, { "title": "Customize the Favicon", "url": "/posts/customize-the-favicon/", "categories": "Blogging, Tutorial", "tags": "favicon", "date": "2019-08-10 12:34:00 -0400", "snippet": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons...", "content": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.Download &amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "Getting Started", "url": "/posts/getting-started/", "categories": "Blogging, Tutorial", "tags": "getting started", "date": "2019-08-09 08:55:00 -0400", "snippet": "Creating a Site RepositoryWhen creating your site repository, you have two options depending on your needs:Option 1. Using the Starter (Recommended)This approach simplifies upgrades, isolates unnec...", "content": "Creating a Site RepositoryWhen creating your site repository, you have two options depending on your needs:Option 1. Using the Starter (Recommended)This approach simplifies upgrades, isolates unnecessary files, and is perfect for users who want to focus on writing with minimal configuration. Sign in to GitHub and navigate to the starter. Click the Use this template button and then select Create a new repository. Name the new repository &lt;username&gt;.github.io, replacing username with your lowercase GitHub username.Option 2. Forking the ThemeThis approach is convenient for modifying features or UI design, but presents challenges during upgrades. So don’t try this unless you are familiar with Jekyll and plan to heavily modify this theme. Sign in to GitHub. Fork the theme repository. Name the new repository &lt;username&gt;.github.io, replacing username with your lowercase GitHub username.Setting up the EnvironmentOnce your repository is created, it’s time to set up your development environment. There are two primary methods:Using Dev Containers (Recommended for Windows)Dev Containers offer an isolated environment using Docker, which prevents conflicts with your system and ensures all dependencies are managed within the container.Steps: Install Docker: On Windows/macOS, install Docker Desktop. On Linux, install Docker Engine. Install VS Code and the Dev Containers extension. Clone your repository: For Docker Desktop: Start VS Code and clone your repo in a container volume. For Docker Engine: Clone your repo locally, then open it in a container via VS Code. Wait for the Dev Containers setup to complete.Setting up Natively (Recommended for Unix-like OS)For Unix-like systems, you can set up the environment natively for optimal performance, though you can also use Dev Containers as an alternative.Steps: Follow the Jekyll installation guide to install Jekyll and ensure Git is installed. Clone your repository to your local machine. If you forked the theme, install Node.js and run bash tools/init.sh in the root directory to initialize the repository. Run command bundle in the root of your repository to install the dependencies.UsageStart the Jekyll ServerTo run the site locally, use the following command:$ bundle exec jekyll s If you are using Dev Containers, you must run that command in the VS Code Terminal.After a few seconds, the local server will be available at http://127.0.0.1:4000.ConfigurationUpdate the variables in _config.yml as needed. Some typical options include: url avatar timezone langSocial Contact OptionsSocial contact options are displayed at the bottom of the sidebar. You can enable or disable specific contacts in the _data/contact.yml file.Customizing the StylesheetTo customize the stylesheet, copy the theme’s assets/css/jekyll-theme-chirpy.scss file to the same path in your Jekyll site, and add your custom styles at the end of the file.Starting with version 6.2.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, copy the main SASS file _sass/main.scss to the _sass directory in your site’s source, then create a new file _sass/variables-hook.scss and assign your new values there.Customizing Static AssetsStatic assets configuration was introduced in version 5.1.0. The CDN of the static assets is defined in _data/origin/cors.yml. You can replace some of them based on the network conditions in the region where your website is published.If you prefer to self-host the static assets, refer to the chirpy-static-assets repository.DeploymentBefore deploying, check the _config.yml file and ensure the url is configured correctly. If you prefer a project site and don’t use a custom domain, or if you want to visit your website with a base URL on a web server other than GitHub Pages, remember to set the baseurl to your project name, starting with a slash, e.g., /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy Using Github ActionsPrepare the following: If you’re on the GitHub Free plan, keep your site repository public. If you have committed Gemfile.lock to the repository, and your local machine is not running Linux, update the platform list of the lock file: $ bundle lock --add-platform x86_64-linux Next, configure the Pages service: Go to your repository on GitHub. Select the Settings tab, then click Pages in the left navigation bar. In the Source section (under Build and deployment), select GitHub Actions from the dropdown menu. Push any commits to GitHub to trigger the Actions workflow. In the Actions tab of your repository, you should see the workflow Build and Deploy running. Once the build is complete and successful, the site will be deployed automatically. You can now visit the URL provided by GitHub to access your site.Manual Build and DeploymentFor self-hosted servers, you will need to build the site on your local machine and then upload the site files to the server.Navigate to the root of the source project, and build your site with the following command:$ JEKYLL_ENV=production bundle exec jekyll bUnless you specified the output path, the generated site files will be placed in the _site folder of the project’s root directory. Upload these files to your target server." }, { "title": "Writing a New Post", "url": "/posts/write-a-new-post/", "categories": "Blogging, Tutorial", "tags": "writing", "date": "2019-08-08 02:10:00 -0400", "snippet": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and P...", "content": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateTo accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Animal, Insect]tags: [bee]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:Adding author information in _data/authors.yml (If your website doesn’t have this file, don’t hesitate to create one).&lt;author_id&gt;: name: &lt;full name&gt; twitter: &lt;twitter_of_author&gt; url: &lt;homepage_of_author&gt;And then use author to specify a single entry or authors to specify multiple entries:---author: &lt;author_id&gt; # for single entry# orauthors: [&lt;author1_id&gt;, &lt;author2_id&gt;] # for multiple entries---Having said that, the key author can also identify multiple entries. The benefit of reading the author information from the file _data/authors.yml is that the page will have the meta tag twitter:creator, which enriches the Twitter Cards and is good for SEO.Post DescriptionBy default, the first words of the post are used to display on the home page for a list of posts, in the Further Reading section, and in the XML of the RSS feed. If you don’t want to display the auto-generated description for the post, you can customize it using the description field in the Front Matter as follows:---description: Short summary of the post.---Additionally, the description text will also be displayed under the post title on the post’s page.Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MediaWe refer to images, audio and video as media resources in Chirpy.URL PrefixFrom time to time we have to define duplicate URL prefixes for multiple resources in a post, which is a boring task that you can avoid by setting two parameters. If you are using a CDN to host media files, you can specify the cdn in _config.yml. The URLs of media resources for site avatar and posts are then prefixed with the CDN domain name. cdn: https://cdn.com To specify the resource path prefix for the current post/page range, set media_subpath in the front matter of the post: ---media_subpath: /path/to/media/--- The option site.cdn and page.media_subpath can be used individually or in combination to flexibly compose the final resource URL: [site.cdn/][page.media_subpath/]file.extImagesCaptionAdd italics to the next line of an image, then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeTo prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image.![Desktop View](/assets/img/sample/mockup.png){: width=\"700\" height=\"400\" } For an SVG, you have to at least specify its width, otherwise it won’t be rendered.Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=\"700\" h=\"400\" }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. Once the position is specified, the image caption should not be added. Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } Dark/Light modeYou can make images follow theme preferences in dark/light mode. This requires you to prepare two images, one for dark mode and one for light mode, and then assign them a specific class (dark or light):![Light mode only](/path/to/light-mode.png){: .light }![Dark mode only](/path/to/dark-mode.png){: .dark }ShadowThe screenshots of the program window can be considered to show the shadow effect:![Desktop View](/assets/img/sample/mockup.png){: .shadow }Preview ImageIf you want to add an image at the top of the post, please provide an image with a resolution of 1200 x 630. Please note that if the image aspect ratio does not meet 1.91 : 1, the image will be scaled and cropped.Knowing these prerequisites, you can start setting the image’s attribute:---image: path: /path/to/image alt: image alternative text---Note that the media_subpath can also be passed to the preview image, that is, when it has been set, the attribute path only needs the image file name.For simple use, you can also just use image to define the path.---image: /path/to/image---LQIPFor preview images:---image: lqip: /path/to/lqip-file # or base64 URI--- You can observe LQIP in the preview image of post \"Text and Typography\".For normal images:![Image description](/path/to/image){: lqip=\"/path/to/lqip-file\" }VideoSocial Media PlatformYou can embed videos from social media platforms with the following syntax:{% include embed/{Platform}.html id='{ID}' %}Where Platform is the lowercase of the platform name, and ID is the video ID.The following table shows how to get the two parameters we need in a given video URL, and you can also know the currently supported video platforms. Video URL Platform ID https://www.youtube.com/watch?v=H-B46URT4mg youtube H-B46URT4mg https://www.twitch.tv/videos/1634779211 twitch 1634779211 https://www.bilibili.com/video/BV1Q44y1B7Wf bilibili BV1Q44y1B7Wf Video FilesIf you want to embed a video file directly, use the following syntax:{% include embed/video.html src='{URL}' %}Where URL is a URL to a video file e.g. /path/to/sample/video.mp4.You can also specify additional attributes for the embedded video file. Here is a full list of attributes allowed. poster='/path/to/poster.png' — poster image for a video that is shown while video is downloading title='Text' — title for a video that appears below the video and looks same as for images autoplay=true — video automatically begins to play back as soon as it can loop=true — automatically seek back to the start upon reaching the end of the video muted=true — audio will be initially silenced types — specify the extensions of additional video formats separated by |. Ensure these files exist in the same directory as your primary video file.Consider an example using all of the above:{% include embed/video.html src='/path/to/video.mp4' types='ogg|mov' poster='poster.png' title='Demo video' autoplay=true loop=true muted=true%}AudiosIf you want to embed an audio file directly, use the following syntax:{% include embed/audio.html src='{URL}' %}Where URL is a URL to an audio file e.g. /path/to/audio.mp3.You can also specify additional attributes for the embedded audio file. Here is a full list of attributes allowed. title='Text' — title for an audio that appears below the audio and looks same as for images types — specify the extensions of additional audio formats separated by |. Ensure these files exist in the same directory as your primary audio file.Consider an example using all of the above:{% include embed/audio.html src='/path/to/audio.mp3' types='ogg|wav|aac' title='Demo audio'%}Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Highlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` The Jekyll tag {% highlight %} is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho 'No more line numbers!'```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=\"path/to/file\" }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains 'Pack' %} This product's title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.MathematicsWe use MathJax to generate mathematics. For website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---After enabling the mathematical feature, you can add math equations with the following syntax: Block math should be added with $$ math $$ with mandatory blank lines before and after $$ Inserting equation numbering should be added with $$\\begin{equation} math \\end{equation}$$ Referencing equation numbering should be done with \\label{eq:label_name} in the equation block and \\eqref{eq:label_name} inline with text (see example below) Inline math (in lines) should be added with $$ math $$ without any blank line before or after $$ Inline math (in lists) should be added with \\$$ math $$&lt;!-- Block math, keep all blank lines --&gt;$$LaTeX_math_expression$$&lt;!-- Equation numbering, keep all blank lines --&gt;$$\\begin{equation} LaTeX_math_expression \\label{eq:label_name}\\end{equation}$$Can be referenced as \\eqref{eq:label_name}.&lt;!-- Inline math in lines, NO blank lines --&gt;\"Lorem ipsum dolor sit amet, $$ LaTeX_math_expression $$ consectetur adipiscing elit.\"&lt;!-- Inline math in lists, escape the first `$` --&gt;1. \\$$ LaTeX_math_expression $$2. \\$$ LaTeX_math_expression $$3. \\$$ LaTeX_math_expression $$ Starting with v7.0.0, configuration options for MathJax have been moved to file assets/js/data/mathjax.js, and you can change the options as needed, such as adding extensions.If you are building the site via chirpy-starter, copy that file from the gem installation directory (check with command bundle info --path jekyll-theme-chirpy) to the same directory in your repository.MermaidMermaid is a great diagram generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-07 23:33:00 -0400", "snippet": "HeadingsH1 — headingH2 — headingH3 — headingH4 — headingParagraphQuisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risu...", "content": "HeadingsH1 — headingH2 — headingH3 — headingH4 — headingParagraphQuisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risus, sit amet vulputate urna facilisis ac. Phasellus semper dui non purus ultrices sodales. Aliquam ante lorem, ornare a feugiat ac, finibus nec mauris. Vivamus ut tristique nisi. Sed vel leo vulputate, efficitur risus non, posuere mi. Nullam tincidunt bibendum rutrum. Proin commodo ornare sapien. Vivamus interdum diam sed sapien blandit, sit amet aliquam risus mattis. Nullam arcu turpis, mollis quis laoreet at, placerat id nibh. Suspendisse venenatis eros eros.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph ToDo list Job Step 1 Step 2 Step 3 Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blocksCommonThis is a common code snippet, without syntax highlight and line number.Specific Languageif [ $? -ne 0 ]; then echo \"The command was not successful.\"; #do the needful / exitfi;Specific filename@import \"colors/light-typography\", \"colors/dark-typography\";MathematicsThe mathematics powered by MathJax:\\[\\begin{equation} \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} \\label{eq:series}\\end{equation}\\]We can reference the equation as \\eqref{eq:series}.When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dImagesDefault (with caption)Full screen width and center alignmentLeft alignedFloat to leftPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Float to rightPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Dark/Light mode &amp; ShadowThe image below will toggle dark/light mode based on theme preference, notice it has shadows.VideoReverse Footnote The footnote source &#8617;&#xfe0e; The 2nd footnote source &#8617;&#xfe0e; " }, { "title": "Cramming Effectively - How to Cover 6 Months of Study in 72 Hours", "url": "/posts/cover-6mn-study-in-3days/", "categories": "Learning", "tags": "learning, strategy", "date": "2019-03-20 04:07:44 -0400", "snippet": "When time is limited before an exam, strategic cramming can help you cover a significant amount of material and maximize your chances of success. In this blog post, we’ll outline the key principles...", "content": "When time is limited before an exam, strategic cramming can help you cover a significant amount of material and maximize your chances of success. In this blog post, we’ll outline the key principles from Dr. Justin Sung’s video on “How to Finish 6 Months of Study in 72 Hours.”Prioritize StrategicallyAccept that you won’t be able to cover everything and focus on the most important material first. Identify the learning objectives and prioritize topics based on their significance and the level of comprehension required (fact recall, analysis, or evaluation).Employ Logical ReasoningUse logical reasoning to deduce answers for unknown topics during the exam. Connect related concepts to make educated guesses, even when you don’t know the exact answer.Focus on High-Logic-Point MaterialPrioritize understanding major concepts and principles that have high “logic points.” These are the topics that can help you understand other concepts, allowing you to infer answers to other questions.Structured Cramming Schedule Day 1: Identify key learning objectives and study major concepts that will give you the best chance of answering questions. Day 2: Focus on mid-level concepts that support the high-level material covered on day one. Day 3: Review learning objectives for gaps and memorize important but detailed information.Consider Efficient Learning MethodsFor a more comprehensive approach to learning, consider joining the iCanStudy program. It offers guided strategies to help students study more efficiently and achieve better results.Remember, cramming should be a last resort, and consistent, effective learning throughout the semester is the best way to prepare for exams. However, if time is limited, following these principles can help you make the most of your cramming efforts." }, { "title": "Using SwiftUI in a playground", "url": "/posts/swiftui-playground/", "categories": "SwiftUI", "tags": "swiftui, xcode, swift playground, swift5.1", "date": "2019-03-13 18:02:00 -0400", "snippet": "You can totally start learning and experimenting with SwiftUI in an Xcode playground. Just import PlaygroundSupport, and assign a UIHostingController as your live view:import SwiftUIimport Playgrou...", "content": "You can totally start learning and experimenting with SwiftUI in an Xcode playground. Just import PlaygroundSupport, and assign a UIHostingController as your live view:import SwiftUIimport PlaygroundSupportstruct MyView: View { var body: some View { Text(\"Hello, world!\") }}let vc = UIHostingController(rootView: MyView())PlaygroundPage.current.liveView = vc" }, { "title": "Mastering Concepts with the Feynman Technique", "url": "/posts/feynman-technique/", "categories": "Learning", "tags": "learning, technique, feynman", "date": "2019-03-11 02:54:10 -0400", "snippet": "The Feynman Technique is a powerful learning strategy that can significantly enhance your understanding of new concepts and improve your retention, making it an invaluable tool for students and lif...", "content": "The Feynman Technique is a powerful learning strategy that can significantly enhance your understanding of new concepts and improve your retention, making it an invaluable tool for students and lifelong learners alike. Named after the renowned physicist Richard Feynman, this technique promotes active engagement with the material, allowing you to identify gaps in your knowledge and reinforce your understanding. Here’s a detailed look at the four key steps of the Feynman Technique.Step 1: Choose a TopicThe first step in the Feynman Technique is to select a topic you want to learn. This could be anything from a complex scientific theory to a new language or a historical event. Take the time to study the subject thoroughly using textbooks, articles, videos, or any resources available to you. The goal is to familiarize yourself with the core concepts and terminology associated with the topic.Tips for Choosing a Topic: Pick something that genuinely interests you or is relevant to your studies. Ensure that the topic has enough depth to warrant exploration but is manageable within your current knowledge base.Step 2: Teach ItOnce you have a solid understanding of the topic, the next step is to teach it. Write down your explanation as if you were teaching it to someone else, such as a friend or a student. This could be in the form of notes, a presentation, or even a video.Why Teaching Works: Teaching forces you to articulate your thoughts clearly, which helps solidify your understanding. As you attempt to explain the topic, you may uncover gaps in your knowledge that need to be addressed.Step 3: Go Back and ReviewIf you encounter difficulties while teaching the topic, don’t hesitate to return to your study materials. This step is crucial for filling in the gaps in your understanding. Review the areas where you struggled, take additional notes, and seek clarification on complex points.Importance of Review: This iterative process reinforces your learning by connecting new information with existing knowledge. It helps you develop a more comprehensive understanding of the topic.Step 4: SimplifyThe final step in the Feynman Technique is to simplify your explanation. Use plain language and analogies to make the concept accessible. If your explanation still feels convoluted or confusing, it’s a sign that you may not have fully grasped the material yet.Techniques for Simplification: Break down complex ideas into smaller, digestible parts. Use analogies that relate the new concept to something familiar. Avoid jargon and technical terms unless absolutely necessary.ConclusionThe Feynman Technique is not just a method for rote memorization; it’s a dynamic approach to learning that encourages active engagement and critical thinking. By choosing a topic, teaching it, reviewing your understanding, and simplifying your explanation, you can deepen your comprehension and enhance long-term retention of the material.Whether you’re preparing for an exam, learning a new skill, or seeking to understand a complex idea, the Feynman Technique can help you master the content effectively. Embrace this technique in your learning journey, and watch as your understanding and confidence grow!" }, { "title": "Overcoming the Emotional Barriers to Learning Hard Things", "url": "/posts/overcoming-emotional-barriers/", "categories": "Learning", "tags": "learning, strategy", "date": "2019-03-10 00:46:00 -0500", "snippet": "Learning new, challenging skills can be incredibly rewarding, but it’s also common to face significant emotional hurdles along the way. In this blog post, I’ll share insights on the emotional chall...", "content": "Learning new, challenging skills can be incredibly rewarding, but it’s also common to face significant emotional hurdles along the way. In this blog post, I’ll share insights on the emotional challenges of learning hard things and provide strategies to help you push through those barriers.The Real Hurdle: EmotionsOne of the most important lessons I’ve learned from my research on ultralearning is that how we feel about learning is the overwhelming cause of the results we experience. While intelligence, talent, and good teachers are helpful, the real obstacle to learning is often the emotional resistance we face—whether it’s frustration, fear, or lack of interest.Confronting the Frustration BarrierWhen trying to learn something new and difficult, it’s normal to feel frustrated. This frustration barrier occurs when the gap between your current abilities and the desired skill level feels insurmountable. I vividly remember experiencing this when I tried to learn salsa dancing. I felt clumsy, self-conscious, and tempted to give up.Strategies for Overcoming Emotional Barriers Dive straight into learning: Immersing yourself in the learning process can help shorten the frustration period. Don’t wait until you feel ready—start practicing and learning as soon as possible. Avoid unhealthy comparisons: It’s easy to get discouraged when comparing yourself to others who seem to be progressing faster. Remember that everyone has their own unique learning journey. Focus on your own progress and celebrate small wins. Embrace frustration as normal: Reframe your emotional response to learning. Frustration, confusion, and self-doubt are all normal parts of the process. Expect them to arise and be prepared to push through. The Payoff: Growth and FulfillmentBy shifting your mindset and persevering through challenging experiences, you can overcome emotional barriers and reap the rewards of learning hard things. Pushing past frustration leads to growth, greater enjoyment, and a broader sense of life’s possibilities.Learning difficult skills is not easy, but it’s worth the effort. Embrace the emotional challenges as opportunities for personal development. With the right strategies and a resilient mindset, you can achieve your learning goals and unlock your full potential." }, { "title": "Five Steps to Ace Your Exam", "url": "/posts/five-steps-ace-your-exam/", "categories": "Learning", "tags": "learning, strategy, exam", "date": "2019-03-07 23:40:08 -0500", "snippet": "Five Scientific Steps to Ace Your Next ExamIn Scott H. Young’s insightful article, “Five Scientific Steps to Ace Your Next Exam,” he presents a research-backed approach to studying that can signifi...", "content": "Five Scientific Steps to Ace Your Next ExamIn Scott H. Young’s insightful article, “Five Scientific Steps to Ace Your Next Exam,” he presents a research-backed approach to studying that can significantly enhance your exam performance. Drawing from principles of cognitive science, Young emphasizes structured and intentional learning strategies over passive review. Here’s a breakdown of his key recommendations:1. Spacing: Spread Your Study SessionsPrinciple: Rather than cramming, distribute your study sessions over time. Why It Works: Research shows that spaced repetition leads to better retention of information. By reviewing material multiple times over an extended period, you reinforce your learning and improve recall. Actionable Tip: Aim to review each topic at least five times from the moment you first learn it until the exam. Create a study schedule that spaces out these sessions to maximize retention. 2. Retrieval Practice: Actively Recall InformationPrinciple: Instead of rereading notes or textbooks, focus on actively recalling information from memory. Why It Works: Free recall—attempting to remember information without looking—has been shown to produce better learning outcomes than passive review methods like rereading or creating concept maps. Actionable Tip: After studying a topic, close your materials and write down everything you remember. Use flashcards or practice tests to reinforce your recall abilities. 3. Practice with Relevant ProblemsPrinciple: Engage with mock exams and practice problems that closely match the format of the actual exam. Why It Works: This strategy, known as transfer-appropriate processing, ensures that your practice effectively translates into real exam performance. Actionable Tip: Prioritize solving past exam papers, in-class problems, and relevant writing prompts. Familiarize yourself with the types of questions you’ll encounter and practice under similar conditions. 4. Deep Understanding: Focus on Core ConceptsPrinciple: Strive for a deep understanding of the material rather than rote memorization of facts. Why It Works: Understanding the underlying concepts allows you to apply your knowledge flexibly and adaptively. The Feynman Technique—explaining concepts in simple terms—can help identify gaps in your understanding. Actionable Tip: After studying a concept, try to explain it as if teaching it to someone else. If you struggle, revisit the material and clarify your understanding until you can explain it confidently. 5. Simulate Exam Conditions: Reduce AnxietyPrinciple: Practice under conditions that mimic the actual exam environment to alleviate anxiety. Why It Works: Simulating the exam setting helps you manage stress and improves focus. It also allows you to gauge your performance realistically. Actionable Tip: Conduct full mock exams under timed conditions, replicating the seating, materials, and constraints of the actual test. This exposure will help you feel more prepared and less anxious on exam day. ConclusionScott H. Young’s advice emphasizes the importance of structured, intentional learning strategies that actively engage students with the material. By incorporating these five scientific steps into your study routine, you not only prepare effectively for exams but also foster long-term mastery of the subject matter.Implementing these strategies can transform your approach to studying, making it more effective and rewarding. As you prepare for your next exam, remember that consistent practice, active engagement, and a focus on understanding will lead to greater success.By following these principles, you can enhance your learning experience and improve your performance on exams, turning challenges into opportunities for growth." }, { "title": "The Swift5.1 features that power SwiftUI's API", "url": "/posts/swift5.1-features-power-swiftui-api/", "categories": "SwiftUI", "tags": "swiftui, language features, swift5.1", "date": "2019-03-06 17:02:00 -0500", "snippet": "The introduction of SwiftUI, Apple’s declarative new UI framework, was clearly one of the most impactful announcements made during this year’s WWDC conference. As a brand new way of building UIs fo...", "content": "The introduction of SwiftUI, Apple’s declarative new UI framework, was clearly one of the most impactful announcements made during this year’s WWDC conference. As a brand new way of building UIs for all of Apple’s platforms, using a coding style that’s vastly different from the way UIKit works, SwiftUI isn’t just a new framework - it’s paradigm shift.As a new, modern task on UI development for Apple’s platforms, SwiftUI also pushes the Swift language itself to new limits - by making heavy use of a set of key new syntax features, that are being introduced as part of Swift5.1, in order to provide a very DSL-like API. (Domain Specific Language)Let’s take a look at those features, and how learning more about them - and how they work - can let us again a more thorough understanding of SwiftUI’s API and how it was built. While this won’t be an introduction to SwiftUI per se, it’ll hopefully serve as a bit of a peek under the hood of Apple’s exciting new UI framework.Opaque return typesOne feature that sort of stands out when looking at most of the SwiftUI sample code that has been shared so far, is the new some keyword. Introduced through Swift Evolution proposal SE-0244 - this new keyword enables functions, subscripts, and computed properties to declare opaque return types.What that means, is that even generic protocols(a protocol that either has associated types or references to Self) can now be used as return types - just as if they were concrete types - like no-generic protocols, classes or structs. When using SwiftUI, some is very often used when declaring a view’s body - like this:struct ContentView: View { var body: some View { Text(\"Hello, world!\") }}The View protocol is used to define SwiftUI view descriptions, and in order to enable each view to decide what type to use for its body property - that property requirement is defined using an associated Body type. Prior to Swift5.1, attempting to reference such a protocol(without the some keyword), would lead to a compile error saying that View can only be used as a generic constraint. To work around that, we’d then have to specify a concrete type confirming to View instead - for example like this:struct ContentView: View { var body: Text { Text(\"Hello, world!\") }}Another option would be to use type erasure, and require each View implementation to be boxed into a type-erased AnyView instance before being returned:struct ContentView: View { var body: AnyView { AnyView(Text(\"Hello, word!\")) }}But now, by using the some keyword, we’re free to return any value confirming to the specific protocol(like View, in the case of SwiftUI) - and any other code that calls into our implementation can still use all of the properties and methods from that protocol when working with our return value - without requiring us to use wrapper types (like AnyView), or to break our code’s encapsulation by exposing concrete types as part our API.A nice side-effect of this new keyword is the additional flexibility it gives us, since we no longer have to modify our public API in order to change what exact return type that’s used under the hood. That’s especially important for a view framework, like SwiftUI - since a kye part of writing maintainable view code is to constantly refactor and split up the various parts of a UI into separate, smaller building blocks.Omitted return keywordsPerhaps not as important as the new some keyword, but a nice implementation in terms of consistency, and a big factor when it comes to how lightweight SwiftUI’s API feels - is the fact that the return keyword can now be omitted for single-expression functions.Swift Evolution proposal SE-0255 made functions and computed properties act the same way as closures - in that if there’s only one expression within them, using the return keyword is no longer required - making the following two implementations act the exact same way:struct ContentView: View { var body: some View { // Using an explicit return keyword return Text(\"Hello, world!\") }}struct ContentView: View { var body: some View { // Omitting the return keyword, just like within a closure Text(\"Hello, world!\") }}While the above might take a while to get used to, it does server as a way to make single expressions within functions and computed properties a bit more clean - for example in things like factory methods:func makeProfileViewController(for user: User) -&gt; UIViewController { ProfileViewController( logicController: ProfileLogicController( user: user, networking: networking ) )}However, the compiler will continue to accept code that uses the return keyword, as well as code that omits it - so each developer is free to choose whichever style that they prefer.Function buildersWith both the some keyword and omitted returns, we now have an answer as to how SwiftUI’s top-level View declaration API is made possible - but so far we still don’t have an explanation for how multiple views can be grouped together, without any sort of keyword or additional syntax - like this:struct HeaderView: View { let image: UIImage let title: String let subtitle: String var body: some View { VStack { // Here three separate expressions are evaluated, // without any return keyword or additional syntax. Image(uiImage: image) Text(title) Text(subtitle) } }}SwiftUI’s grouping views - such as VStack, HStack, and Group - enable multiple views to be grouped together by simply creating new instances within a closure. Since there are closures with multiple expressions, it means that we’re not dealing with an omitted return keyword here - so how exactly is that kind of syntax made possible?The answer, is function builders - which is such a new feature that, at the time of writing, it doesn’t even have a formal proposal yet. An initial draft for a proposal can be found here, but interestingly this feature has already been implemented in the Swift compiler.Function builders enables the builder pattern to be implemented using closures - providing a very DSL-like development experience, by passing the expressions defined within such a closure to a dedicated builder type.Without the new function builder feature, we’d have to manually create a builder in order to construct instances of containers like VStack, giving us code that’s look something like this:struct HeaderView: View { let image: UIImage let title: String let subtitle: String var body: some View { var builder = VStackBuilder() builder.add(Image(uiImage: image)) builder.add(Text(title)) builder.add(Text(subtitle)) return builder.build() }} The above definition isn’t bad, but it does make the API feel much less lightweight.So how do function builders work? It all starts with the new @functionBuilder attribute (or @_functionBuilder, as it’s currently implemented as, since this feature is still considered a private implementation detail) - which marks a given type as being a builder.Similar to how new custom string literal API works, a builder then declares different overloads of the buildBlock method in order to provide support for closures containing various kinds of expressions. For example, here is “paraphrased” implementation of what SwiftUI’s own ViewBuilder type might look like:@functionBuilderstruct ViewBuilder { // Build a value from an empty closure, resulting in an // empty view in this case: static func buildBlock() -&gt; EmptyView { return EmptyView() } // Build a sing view from a closure that contains a single view expression: static func buildBlock&lt;V: View&gt;(_ view: V) -&gt; some View { return view } // Build a combining TupleView from a closure that contains two view expressions: static func buildBlock&lt;A: View, B: View&gt;(_ viewA: A, _viewB: B) -&gt; some View { return TupleView((viewA, viewB)) } // An so on, and so forth}Note how each closure variant needs to be explicitly handled by the builder above, since we might be dealing with different kinds of View implementations defined within the same closure. If that wasn’t the case, ViewBuilder could’ve instead used a variadic parameter to handle closures containing multiple expressions - like this:@functionBuilderstruct ViewBuilder { static func buildBlock(_ views: View...) -&gt; CombinedView { return CombinedView(views: views) }} The above code is just an example, it won’t even compile, since View has an associated type.With the above VieBuilder type in place, the compiler will now synthesize an attribute that matches its name (@ViewBuilder) - which we can then use to mark all the closure parameters that we wish to use our new builder with, like this:struct VStack&lt;Content: View&gt;: View { init(@ViewBuilder builder: () -&gt; Content) { // A function builder closure can be called just like any other, // and the resulting expression can then be used to, // for instance, construct a container view. let content = builder() ... }}Using the above two pieces - a function builder type, and closures marked as users of that type, building really lightweight DSLs now becomes possible - which is exactly what Apple has done to achieve SwiftUI’s view building syntax:VStack { Image(uiImage: image) Text(title) Text(subtitle)}As a feature, function builders definitely lean towards the advanced end of the spectrum - but the beauty of them is that developers using DSL-based frameworks, like SwiftUI, should ideally never even notice them - since the whole builder part simply becomes an implementation detail of the DSL itself.Property wrappersThe final core new Swift5.1 feature that SwiftUI’s API is powered by is property wrappers (formally known as “property delegates”). Introduced as part of proposal SE-0258, this new feature enables property values to be automatically wrapped using specific types. It works quite similarly to function builders in that regard - in that implementing property wrappers requires both a custom attribute, and a type that handles that attribute.SwiftUI uses property wrappers to make it much easier to define various kinds of bindable properties. For example, in order to define a property for managing a part of a view’s state, the @State attribute can be used to automatically wrap such a property’s value in an instance of the bindable State type:struct SettingView: View { @State var saveHistory: Bool @State var enableAutofill: Bool var body: some View { return VStack { // We can now access bindable versions of our state // properties by prefixing their name with '$': Toggle(isOn: $saveHistory) { Text(\"Save browsing history\") } Toggle(isOn: $enableAutofill) { Text(\"Autofill my information\") } } }}Since all that property wrappers really do is to act as a sort of interface between a property value and an underlying storage type, the above code sample is essentially equivalent to this next implementation - which does the exact same thing, but by using the underlying State struct directly instead:struct SettingsView: View { var saveHistory: State&lt;Bool&gt; var enableAutofill: State&lt;Bool&gt; var body: some View { VStack { Toggle(isOn: saveHistory.binding) { Text(\"Save browsing history\") } Toggle(isOn: enableAutofill.binding) { Text(\"Autofill my information\") } } }}Again, the design of property wrappers is very similar to that of function builders, in that delegation attributes (such as @State) gets mapped to their corresponding underlying type using the @propertyWrapper attribute. For example, here’s a simplified version of what the public API of SwiftUI’s State struct looks like:@propertyWrapperstruct State&lt;Value&gt; { init(initialValue: Value) { ... } var wrappedValue: Value { get { ... } set { ... } }}What’s particularly exciting about property wrappers, is that they open up opportunities for a lot of different kinds of boilerplate to be eliminated, even in our own code as well. For example, we could define a @Transformed attribute that lets us automatically apply transformations to various values, or a @Database attribute for automatically syncing property values to an underlying database - there’s a ton of different possibilities here.ConclusionSwiftUI doesn’t only bring a new way of building UIs for Apple’s platform, and brand new Swift coding styles - it has also most likely been the driving factor behind many of new features that are being introduced in Swift5.1 - which makes the language more powerful for everyone, even those who are not yet adopting SwiftUI itself.Thanks for reading!" }, { "title": "Self-improvement, No such thing as Miracle", "url": "/posts/no-such-thing-as-miracle-feynman/", "categories": "Learning", "tags": "learning, wisdom, strategy", "date": "2019-02-25 22:59:11 -0500", "snippet": "In a recent self-improvement video, Richard Feynman, the renowned physicist and Nobel laureate, shared invaluable insights on the nature of learning, particularly in complex subjects like quantum m...", "content": "In a recent self-improvement video, Richard Feynman, the renowned physicist and Nobel laureate, shared invaluable insights on the nature of learning, particularly in complex subjects like quantum mechanics. His perspective emphasizes that success in these areas is not a result of innate talent but rather a commitment to hard work and persistent study.The Essence of LearningFeynman asserts that anyone can master difficult topics through dedication. He reflects on his own journey, noting that his achievements stemmed from consistent effort rather than any extraordinary genius. This idea is empowering; it suggests that with the right mindset and work ethic, we can all tackle challenging subjects.Visualizing the AbstractOne of Feynman’s key points is the inherent difficulty in visualizing concepts at the subatomic level. Our brains are wired for everyday experiences, which makes it challenging to grasp the abstract nature of quantum mechanics. To bridge this gap, Feynman advocates for the use of mathematics as a tool to understand these complex ideas. He likens this process to how a computer operates—calculating results without necessarily “seeing” them.The Path to MasteryFeynman’s approach to mastering difficult topics can be summarized as follows: Practice and Study: Continuous engagement with the material is crucial. This means not just reading or skimming but diving deep into the subject matter. Refining Mental Models: Over time, we should aim to refine our understanding and mental representations of concepts. This iterative process helps solidify knowledge and improve comprehension. Embrace Challenges: Feynman encourages learners to embrace the difficulty of complex subjects. Rather than shying away from challenging material, we should confront it head-on, using it as an opportunity for growth. ConclusionUltimately, Richard Feynman’s wisdom serves as a reminder that learning is a journey that requires dedication, practice, and a willingness to engage with difficult concepts. By adopting his principles, we can enhance our understanding of complex topics and develop a more profound appreciation for the learning process. As we strive to master new skills and knowledge, let us remember that persistence and effort are the true keys to success.Incorporating Feynman’s insights into our daily learning habits can transform our approach to education, making it more effective and rewarding." }, { "title": "9 Principles of Ultra Learning", "url": "/posts/ultralearning-principles/", "categories": "Learning", "tags": "ultralearning, principles", "date": "2019-02-10 21:25:01 -0500", "snippet": "Here’s an improved format for better readability, with clear headings, bullet points, and examples for each ultralearning principle:Daily Routine for Applying Ultralearning PrinciplesTo effectively...", "content": "Here’s an improved format for better readability, with clear headings, bullet points, and examples for each ultralearning principle:Daily Routine for Applying Ultralearning PrinciplesTo effectively apply ultralearning principles in your daily routine, consider the following specific actions for each principle:1. Meta-learning (Plan Your Learning) Daily Task: Dedicate 5-10% of your learning time to research the skill. Action Steps: Identify the best resources (books, tutorials, courses). Create a learning roadmap with clear milestones. Example: If learning to code, spend the first few minutes planning what topics to focus on that day (e.g., functions, algorithms) and how they fit into the bigger picture.2. Focus (Maintain Concentration) Daily Task: Use time-blocking techniques like the Pomodoro method. Action Steps: Set specific goals for each session. Block 25-minute focused sessions with 5-minute breaks. Eliminate distractions (turn off notifications, use apps like Forest). Example: If learning a new language, block 25 minutes for speaking practice, then take a break.3. Directness (Learn by Doing) Daily Task: Engage in real-world applications of the skill every day. Action Steps: Practice in environments where you plan to use the skill. Example: If learning a new language, spend time speaking with native speakers through apps like iTalki or record yourself speaking about daily topics.4. Drill (Target Weaknesses) Daily Task: Identify areas where you struggle the most and focus on them. Action Steps: Break complex skills into smaller parts. Drill on the most difficult aspects. Example: If practicing math, spend time targeting specific problems (e.g., calculus problems you find hardest) and drill on those.5. Retrieval (Practice Recall) Daily Task: Spend 5-10 minutes recalling key concepts at the end of each session. Action Steps: Write down everything you remember without looking at notes. Use flashcards (e.g., Anki) to test yourself. Example: After reading a chapter or watching a tutorial, write down everything you remember.6. Feedback (Seek Constructive Criticism) Daily Task: Seek immediate feedback on your progress. Action Steps: Test yourself, get reviews from peers or experts, or use automated tools. Example: If learning to draw, post your daily sketches on platforms like Reddit or DeviantArt to get critiques and advice for improvement.7. Retention (Improve Memory) Daily Task: Use spaced repetition and active recall to reinforce learning. Action Steps: Schedule review sessions over increasing intervals (e.g., review on Day 1, Day 3, and Day 7). Example: Use apps like Anki to schedule spaced repetition for vocabulary words or coding concepts you’re learning.8. Intuition (Deep Understanding) Daily Task: Work on hard problems and prove concepts to yourself. Action Steps: Explain concepts out loud or teach them to others. Example: If learning programming, explain code to yourself or a peer, or write blog posts explaining the logic behind complex algorithms.9. Experimentation (Find New Approaches) Daily Task: Experiment with different learning techniques or tackle problems from different angles. Action Steps: Allocate time each day for experimentation. Example: If learning music, try different styles of playing or practicing improvisation instead of following traditional practice routines.Additional Tools for Daily Routine Learning Journal: Track your progress, record feedback, and reflect on daily learning. Daily Micro-Goals: Break down big projects into daily tasks to ensure steady progress each day.By integrating these principles into your daily learning routine, you can accelerate your progress and master new skills more effectively." }, { "title": "Refactoring SwiftUI views using functions", "url": "/posts/swiftui-refactoring-view-using-functions/", "categories": "SwiftUI", "tags": "swiftui, code structure, functions, swift", "date": "2019-02-10 15:02:00 -0500", "snippet": "I see lots of SwiftUI comments about how it’ll “force” developers to write “Pyramids of Doom” with heavily nested code, which is just as false as MVC forcing developers to build massive view contro...", "content": "I see lots of SwiftUI comments about how it’ll “force” developers to write “Pyramids of Doom” with heavily nested code, which is just as false as MVC forcing developers to build massive view controllers. Here’s one example of how nesting can be avoided, by using inline functions:struct MyView: View { var body: some View { func makeVStack() -&gt; some View { VStack { ForEach(0..&lt;5) { _ in makeHStack() } } } func makeHStack() -&gt; some View { HStack { Text(\"Leading\") Text(\"Trailing\") } } return ZStack { Color.gray makeVStack() } }}" }, { "title": "Shifting paradigms in Swift", "url": "/posts/shifting-paradigms-in-swift/", "categories": "SwiftUI", "tags": "swiftui, architecture, swift5.0", "date": "2019-01-16 17:02:00 -0500", "snippet": "Every year during WWDC, we in the Apple developer community are presented with a whole suite of new tools, APIs and technologies that we can use to further improve our apps along with the systems t...", "content": "Every year during WWDC, we in the Apple developer community are presented with a whole suite of new tools, APIs and technologies that we can use to further improve our apps along with the systems that they run on. While most of those changes tend to be quite slow and steady - a sort of natural evolution happening over a long period of time - this year, things have turned out a bit different.The introduction of technologies like SwiftUI (Apple’s new declarative UI framework), Catalyst(iOS apps running on the Mac), and Combine (a built in “Rx-like” reactive data library) might very well be considered the beginning of a “new era” of app development for Apple’s platform. That might sound a bit hyperbolic - but I think it’s fair to say that Apple haven’t attempted to make this big of a leap when it comes to their developer tools since the introduction of Swift in 2014.So what does that mean for us third party developers, and how can we prepare ourselves for undergoing a major paradigm shift over the next few years - as we move from imperative to declarative, from Objective-C to Swift, and from delegates to subscribers?Gradual adoption, gradual masteryWhen presented with any kind of technological transition - whether that’s from frame-based layouts to Auto Layout, from 32-bit to 64-bit apps, or from UIkit to SwiftUI - it can be tempting to use that transition as an opportunity to start over from a completely clean slate. Out with the old legacy code, and in with the beautiful,shiny new APIs.“We rewrote our app from the ground up” is something that you often hear companies of all sizes proudly announce in blog posts, conference talks, and even in customer-facing marketing - and both developers and customers alike often get genuinely excited when hearing sentences like that. It makes an app update seem like something fresh and brand new, rather than just an incremental upgrade.However, while full rewrites do have their metrics - and might be warranted if a code base has truly gone beyond the point of on return - they often turn out to be way less appealing in practice then they are in theory, as old bugs are replaced by new bugs, and various subtitles and handling of edge cases are missed when writing the new implementation.Especially when undergoing major paradigm shifts, gradual adoption instead lets us ease into using all of the new technologies, pattern and tools that have just been introduced - in a way that both lets us keep leveraging our existing code base, and lets us keeping shipping our app to our users on a regular basis.Gradually adopting new technologies also lets us dip our toes in the water before completely diving in - giving us a way to gradually master the new APIs and conventions as we’re starting to use them.After all, there’s no real rush when it comes to adopting new tools and frameworks - it’s not like the existing frameworks and code that we’ve been shipping will stop working overnight.Mix and matchWhile gradual adoption might sound great on paper, actually getting it done in practice can be much less straightforward. The key is often to find a nice way to “mix and match” our existing code and functionality with the new code and patterns that we’re introducing.As an example, when it comes to new framework like SwiftUI, Apple has - thankfully - already considered this, and is offering full interoperability between SwiftUI and UIKit. So if we wanted to start easing our way into adopting SwiftUI, by building a single screen using it - say a view for rendering in app promotions - then we could do so by wrapping our new SwiftUI PromotionView in an instance of UIHostingController, like this:let vc = UIHostingController(rootView: PromotionView())Since UIHostingController is just a regular UIKit view controller - it can be presented, embedded as a child, or pushed onto a navigation controller - all while being powered by SwiftUI under the hood.When a framework has a clear backward compatibility story, that’s most often a great sign - as it shows that the authors didn’t only focus on building a great new set of tools, but also on how those tools will be integrated into existing projects, which usually makes for much more complete API.Even better is when new tools and frameworks don’t only offer backward compatibility, but forward compatibility as well. Again using SwiftUI as an example, an UIView can quite easily be made SwiftUI-compatible, by wrapping it in an implementation of the UIViewPresentable protocol.Here’s an example that wraps an existing ProfileView, which is a UIView subclass used to render a user’s profile:struct ProfileSwiftUIView: UIViewPresentable { let user: User func makeUIView(context: Context) -&gt; ProfileView { return ProfileView() } func updateUIView(_ view: ProfileView, context: Context) { view.nameLabel.text = user.name view.imageView.image = user.image }}Both of the above two techniques are clear example of how new developer paradigms - like SwiftUI - can be implemented piece by piece, rather than through full rewrites, given that the right tooling has been put in place.Gradual adoption isn’t always possible, but when it is, it lets us both use new technologies to build new features - while still making full use of our existing code as well - kind of giving us the best of both worlds.Parallel implementationSince paradigm shifts usually take place over quite a long period of time(years often), simply being able to build new features using new tools an APIs isn’t always enough. We might not be prepared to drop support for old OS versions, or we might not be completely confident that a certain new tool is the right fit for what we’re trying to build.In both of those cases, using multiple, parallel implementations of the same feature can be an option. While it’s an option that certainly has a cost, it does give us a ton of additional flexibility - as we can start replacing the implementation details of a given feature using new technologies, all without either abandoning existing users, existing code.Let’s take a look at how that could be done in practice, again using SwiftUI as an example. Say we wanted to start experimenting with using SwiftUI to build one of our app’s features - to learn it, and to figure out how our code base could best make use of it - while still keep shipping our UIKit-based implementation, for now.To do that, we could use the factory pattern to create an abstraction that hides which UI framework that’s currently used to implement our feature - an article reading screen in this case:protocol ArticleViewControllerFactory { func makeViewController(for article: Article) -&gt; UIViewController}Using the above protocol , we could now easily change which UI framework that our feature will use, without changing any other part of our code base. We could start by wrapping our existing UIKit-based ArticleViewController in a factory that simply creates ne instance of it, by passing in its required dependencies - like this:struct ArticleUIKitViewControllerFactory: ArticleViewControllerFactory { let navigator: ArticleNavigator let imageLoader: ImageLoader func makeViewController(for article: Article) -&gt; UIViewController { return ArticleViewController( article: article, navigator: navigator, imageLoader: imageLoader) }}Similarly, we could create another ArticleViewControllerFactory implementation that instead creates an instance of our SwiftUI-based ArticleView - and then wraps that in a UIHostingController, which is then returned. To be able to keeping shipping our app to users using iOS 12 and below, we’ll also mark this factory implementation as being only available on iOS 13:@available(iOS 13, *)struct ArticleSwiftUIViewControllerFactory: ArticleViewControllerFactory { let navigator: ArticleNavigator let imageLoader: ImageLoader func makeViewController(for article: Article) -&gt; UIViewController { let view = ArticleView( navigator: navigator, imageLoader: imageLoader ) return UIHostingController(rootVie: view) }}With the above in place, we’d now be able to select which implementation to use based on any number of conditions - for example whether the device we’re targeting is already running on iOS 13, and whether we’ve enable a custom USE_SWIFT_UI compiler flag:let articleViewControllerFactory: ArticleViewControllerFactory = { #if USE_SWIFT_UI if #available(iOS 13, *) { return ArticleSwiftUIViewControllerFactory( navigator: navigator, imageLoader: imageLoader ) } #endif return ArticleUIKitViewControllerFactory( navigator: navigator, imageLoader: imageLoader )}()Doing something like the above may seem like a lot of unnecessary extra work (why maintain two implementations of the same feature, when we can just have one?) - but it does give us a ton of extra flexibility - and a way to start easing into a new paradigm (and giving us a “safe place” to learn all of its new patterns) while still being able to keep shipping our existing code, just like before.The above approach can also be a great way to improve the overall architecture and separation of concerns within a code base, since we’d have to make sure that our core services - like ArticleNavigator and ImageLoader in this example - are completely separated from our UI code, which usually makes for an overall clear structure and easier testing.ConclusionBig paradigm shifts can both be incredibly exciting, but also confusing, and a bit scary. While it’s very common to get the feeling that the introduction of major new APIs and technologies suddenly turned all of our existing code into “tech debt” - that’s rarely the case, and there’s often a way to be found that lets us keep leveraging our existing code’s functionality, while also starting to adopt and learn the new paradigm’s technologies and patterns.Thanks for reading!" }, { "title": "Stacking views in SwiftUI", "url": "/posts/swiftui-stacking-views/", "categories": "SwiftUI", "tags": "swiftui, ui development, swift5.1", "date": "2019-01-06 15:02:00 -0500", "snippet": "SwiftUI ships with 3 main stack types - that enables you to easily align content on either the X, Y, or Z axis. And, since all SwiftUI views are composable, they can also be nested in order to buil...", "content": "SwiftUI ships with 3 main stack types - that enables you to easily align content on either the X, Y, or Z axis. And, since all SwiftUI views are composable, they can also be nested in order to build really complex UIs:The above will result in a view which content looks like this:struct MyView: View { var body: some View { // A ZStack stacks its child views on top of each other // in terms of depth, from back to front. ZStack { Color.gray // A VStack stacks its child views vertically, from top to bottom. VStack { // The ForEach construct enables you to create // multiple views in one go, for example by // mapping a collection of elements to views. ForEach(0..&lt;5) { _ in // A HStack aligns its child views horizontally, // from the leading to the trailing edge. HStack { Text(\"Leading\") Text(\"Trailing\") } } } } }}" }, { "title": "SwiftUI mix and match", "url": "/posts/swiftui-mix-match/", "categories": "SwiftUI", "tags": "swiftui, ui development, swift", "date": "2019-01-05 15:02:00 -0500", "snippet": "You can definitely mix and match SwiftUI with UIKit/AppKit, which means that you can adopt it gradually. Any SwiftUI hierarchy can be embedded in a view controller, and UIKit views can be retrofit...", "content": "You can definitely mix and match SwiftUI with UIKit/AppKit, which means that you can adopt it gradually. Any SwiftUI hierarchy can be embedded in a view controller, and UIKit views can be retrofitted with SwiftUI support:// You can easily use SwiftUI view hierarchies in UIKit-based// UIs, by wrapping them in a UIHostingController:let vc = UIHostingController(rootView: HeaderView())// For the opposite direction, using UIViews with SwiftUI - just// create a wrapper that conforms to UIViewRepresentable, which// acts just like any other SwiftUI view:struct ProfileSwiftUIView: UIViewRepresentable { let user: User func makeUIView(context: Context) -&gt; ProfileView { return ProfileView() } func updateUIView(_ view: ProfileView, context: Context) { view.nameLabel.text = user.name view.imageView.image = user.image }}" }, { "title": "SwiftUI is a game changer", "url": "/posts/swiftui-game-changer/", "categories": "SwiftUI", "tags": "swiftui, swift5.1", "date": "2019-01-04 17:02:00 -0500", "snippet": "There is no doubt in my mind that SwiftUI is a complete game changer. This is all the code you need to define a UI that has: A navigation controller A table view Cell configuration Label font s...", "content": "There is no doubt in my mind that SwiftUI is a complete game changer. This is all the code you need to define a UI that has: A navigation controller A table view Cell configuration Label font setupstruct ArticleListView: View { let articles: [Article] var body: some View { NavigationView { List(articles.identified(by: \\.id)) { article in VStack(alignment: .leading) { Text(article.title).font(.headline) Text(article.preview).font(.subheadline) } }.navigationBarTitle(Text(\"Articles\")) } }}" }, { "title": "Launch arguments in Swift", "url": "/posts/launch-arguments-swift/", "categories": "Swift", "tags": "scripting, debugging, ui testing, swift5.0, launch arguments", "date": "2018-05-20 18:07:00 -0400", "snippet": "Launch arguments are probably most commonly used as input to command line tools.While Swift can be a great language for creating command line tools, let’s take a look at how we can also use the pow...", "content": "Launch arguments are probably most commonly used as input to command line tools.While Swift can be a great language for creating command line tools, let’s take a look at how we can also use the power of launch arguments when working on, debugging, and testing an iOS app.ParsingSwift provides two built-in ways to parse launch arguments. The first one is through the CommandLine API, which enables easy access to information passed when launching the app (even on platforms like iOS that don’t ship with a terminal). Using CommandLine.arguments we get an array of strings representing each argument, which we here use to determine whether a new implementation of a ProfileViewController should be used:func makeProfileViewController() -&gt; UIViewController { if CommandLine.arguments.contains(\"-new-profile\") { // If the \"-new-profile\" arguments was passed, then return // an instance of the new implementation. return ProfileViewController() } // Fall back to the old implementation, which is what we // still use in production. return ProfileLegacyViewController()}While CommandLine provides a simple way to get access to the “raw” arguments, it doesn’t actually provide any sophisticated parsing capabilities. To gain access to a bit more powerful features, we can actually use UserDefaults to parse launch arguments.The fact that UserDefaults contains all arguments passed on the command line - and can do basic conversion to types like Bool, Int and Double - is a bit of a hidden gem. Here we’re using that feature to enable us to override how many articles that should be loaded when performing a network request:func loadArticle(then handler: @escaping (Result&lt;[Article]&gt;) -&gt; Void) { // The UserDefaults API automatically parses types of like integers // doubles from strings passed as launch arguments. In this // case we'll treat 0 as \"no limit\". let limit = UserDefaults.standard.integer(forKey: \"article-limit\") let endpoint = Endpoint.articleList(limit: limit) dataLoader.loadData(from: endpoint) { result in handler(result.decoded()) }}PassingNow that we have ways to parse command line arguments in an iOS app, we also need a way to pass them.When running and debugging, this is done through Xcode, by adding any arguments that we want to pass to our app’s scheme. An easy way to access this option in Xcode is press ⌘⌥R, selecting “Arguments” and adding our arguments under “Arguments Passed On Launch”.So what can launch arguments be useful for? Let’s take a look at three use cases - debugging, working on a new feature and UI testing.Debug actionsLaunch arguments can provide a simple and quick way to perform common actions when debugging an app.For example, let’s say that we’re working on an app that performs a large number of network request, and that we’ve had reports from users that the app performs poorly when used on slow networks.To debug this problem, we might want to add artificial delays to all network requests, so that we can observe how our app behaves under this type of conditions. While iOS provides the Link conditioner tool, which is an excellent option for on-device debugging, when initially working on a fix we might want quicker iteration times by using the simulator.To make that happen, we can use a launch arguments to add delays to all network requests, like this:class DataLoader { private let session: URLSession private let userDefaults: UserDefaults func loadData(from endpoint: Endpoint, then handler: @escaping (Result&lt;Data&gt;) -&gt; Void) { let url = endpoint.url let task = makeTask(for: url, handler: handler) // If the app is running in debug mode, add a delay // according to the \"network-delay\" launch argument #if DEBUG let delay = userDefaults.double(forKey: \"network-delay\") DispatchQueue.main.asyncAfter(deadline: .now() + delay, execute: task.resume) #else task.resume() #endif }}Important to note above is that we wrap our delaying code in a #if DEBUG compiler condition, to prevent this code from accidentally shipped to the App Store. That way, our debug code won’t even be compiled when doing a release build.Similarly, we can add launch arguments to control other types of debug actions in our app - for example;navigating to a given screen when the app launches, whether an in-app purchase has been unlocked, or simulating data from things like the Health app.Overriding feature flagsAnother situation when launch arguments can come in handy is working on a new feature that hasn’t been fully shipped yet. Like we took a look at in “Feature flags in Swift”, using feature flags can be a great way to enable features to be gradually rolled out to users, and to perform experiments and A/B testing.However, when working on an app that relies heavily on feature flags, it can be a bit time consuming and tricky to get the app into the exact state that’s needed in order to be able to work on a specific feature. To solve that problem, we can introduce launch arguments that let us add local overrides to feature flags that normally get their value dynamically from our server:func enableSearchIfNeeded() { var override: Bool? let key = \"search\" // We first check if the argument was actually passed, before // asking UserDefaults to covert it into a Bool, otherwise, // the dynamic value will never be used. #if DEBUG if userDefaults.value(forKey: key) != nil { override = userDefaults.bool(forKey: key) } #endif if override ?? featureFlags.searchEnabled { enableSearch() }}While the above code is specific to the Search feature, we could quite easily generalize it to be applicable to any feature flag, and call it in a shared code path - for example, when we load our values from the server.Setting stateOften when debugging or testing an app, we need to put it into a specific state, in order to either reproduce a bug or to be able to use a certain feature. This is something that is often repetitive (and a bit annoying) to have to do over and over again when working on something - so let’s automate it using launch arguments!First up, let’s add an easy way to completely reset our app. This could of course be done by manually uninstalling the app, but it’d be a lot easier to simply pass a -reset argument when running the app to have it launch in a blank state. To do that, let’s add a resetIfNeeded() method that we call from our AppDelegate when the app launches, like this:extension AppDelegate { func resetIfNeeded() { guard CommandLine.arguments.contains(\"-reset\") else { return } // We can reset our user defaults by removing the persistance // for our app's bundle identifier let defaultsName = Bundle.main.bundleIdentifier! userDefaults.removePersistentDomain(forName: defaultName) // Reset any caching mechanisms, databases, etc. cache.reset() database.reset() }}Similarly, it’s also really convenient to not only be able to reset the app, but also enable it to be put into a specific initial state.Let’s say we’re building an app that contains a list of user contacts. Using a launch argument, we can provide a quick way to pre-populate our contacts database with a given list of names (in this case we use a comma-separated list as our input format):extension ContactsManager { func addNamesFromCommandLine() { guard let argument = userDefaults.string(forKey: \"contacts\") else { return } let names = argument.components(separatedBy: \",\") names.forEach(add) }}UI testingBoth being able to completely reset our app’s state, and to pre-populated its data, is not only useful for debugging - it also enables us to easily setup a specific state when doing UI testing.Like we took a look at in “Getting stated with Xcode UI testing in Swift” and “UI testing analytics code in Swift”, we can add launch arguments to XCUIApplication that can then be read by our app, providing an (otherwise missing) channel of communication between our tests and our app. Here we use both the -reset and -contacts launch arguments to setup an initial state for a test that verifies that we can remove a contact from the contact list:func testRemovingContact() { // Setup and launch the app let app = XCUIApplication() app.launchArguments = [\"-reset\", \"-contacts\", \"John, Mary\"] app.launch() // Verify that we initially are displaying 2 contacts XCTestAssertEqual(app.tables.cells.count, 2) // Swipe the cell for a given contact and tap the \"Remove\" button let cell = app.tables.cells[\"Contact-John\"] cell.swipeLeft() cell.buttons[\"Remove\"].tap() // Verify that we no only have 1 contact left XCTAssertEqual(app.tables.cells.count, 1)}Using the above technique to set a specific initial state for our UI tests is a great way to combat flakiness and to speed up to the overall execution time of our test suite, since we don’t have to always perform a lot of setup in each test and can instead just jump to the feature we want to verify.ContainmentOne concern when doing all of the above, is how we’ve essentially now scattered a lot of debugging and testing code all over our normal app code.While adding code specifically for debugging is not necessarily a bad thing (our code base is kind of our “digital work place”, at all), it would be better if we could contain it all in one place instead of spreading it all over our app. That way we’d be more in control over what debug code we actually have, and it becomes much easier to prevent such code from accidentally making its way into a release build.One way to do is that is to move all code related to launch arguments into a dedicated type. As an example, here’s how we could move our actions for resetting, delaying network requests and adding mocked contacts into a single, contained LaunchArgumentsHandler:struct LaunchArgumentsHandler { let userDefaults: UserDefaults let contactsManager: ContactsManager let dataLoader: DataLoader let cache: Cache let database: Database func handle() { resetIfNeeded() addNetworkDelayIfNeeded() addContactsIfNeeded() } private func resetIfNeeded() { guard CommandLine.arguments.contains(\"-reset\") else { return } let defaultsName = Bundle.main.bundleIdentifier! userDefaults.removePersistentDomain(forName: defaultsName) cache.reset() database.reset() } private func addNetworkDelayIfNeeded() { let delay = userDefaults.double(forKey: \"network-delay\") guard delay &gt; 0 else { return } // We've abstracted the delaying of data loader tasks // into an \"executor\" closure, leaving our production // code free of any delaying code. dataLoader.taskExecutor = { task in DispatchQueue.main.asyncAfter(deadline: .now() + delay, execute: task.resume) } } private func addContactsIfNeeded() { guard let arguments = userDefaults.string(forKey: \"contacts\") else { return } let name = argument.components(separated: \",\") names.forEach(contactManager.add) }}We can now either surround the above LaunchArgumentsHandler declaration with the same #if DEBUG condition as before, or if we’re using separate Xcode targets for our debug/staging builds and release builds - we can simply exclude the LaunchArgumentsHandler.swift file from our production target.By doing that, we now both get a much cleaner overview over what launch argument actions that are available, and we’ll get a compilation error when doing a release build in case we’re accidentally using any debug code where it doesn’t belong.All we have to do now is to call LaunchArgumentsHandler after we’ve set up our app (for example in our AppDelegate) and surround that call with #if DEBUG to have it compile correctly under all conditions.ConclusionLaunch arguments can provide an easy way to set up really useful debugging and mocking actions that can help speed up our development and testing. By adding launch arguments we can quickly get into a state that we desire, or reset our app completely on each launch, removing the need to manually set these things up every time we run the app.While there’s definitely a risk associated with adding these type of debugging actions - since we are introducing more code paths and possible states to our app - keeping the amount of launch arguments low and containing all code dealing with them in a single place really helps mitigating that risks. Like with many things, it becomes a balancing act of risks vs reward, and picking some key launch arguments (and clean up old ones) can definitely make that balance tip in our favor.What do you think? Do you use launch arguments to speed up your development and testing, or is it something you’ll try out? Let me know.Thanks for reading!" }, { "title": "Struct convenience initializers", "url": "/posts/struct-convenience-init-swift/", "categories": "Swift", "tags": "swift5.0, models, api design", "date": "2018-04-26 19:07:00 -0400", "snippet": "I love to struct my code using extensions in Swift. One big benefit of doing so when it comes to struct initializers, is that defining a convenience initializer doesn’t remove the default one the c...", "content": "I love to struct my code using extensions in Swift. One big benefit of doing so when it comes to struct initializers, is that defining a convenience initializer doesn’t remove the default one the compiler generates - best of both world!struct Article { let date: Date var title: String var text: String var comments: [Comment]}extension Article { init(title: String, text: String) { self.init(date: Date(), title: title, text: text, comments: []) }}let articleA = Article(title: \"Best Cupcake Recipe\", text: \"...\")let articleB = Article( date: Date(), title: \"Best Cupcake Recipe\", text: \"...\", comments: [ Comment(user: currentUser, text: \"Yep, can confirm!\") ])" }, { "title": "Feature flags in Swift", "url": "/posts/feature-flags-in-swift/", "categories": "Swift", "tags": "architecture, feature flags, maintenance, swift5.0", "date": "2018-03-25 18:02:00 -0400", "snippet": "When developing new features for an app, it can be really useful to have some form of mechanism to gradually roll out new implementation &amp; functionality, instead of having to launch to every si...", "content": "When developing new features for an app, it can be really useful to have some form of mechanism to gradually roll out new implementation &amp; functionality, instead of having to launch to every single user at once. Not only can this help “de-risk” the launch of a big change (if something breaks, we can always roll back), it can also help us gather feedback on a partially finished feature, or perform experiments using techniques like A/B testing.Feature flags can act as such a mechanism. They essentially allow us to gate certain parts of our code base off under certain conditions, either at compile time or at runtime. Let’s take a look at a few different ways that feature flags can be used in Swift.Conditional compilationWhen working on a code base there are multiple strategies we can use when it comes to dealing with features that are a work in progress. We can -for example - use something like feature branches, and use version control to keep a feature that’s under development completely separated from our main master branch.Once the feature is ready to be released, we simply merge it in and ship it.However, there are some big advantages to instead continuously integrate new implementation and features into our main branch. It lets us detect bugs and problems earlier, it saves us the pain of having to solve a massive number of merge conflicts if the two branches have diverged a lot, and it can let us ship a pre-release version of a new feature internally or to beta testers.But we still need some way to remove code that shouldn’t be shipped live to the App Store. One way of doing so is to use compiler flags, which lets us mark a code block so that is only get compiled in if the flag has been set.Let’s say our app is currently using Core Data, and we want to keep it that way in production(for now), while still being able to try out a new solution - like Realm. To do that, we can use a DATABASE_REALM compiler flag, that we only add for builds that we want to use Realm in (for example for beta builds). We can then tell the compiler to check that flag when building our app, like this:class DataBaseFactory { func makeDatabase() -&gt; Database { #if DATABASE_REALM return RealmDatabase() #else return CoreDataDatabase() #endif }}To toggle the above flag on or off, we can simply open up our target’s build settings in Xcode and add or remove DATABASE_REALM under Swift Compiler - Custom Flags &gt; Active Compilation Conditions. This is especially useful for features that are still under active development, which lets the developers working on such a feature easily turn it on locally without affecting production builds.Static flagsConditional compilation is super useful when you want to completely remove the code for a new implementation from an app. But sometimes that’s either not needed or not practical and in those cases defining feature flags in code instead can be a much better option.struct FeatureFlags { static let searchEnabled = false static let maximumNumberOfFavorites = 10 static let allowLandscapeMode = true} As you can see above, flags can also be super useful in order to tweak an existing feature, not only to roll out brand new ones. Using the above maximumNumberOfFavorites property we can easily experiment with how many favorites a user can have, to find a value that we think will strike the right balance.With the above FeatureFlags type in place, we can now place checks in code paths where we’d activate a give feature. Here’s an example method that conditionally activates the search feature, which gets called from the viewDidLoad() method of a ListViewController:extension ListViewController { func addSearchIfNeeded() { // If the search feature shouldn't be enabled, we simply return guard FeatureFlags.searchEnabled else { return } let resultsVC = SearchResultsViewController() let searchVC = UISearchController(searchResultsController: resultsVC) searchVC.searchResultsUpdater = resultsVC navigationItem.searchController = searchVC }}The benefit of static flags is that they, just like compiler flags, are quite easy to setup and integrate. However, they don’t let us modify the value of our app has been compiled. To be able to do that, we need to start using runtime flags.Runtime flagsAdding the option to configure our app’s feature flags at runtime can be a bit of a “double edged sword”. On one hand, it can enable us to perform A/B testing by changing the value of a given flag for a certain percentage of our user base, and on the other hand it can make our app more difficult to maintain &amp; debug - since the code paths it’ll end up using are not fully determined at compile time.Runtime flags are often loaded from some form of backend system, and could potentially (depending on the app’s architecture) even be included in the response the app receives as part of logging a user in (otherwise it’s common to have a /feature_flags endpoint or similar that the app queries at launch). Optionally, we could also enable flags to be tweaked in the app itself using some form of debug UI.Regardless of how we load the values from our feature flags, we’ll want to update our FeatureFlags type to use instance properties instead of static ones. That way we can load the values for our flags and then transform them into a FeatureFlags instance, which we’ll then inject whenever needed. Our flags type now looks like this:struct FeatureFlags { let searchEnable: Bool let maximumNumberOfFavorites: Int let allowLandscapeMode: Bool}To be able to transform an instance from a serialized format, we’ll also add an initializer that takes a dictionary. That way we can either create our feature flags from a JSON backend response, or from values stored locally in the app (for example from a cache):extension FeatureFlags { init(dictionary: [String: Any]) { searchEnabled = dictionary.value(for: \"search\", default: false) maximumNumberOfFavorites = dictionary.value(for: \"favorites\", default: 10) allowLandscapeMode = dictionary.value(for: \"landscape\", default: true) }}private extension Dictionary where Key == String { func value&lt;V&gt;(for key: Key, default defaultExpression: @autoclosure () -&gt; V) -&gt; V { return (self[key] as? V) ?? defaultExpression() }} The reason we’re not using Codable above is that we want to use default values (in case our backend hasn’t been updated with a given flag), which is much easier done with a simple Dictionary extension.For more information about @autoclosure, which is used above, check more details.We can now load our feature flags whenever our app launches or a user logs in (depending on if we want our flags to be user-specific), and inject them whenever needed, like this:class FavoriteManager { private let featureFlags: FeatureFlags init(featureFlags: FeatureFlags) { self.featureFlags = featureFlags } func canUserAddMoreFavorites(_ user: User) -&gt; Bool { let maxCount = featureFlags.maximumNumberOfFavorites return user.favorites.count &lt; maxCount }}We now have a lot more freedom when it comes to togging certain features on and off, or tweaking values that determine part of our app’s logic. We could also enable our flags to be mutated while our app is running (and add some form of observation API to react to changes), but personally I rarely think adding that much complexity is worth it. Just loading the values once and setting up the app after that helps keep things simple and avoids introducing tricky corner cases.ConclusionUsing feature flags can be key when it comes to being able to quickly iterate on an app, especially as its team grows and the code base changes with a much higher velocity. By being able to conditionally enable certain features or tweak their behavior, we can usually integrate our code quicker into our main branch and still keep shipping our app.Feature flags can also add a bit of complication to our setup, especially when runtime flags are used. It can be come harder to reproduce bugs that only occur when a certain combination of flags are on, and testing all of our app’s potential code paths can quickly become much more complicated and time consuming.If you haven’t used feature flags before, I suggest to start simple (perhaps with compiler flags or static ones), and work your way from there. Like with most tools, they might require you to adopt your workflow a bit, especially if the project doesn’t currently use much automated testing (which makes using feature flags much less risky)." }, { "title": "Using @autoclosure when designing Swift APIs", "url": "/posts/using-autoclosure-swift/", "categories": "Swift", "tags": "language features, api design, Swift5.0", "date": "2017-05-28 18:02:00 -0400", "snippet": "Swift’s @autoclosure attribute enables you to define an argument that automatically gets wrapped in a closure. It’s primarily used to defer execution of a (potentially expensive) expression to when...", "content": "Swift’s @autoclosure attribute enables you to define an argument that automatically gets wrapped in a closure. It’s primarily used to defer execution of a (potentially expensive) expression to when it’s actually needed, rather than doing it directly when the argument is passed.Let’s explain this in more detail using the following code example. In this example, we’ve created a debugLog method and a Person struct which we’re going to print out:struct Person: CustomStringConvertible { let name: String var description: String { print(\"Asking for Person description\") return \"Person name is \\(name)\" }}let isDebuggingEnabled: Bool = falsefunc debugLog(_ message: String) { // You could replace this in projects with #if DEBUG if isDebuggingEnabled { print(\"[DEBUG] \\(message)\") }}let person = Person(name: \"Bernie\")debugLog(person.description)Even though we disabled debugging, the Person struct is still asked for its description. This is because the message argument of debugLog is directly computed.We can solve this by making use of a closure:let isDebuggingEnabled = falsefunc debugLog(_ message: () -&gt; String) { if isDebuggingEnabled { print(\"[DEBUG] \\(message())\") }}let person = Person(name: \"Bernie\")debugLog({ person.description })The message() closure is only called when debugging is enabled. You can see that we now need to pass in a closure argument to the debugLog method which doesn’t look so nice.We can improve this code by making use of the @autoclosure keyword:func debugLog(_ message: @autoclosure () -&gt; String) { ...}let person = Person(name: \"Bernie\")debug(person.description)The logic within the debugLog method stays the same and still has to work with a closure. However, on the implementation level, we can now pass on the argument as if it were a normal expression. It looks both clean and familiar while we did optimize our debug logging code.Another example of when this is used in the Swift standard library is the assert function. Since asserts are only triggered in debug builds, there’s no need to evaluate the expression that is being asserted in a release build.This is where @autoclosure comes in:func assert(_ expression: @autoclosure () -&gt; Bool, _ message: @autoclosure () -&gt; String) { guard isDebug else { return } // Inside assert we can refer to expression as a normal closure if !expression() { assertFailure(message()) }} I’m paraphrasing the implementation of assert a bit above, the actual implementation can be found here.The nice thing about @autoclosure is that it has no effect on the call site. If assert was implemented using “normal” closures you’d have to use it like this:assert({ someCondition() }, { \"Hey, it failed!\" })But now, you can just call it like you would any function that takes non-closure arguments:assert(someCondition(), \"Hey it failed!\")Let’s take a look at how we can use @autoclosure in our own code, and how it enables us to design some pretty nice APIs.Inlining assignmentsOne thing that @autoclosure enables is to inline expressions in a function call. This enables us to do things like passing assignment expressions as an argument. Let’s take a look at an example where this can be useful.On iOS, you normally define view animations using this API:UIView.animate(withDuration: 0.25) { view.frame.origin.y = 100}With @autoclosure, we could write an animate function that automatically creates an animation closure and executes it, like this:func animate(_ animation: @autoclosure @escaping () -&gt; Void, duration: TimeInterval = 0.25) { UIView.animate(withDuration: duration, animations: animation)}Now, we can simply perform our animation with a simple function call without any extra {} syntax:animate(view.frame.origin.y = 100)Using the above technique, we can really reduce the verbosity of our animation code, without sacrificing readability or expressiveness.Passing errors as expressionsAnother situation that I find @autoclosure very useful in is when writing utilities that deal with errors. For example, let’s say we want to add an extension on Optional that enables us to unwrap it using a throwing API. That way we can require the optional to be non-nil, or else throw an error, like this:extension Optional { func unwrapOrThrow(_ errorExpression: @autoclosure () -&gt; Error) throws -&gt; Wrapped { guard let value = self else { throw errorExpression() } return value }}Similar to how assert is implemented, we only evaluate the error expression when needed, rather than having to do it for every time we attempt to unwrap an optional. We can now use our unwrapOrThrow API like this:let name = try argument(at: 1).unwrapOrThrow(ArgumentError.missingName)Type inference using default valuesThe final use case for @autoclosure that I’ve found is when extracting an optional value from a dictionary, a database, or UserDefaults.Normally, when extracting a value from an untyped dictionary and providing a default value, you’d have to write something like this:let coins = (dictionary[\"numberOfCoins\"] as? Int) ?? 100That’s kind of hard to read, and has a lot of syntax cruft with the casting and the ?? operator. With @autoclosure, we can define an API that enables us to write the same expression like this instead:let coins = dictionary.value(forKey: \"numberOfCoins\", defaultValue: 100)Above, we can see that the default value is both used for when a value was missing, but also enables Swift to do type inference on the value, without us having to specify the type or perform casting. Pretty neat.extension Dictionary where Value == Any { func value&lt;T&gt;(forKey key: Key, defaultValue: @autoclosure () -&gt; T) -&gt; T { guard let value = self[key] as? T else { return defaultValue() } return value }} Again, we use @autoclosure to avoid having to evaluate the default value every time this method is called.ConclusionReducing verbosity is always something that needs to be done with careful consideration. Our goal should always be to write expressive, easy to read code, so we need to make sure that we don’t remove important information from the call site when designing low verbosity APIs.I think when used in appropriate situation, @autoclosure is a great tool for doing just that. Dealing with expressions, instead of just values, enables us to reduce verbosity and cruft, while also potentially gaining better performance.Thanks for reading!" }, { "title": "Picking up the right ways of failing in Swift", "url": "/posts/picking-error-handling-swift/", "categories": "Swift", "tags": "swift, error handling", "date": "2017-04-29 17:07:00 -0400", "snippet": "One major focus of Swift is compile time safety - enabling us as developers to easily focus on writing code that is more predictable and less prone to runtime errors. However, sometimes things do f...", "content": "One major focus of Swift is compile time safety - enabling us as developers to easily focus on writing code that is more predictable and less prone to runtime errors. However, sometimes things do fail for various reason.Let’s take a look at how we can handle such failures appropriately, and what tools we have at our disposal for doing so.Here are all ways that you can handle errors in Swift: Return nil or an error enum case. The simplest form of error handling is to simply return nil (or an .error case if you’re using a Result enum as your return type) from a function that encountered an error. While this can be really useful in many situations, over-using it for all error handling can quickly lead to APIs that are cumbersome to use, and also risks hiding faulty logic. Throwing an error (using throw MyError), which requires the caller to handle potential errors using the do, try, catch pattern. Alternatively, errors can be ignored using try? at the call site. Using assert() and assertionFailure() to verify that a certain condition is true. Per default, this causes a fatal error in debug builds, while being ignored in release builds. It’s therefore not guaranteed that execution will stop if an assert is triggered, so its’s kind of like a severe runtime warning. Using precondition() and preconditionFailure() instead of asserts. The key difference is that these are always evaluated, even in release builds. That means that you have a guarantee that execution will never continue if the condition isn’t met. Calling fatalError() which can probably seen in Xcode-generated implementations of init(coder:) when subclassing an NSCoding - confirming system class, such as UIViewController. Calling this directly kills your process. Calling exit() ,which exits your process with a code. This is very useful in command line tools and scripts, when you might want to exit out of the global scope (for example in main.swift)Unless you are compiling using the unchecked optimization mode.Recoverable vs non-recoverableThe key thing to consider when picking the right way of failing is to determine whether the error that occurred is recoverable or not.For example, let’s say that we’re calling our server and we receive an error response. That is something that’s bound to happen, no matter how awesome programmers we are and how solid our server infrastructure is. So treating these type of errors as fatal &amp; non-recoverable is usually a mistake. Instead, what we want is to recover and probably display some form of error screen to our users.So, how to pick an appropriate way of failing in this case? If we take a look at the list above, we can kind of split it up into recoverable and non-recoverable techniques, like this:Recoverable Returning nil or an error enum case Throwing an errorNon-recoverable Using assert() Using precondition() Calling fatalError() Calling exit()In this case, since we’re dealing with an asynchronous task, returning nil or an error enum case is probably the best choice, like this:class DataLoader { enum Result { case success(Data) case failure(Error?) } func loadData(from url: URL, completionHandler: @escaping (Result) -&gt; Void) { let task = urlSession.dataTask(with: url) { data, _, error in guard let data = data else { return completionHandler(.failure(error)) return } completionHandler(.success(data)) } task.resume() }}For synchronous APIs, throwing is a great option - as it “forces” our API users to handle the error in an appropriate way:class StringFormatter { enum Error: Swift.Error { case emptyString } func format(_ string: String) throws -&gt; String { guard !string.isEmpty else { throw Error.emptyString } return string.replacingOccurences(of: \"\\n\", with: \" \") }}However, sometimes an error is not recoverable. For example, let’s say that we need to load a configuration file during app launch. If that configuration file is missing, it’ll put our app in an undefined state - so in this case crashing is better than continuing program execution. For that, using one of the stronger, non-recoverable ways of failing is more appropriate.In this case, we use preconditionFailure() to stop execution in case the configuration file was missing:guard let config = FileLoader().loadFile(named: \"Config.json\") else { preconditionFailure(\"Failed to load config file\")}Programmer errors vs execution errorsAnother distinction that is important to make is whether an error was caused by faulty logic or incorrect configuration, or whether the error should be considered a legitimate part of the application’s flow. Basically whether the programmer caused the error or whether an external factor did.When protecting against programmer errors, you almost always want to use the non-recoverable techniques. That way, you don’t have to code around extraordinary circumstances all over your app, and a good suite of tests will make sure that those type of errors will get caught as early as possible.For example, let’s say we’re building a view that requires a view model to be bound to it before it’s used. The view model will be an optional in our code, but we don’t want to have to unwrap it every time we use it.However, we don’t necessarily want to crash the application in production if the view model somehow has gone missing - getting an error about it in debug is good enough. This is a case for using a asset:class DetailView: UIView { struct ViewModel { var title: String var subtitle: String var action: String } var viewModel: ViewModel? override func didMoveToSuperView() { super.didMoveToSuperview() guard let viewModel = viewModel else { assertionFailure(\"No view model assigned to DetailView\") return } titleLabel.text = viewModel.title subtitleLabel.text = viewModel.subtitle actionButton.setTitle(viewModel.action, for: .normal) }}Note that we have to return in our guard statement above, since assertionFailure() will silently fail in release builds.ConclusionI hope this post helped to clear up the difference between various types of error handling techniques available in Swift. My advice is to not only stick to one technique, but to pick the most appropriate one depending on the situation. In general, I would also suggest to always try to recover from errors if at all possible, as to not disrupt the user experience unless the error should be treated as fatal.Also, remember that print(error) is not error handling :)Thanks for reading!" }, { "title": "Four Metrics Every Mobile Developer should care about", "url": "/posts/mobile-metrics-vital/", "categories": "Metrics", "tags": "mobile, metrics", "date": "2017-03-08 17:02:00 -0500", "snippet": "Slow apps frustrate users, which leads to bad reviews, or customers that swipe left to competition.Unfortunately, seeing and solving performance issues can be a struggle and time-consuming.Most dev...", "content": "Slow apps frustrate users, which leads to bad reviews, or customers that swipe left to competition.Unfortunately, seeing and solving performance issues can be a struggle and time-consuming.Most developers use profilers within IDEs like Android Studio or Xcode to hunt for bottlenecks and automated performance tests to catch performance regressions in their code during development. However, testing an app before it ships is not enough.To chat che most frustrating performance issues, you need to explore what’s happening on your users’s devices. That means visibility into how fast your app starts, duration of HTTP requests, number of slow and frozen frames, how fast your views are loading, and more. With Sentry for Mobile, you can now easily monitor your ReactNative, iOS, and Android app’s performance in real-time without additional setup (or accumulating a pile of testing devices).Mobile VitalsWe believe there are four metrics every mobile team should track to better understand how their app is performing: Cold starts, warm starts, slow frames, and frozen frames. These four metrics, as a core part of Sentry’s performance monitoring, give you the details you need to not only priority critical performance issues but trace the issue down to the root cause to solve them faster.Measuring App StartWhen a user taps on your app icon, it should start fast. On iOS, Apple recommends your app take at most 400ms to render the first frame. On Android, the Google Play console warns you when a cold start takes longer than 5 seconds or a warm start longer than 2 seconds. Cold start: App launched from the first time, after a reboot or update. Warm start: App launched at least once and is partially in memory.The exact definitions differ slightly per platform.No matter the platform, it is critical that your app starts quickly to provide a delightful user experience. That’s why on iOS, Mac Catalyst, tvOS, and Android we track how long your app needs to draw your first frame. We grab this information and add spans for different phases of the app start.On Android, it is trickier to hook into the initialization phases of the app start, and therefore we currently add one span from the application launch to the first auto-generated UI transaction. Still this information is very useful and can help you to improve the duration of your app start.Slow and Frozen FramesUnresponsive UI, animation hitches or just jank annoy users and degrade the user experience. Two measurements to track this unwanted experience are slow and frozen frames. A phone or tablet typically renders with 60 frames per second (fps).The frame rate can also be higher, especially as 120 fps displays are becoming more popular. With 60 fps, every frame has 16.67 ms to render. If your app needs more than 16.67 ms for a frame, it is a slow frame.Frozen frames are UI frames that take longer than 700 ms. An app that is running smoothly should not experience either. That’s why the SDK adds these two measurements to all transactions captured. The detail view of the transaction displays the slow, frozen, and total frames to the right.Mobile vitals are a core part of Sentry’s performance monitoring for mobile and unlocks more ways to spot bottlenecks and speed up your apps." }, { "title": "Retaining Computer Science Knowledge", "url": "/posts/retaining-computer-science-knowledge/", "categories": "Computer Science", "tags": "studying, review, retention", "date": "2016-11-13 15:02:00 -0500", "snippet": "Credit: John Washam’s blogI’ve been asked numerous times, “How do you remember all the stuff you’ve been studying?”Here is my method that will keep all the good stuff in your brain.My Mistake in t...", "content": "Credit: John Washam’s blogI’ve been asked numerous times, “How do you remember all the stuff you’ve been studying?”Here is my method that will keep all the good stuff in your brain.My Mistake in the BeginningWhen I first started studying, I was watching videos all day long, taking tons of notes, and trying to remember. I also spent time implementing data structures and their associated algorithms. I would spend 10 hours watching videos on hash tables, for instance.Over time, I realized I was forgetting things. I wasn’t reviewing my notes and felt lke all the hours I invested in watching videos was mostly wasted.Repetition Over TimeWhat I should have done for each topic: watch a couple of hours of videos on the topic, to get the main idea implement the algorithm and data structure make flash cars for important things I should remember about the topic move on to the next topic review the flash cards when you have time after a few days, watch another a video about the topic, maybe same video or shorter one keep reviewing flash cardsThe thing to notice here is repetition. Spending one or two days watching videos on a topic just reinforces what you recently learned in the last hours. This is not strong knowledge reinforcement. You’ll forget.I go back through all my notes and put them into flash cards. Many, many (too many) flash cards.You have to leave a time interval between learning something for the first time, and reviewing it later.The important point is: review a topic over several time intervals. It will continue to reinforce and remind you, and you won’t forget it. The time interval can get longer over time. This is called spaced repetition.Review KnowledgeI use fash cards to review topics and trivia. Not paper flash cards.I originally made it to make flash cards for both information and also for algorithms and data structures, But now I only use it for CS knowledge.For code, I use something a bit more primitive: paper.Review Algorithms and Data StructuresI gather together all these algorithms and data structures, and simply printed them out (so each one fit on a paper, with maybe a couple of exceptions). Then, at my leisure, I can look over some code.It’s probably not necessary to memorize them, just know the concepts and how they work in case you ned to recreate them in an interview.I’ll be taking some time to test myself on these. For example, write out Dijkstra’s algorithm. I’ll write it on the board, without looking, and then look at the paper to see how I did. Do you need to memorize Dijkstra? Probably not, unless you have a Master’s degree and many years of experience. Then for you, it might be expected in an interview." }, { "title": "Mastering the Art of Storytelling for Vlogs and Videos", "url": "/posts/mastering-art-storytelling/", "categories": "Learning, Communication", "tags": "communication, storytelling, vlog", "date": "2014-04-04 02:51:02 -0400", "snippet": "In the digital age, storytelling has become an essential skill for anyone looking to create engaging content, especially for vlogs and videos. Whether you’re a seasoned creator or just starting, ho...", "content": "In the digital age, storytelling has become an essential skill for anyone looking to create engaging content, especially for vlogs and videos. Whether you’re a seasoned creator or just starting, honing your storytelling abilities can significantly enhance your audience’s connection with your content. Here are key strategies to help you become a better storyteller in your videos.1. Tell More StoriesThe foundation of effective storytelling lies in practice. Start by incorporating storytelling into your daily conversations. Instead of merely stating facts or sharing information, aim to add context, emotions, and engaging details to your experiences. This transformation turns mundane moments into compelling narratives that resonate with your audience. Remember, the more you practice, the more natural storytelling will become for you.2. Structure Your StoriesA well-structured story is crucial for keeping your audience engaged. Here’s a simple framework to follow: Setup: Begin by providing background information and setting the scene. This helps your audience understand the context of your story. Challenge: Define a clear goal and outline the obstacles that stand in the way. This creates tension and gives viewers a reason to invest in your story. Complications: Highlight the challenges and setbacks encountered along the way. This adds depth to your narrative and keeps the audience on the edge of their seats. Payoff: Reveal whether or not the goal was achieved. This is the moment of truth that brings resolution to your story. Change: Reflect on how the experience impacted or changed you. This adds a personal touch and encourages viewers to relate to your journey. By following this structure, you can create stories that not only captivate but also inspire your audience.3. Consume More StoriesTo become a better storyteller, immerse yourself in various forms of storytelling. Watch films, read books, and explore different media. Pay close attention to the structure, beats, and techniques used by successful storytellers. Analyze what keeps you engaged and think about how you can incorporate similar elements into your own content.4. Write Down Story StructureAs you consume stories, take the time to break them down using the five structural elements mentioned earlier. Writing down these elements will help you understand how successful stories are constructed and how to apply these principles to your own content. This practice will not only sharpen your analytical skills but also enhance your creativity when crafting your narratives.5. Visual StorytellingIn the world of vlogs and videos, visuals play a crucial role in storytelling. Remember the adage: “Show, don’t just tell.” Use a variety of visual techniques to communicate your story effectively. Incorporate different camera angles, wide shots, close-ups, and medium shots to enhance the narrative and make it more engaging. The right visuals can evoke emotions and create a more immersive experience for your audience.ConclusionThe art of storytelling is a powerful tool for connecting with your audience and making your content memorable. By actively practicing storytelling, employing a clear structure, and being intentional with your presentation—both through words and visuals—you can elevate your vlogs and videos to new heights. So, embrace these strategies, and watch as your storytelling skills flourish, captivating your viewers and leaving a lasting impact. Happy storytelling!" }, { "title": "How to stack multiple git branches and rebase them", "url": "/posts/how-stack-multiple-branches-rebase-them/", "categories": "Git", "tags": "git, rebase, gitflow", "date": "2014-03-27 18:02:00 -0400", "snippet": "It took me quite some time to start feeling comfortable with git, and although I still know I have a lot to learn I finally feel confident not to screw up.I see some people constantly struggle with...", "content": "It took me quite some time to start feeling comfortable with git, and although I still know I have a lot to learn I finally feel confident not to screw up.I see some people constantly struggle with: How to rebase your branch on top of master in the most efficient way. How to stack up branches on top of each other and keep them all updated using rebase.Note: All the tips are assuming you use a rebase workflow and not a merge workflow. I recommend the use of the rebase workflow, however, that’s a discussion for another post :)1. How to rebase your branch on top of master in the most efficient way.a lot of people doing: git checkout master git pull git checkout branch_to_rebase git rebase masterYou can optimize this flow by running the rebase directly against your remote master branch: git fetch origin - it will retrieve the latest meta-data info from your remote, but unlike git pull it will not transfer any file. git rebase origin/master2. How to stack up branches on top of each other and keep them all updated using rebaseIf you have only one branch, the previous way is enough to make sure your branch is always updated with the latest changes in master.The problem starts when you have multiple branches depending on each other.Let’s imagine the following situation: You create a new branch to start working on feature A. You finish all the work and open a Pull Request for your team to review. Since you don’t want to be blocked, you start working on feature B that depends on feature A, so you create a new branch-B on top of branch-A. In the meanwhile, you rebase and update branch-A to fix all the PR comments. You go back to branch-B and you want to update it with the latest changes in branch-A. You try “git rebase branch-A” and git complains about a lot of nonsense conflicts.So, what happened there?Basically, when you updated branch-A you might have changed some of the commit hashes from that branch, but branch-B isn’t aware of them yet. Thus, when you rebase the branch-B on top pf branch-A, git will think the old commits from branch-A are new and part of branch-B, so it might create conflicts with themself. I like to call them: “repeated commits”.This is the number 1 criticized aspect of the superior rebase workflow - the fact that it messes with git history and some tools, including git itself, get confused by that. But honestly, if you understand how git works, that will never be a problem, quite the opposite, it will make your git history as clear as never.There is more than one way to fix this, but there is a neat one-line way of solving this: From branch B run “git rebase -i branch-A” this will make the rebase interactive and editable. Drop all the old/repeated commits from branch-A present on branch-B. You have to make sure only commits from branch-B will be rebased.That’s it! No more nonsense conflicts. You might still have conflicts, but those you really need to fix :)Thank you for reading!" }, { "title": "An overview of dSYM", "url": "/posts/overview-dsym/", "categories": "Apple", "tags": "dsym, ios, debugging", "date": "2014-03-04 16:02:00 -0500", "snippet": "From Apple’s Documentation:https://developer.apple.com/documentation/xcode/building-your-app-to-include-debugging-information When Xcode compiles our source code into machine code, it generates a ...", "content": "From Apple’s Documentation:https://developer.apple.com/documentation/xcode/building-your-app-to-include-debugging-information When Xcode compiles our source code into machine code, it generates a list of symbols in our app - class names, global variables, and methods and function names. These symbols correspond to the file and line numbers where they’re defined; this association creates a debug symbol, so you can use the debugger in Xcode, or refer to line numbers reported by a crash report. Debug builds of an app place the debug symbols inside the compiled binary file by default, while release builds of an app place the debug symbols in a companion Debug Symbol file (dSYM) to reduce the size of the distributed app.Let us try to understand the above complex statement in a detailed and simple manner.When an app crashes, the operating system collects diagnostic information about what the app was doing at the time of crash. One of the most important parts of the crash report are the thread backtraces, which are reported as hexadecimal addresses.What is SYMBOLICATIONNow to understand the root cause of our app crashes, firs we need to translate Thread backtraces into human readable language (i.e. function names and line number in our source code that caused the crash). This process is called symbolication.Role of dSYM (Debug SYMbol)Without dSYM the crash report will just show memory addresses of objects and methods. We won’t be able to read any crash reports without first re-symbolicating binary files.dSYM files store the debug symbols for our app. It contains mapping information to decode a stack-trace into readable format.The purpose of dSYM is to replace symbols in the crash logs with the specific methods names so it will be readable and helpful for debugging the crash.Key points regarding dSYMThe dSYM file is created when the application source code is compiled with the Debugging Information Format set to DWARF with dSYM filedSYM fils change every time we compile our app having code changes.The dSYM file contains UUID that links it to a specific Xcode build. The consequence is that symbolication only works if the UUID of the binary that caused a crash matches the UUID of the dSYM that is used for symbolication.The benefit of using the dSYM is that we don’t need to ship our App with its symbols which makes it difficult to reverse engineer our app. This also reduce our App binary size.How to enable dSYM? In Xcode, select your project in the Project Navigator. In the target list, select the target that builds your application. Select the Build Settings tab. In the Build Options section, make sure that the Debugging Information Format is set to DWARF with dSYM File.Note: It is recommended to set Debug to “DWARF” instead of “DWARF with dSYM File” to reduce the compile time." }, { "title": "How to Crop Video Frames Using FFmpeg", "url": "/posts/remove-video-height-ffmpeg/", "categories": "FFmpeg", "tags": "ffmpeg, command", "date": "2014-03-01 14:19:08 -0500", "snippet": "Why Crop Video Frames?Cropping video frames can be essential for various reasons, including: Removing Unwanted Content: Sometimes, videos may contain unnecessary elements at the top or bottom that...", "content": "Why Crop Video Frames?Cropping video frames can be essential for various reasons, including: Removing Unwanted Content: Sometimes, videos may contain unnecessary elements at the top or bottom that distract from the main subject. Focusing on Key Elements: Cropping helps to emphasize certain parts of the video, drawing viewers’ attention where it matters most. Adjusting Aspect Ratios: Cropping can help fit videos into specific aspect ratios required for different platforms.The FFmpeg Command to Crop Video FramesTo remove a certain height from video frames using FFmpeg and re-write the output as an MP4 file, you can use the following command:ffmpeg -i input.mp4 -vf \"crop=iw:ih-100:0:100\" -c:v libx264 -crf 23 -c:a copy output.mp4Breaking Down the CommandLet’s take a closer look at the components of this command: -i input.mp4: This specifies the input video file you want to edit. Replace input.mp4 with the name of your actual video file. -vf \"crop=iw:ih-100:0:100\": This applies the crop filter to the video. Here’s what each parameter means: iw: Refers to the input video width, keeping the original width intact. ih-100: This indicates the new height of the video, which is the original height minus 100 pixels. This effectively removes 100 pixels from the bottom of the video. 0:100: This specifies the starting point for the crop. In this case, it starts from the top-left corner, keeping the top 100 pixels. -c:v libx264: This sets the video codec to libx264, which is widely used for H.264 encoding, ensuring good quality and compatibility. -crf 23: The Constant Rate Factor (CRF) value determines the quality of the output video. Lower values yield better quality, with a range from 0 to 51 (the default is 23). -c:a copy: This copies the audio stream without re-encoding, preserving the original audio quality. output.mp4: Finally, this specifies the name of the output file. You can change output.mp4 to your desired file name.Adjusting the Height to RemoveIf you want to remove a different height, simply adjust the value in ih-100. For instance, to remove 200 pixels from the bottom, you would modify the command to:ffmpeg -i input.mp4 -vf \"crop=iw:ih-200:0:200\" -c:v libx264 -crf 23 -c:a copy output.mp4Important Considerations Backup Your Work: Before performing any edits, ensure your original video file is backed up. This way, you can always revert if needed. Quality Loss: While re-encoding the video may result in slight quality loss, it is necessary when applying filters like crop. If you want to avoid re-encoding, you can use -c:v copy instead of -c:v libx264 -crf 23. However, this will only work if the input video is already in a compatible format. ConclusionFFmpeg is an incredibly powerful tool for video editing, and cropping video frames is just one of its many capabilities. By following the steps outlined in this post, you can easily remove unwanted height from your video frames and save the result as an MP4 file. Whether you’re a content creator, a video editor, or just someone looking to tidy up your videos, mastering FFmpeg can significantly enhance your editing workflow. Happy editing!" }, { "title": "Git checkout a remote branch", "url": "/posts/git-check-remote-branch/", "categories": "Git", "tags": "git, checkout, remote branch", "date": "2014-02-20 17:02:00 -0500", "snippet": "One of the first Git commands you’ve learned ws certainly “git checkout”:$ git checkout developmentIn this simplest form, it allows you to switch(and even create) local branches -something you need...", "content": "One of the first Git commands you’ve learned ws certainly “git checkout”:$ git checkout developmentIn this simplest form, it allows you to switch(and even create) local branches -something you need countless times in your day-to-day work.However, git checkout’s power is not limited to local branches: it can also be used to create a new branch from a remote one.Collaborating with BranchesRemember that branches are the main way of collaboration in Git. Let’s say that one of your colleagues wants you to collaborate on(or review) a piece of code: She will push the corresponding branch to your common remote server. In order to see this newly published branch, you will have to perform a simple “git fetch” for the remote. Using the “git checkout” command, you can then create a local version of this branch - and start collaborating. Checking out for remote branchesThe syntax for making git checkout “remote-ready” is rather easy: simply add the “–track” flag.$ git checkout --track origin/branch-aBased on the remote branch “origin/branch-a”, we now have a local branch named “branch-a”.Note that, by default, Git uses the same name for the local branch. Being a good convention, there’’s rarely the need to change this." }, { "title": "Undo a git rebase.", "url": "/posts/undo-git-rebase/", "categories": "Git", "tags": "git, rebase", "date": "2014-02-18 17:02:00 -0500", "snippet": "The easiest way would be to find the head commit of the branch as it was immediately before the rebase started using reflog.$ git reflogThen reset the current branch to the old commit. Suppose the ...", "content": "The easiest way would be to find the head commit of the branch as it was immediately before the rebase started using reflog.$ git reflogThen reset the current branch to the old commit. Suppose the commit was HEAD@{10} in the ref log.$ git reset --hard HEAD@{10}In windows, you may need to quote the reference:$ git reset --hard \"HEAD@{10}\"" }, { "title": "Tone and Intonation", "url": "/posts/tone-intonation-liguistics/", "categories": "Linguistics", "tags": "english, linguistics, tone, intonation", "date": "2014-02-06 00:43:35 -0500", "snippet": "The podcast transcript explores key principles of English intonation and how tone and pitch can alter the meaning of words, even if the words themselves remain the same. Here are the key takeaways:...", "content": "The podcast transcript explores key principles of English intonation and how tone and pitch can alter the meaning of words, even if the words themselves remain the same. Here are the key takeaways: Intonation changes meaning: The same word or phrase can have different meanings depending on the pitch or melody used. For example, “ice cream” can be a question, an excited statement, or a neutral comment based on the intonation applied. Pitch contours: Words and phrases can be likened to melodies with high and low pitches. Even though individual voices vary in their natural pitch range, the pitch contour (rising or falling) determines the intonation. For instance, questions in English often rise towards the end, while statements usually fall. Cultural and linguistic differences: Some languages use intonation to change meanings at the sentence level (intonation languages), while others, like Mandarin, use tone at the word level (tone languages). Mandarin uses specific tones to differentiate meanings (e.g., the word “ma” can mean mother, hemp, horse, or scold depending on the tone). Pragmatic effects of intonation: Intonation can also express emotions like enthusiasm, sarcasm, or uncertainty. For example, “great!” can indicate genuine enthusiasm, while “great…” with a flat tone can convey sarcasm. Learning through context: Intonation is often learned through exposure and practice rather than explicit instruction, making it crucial in both acquiring a language and mastering its subtleties. In summary, understanding English intonation involves recognizing how shifts in pitch and tone can drastically alter meaning, emotion, and intent in communication.Transcript(128) 79: Tone and Intonation? Tone and Intonation! - YouTubehttps://www.youtube.com/watch?v=GDjdl9XEKAsTranscript:(00:01) [Music] L: Welcome to Lingthusiasm,   a podcast that’s enthusiastic about  linguistics! I’m Lauren Gawne.   G: I’m Gretchen McCulloch. Today, we’re  getting enthusiastic about the melodies   of words. But first, our most recent  bonus episode was a recording of our   liveshow with Dr. Kirby Conrod about language  and gender that we held as part of LingFest.(00:37) L: Thanks to all the patrons who attended,  asked excellent questions, and also helped   support us by keeping the show ad-free. G: To get access to this bonus episode and many,   many other bonus episodes to listen  to go to patreon.com/lingthusiasm.   [Music] L:   Hey. G: Hey.   L: Hey? G: Hey!   L: Hey! G:   So, here’s one word, “hey,” and it’s  got a bunch of different vibes depending   on what pitch contour we’re using with it.(01:16) L: We can use those pitch contours with a whole   bunch of different words to give them a different  spin. If we have a word like, “ice cream.”   G: “Ice cream.” L: Oh, very serious. Uh, “Ice cream?”   G: That’s a bit of a question. Ice cream…? L: Ice cream and what?   G: Ice cream and ice cream! L: Perfect choice. “Ice cream!”   G: Very excited ice cream.(01:39) L: We’ve said the word “ice cream”   with a whole bunch of different intonation  that’s given it different meaning. That’s   because we’re making use of the way that we can  change the melody of words that we’re saying.   G: When we talk about the different kinds of  pitches that words can have that change the   meanings they have, I think it’s probably useful  to clarify what we mean by “changing the pitches   of the words” in this particular context.(02:07) It’s  more like playing a word on a different kind of   melody, which might be a very simple melody – it  might just be one or two notes – and that melody   is relative to the highness and lowness, the pitch  of the words that came before it. But it’s not   an absolute melody because that’s just sort of  the range my voice lives in most of the time.   L: And different voices live in different ranges  just like if we visit the woodwind section of a   bunch of instruments, we’ve got small instruments  like a piccolo or a big instrument like an oboe   or a bassoon. They can all play exactly the same  tune; they just play them at a different pitch.(02:41) G: If we’re thinking about something that’s  making a pitch intonation – say something   like question intonation, which is one of  the easiest ones to think about because   it’s got that nice question mark for us to grab  onto – different people saying something with   question intonation is sort of like playing  the same song – you know, “Twinkle, Twinkle   Little Star” or something – on different kinds  of instruments.(02:59) It’s all making that same melody   of going down a bit and then up at the end. L: There’s a lot of different meaning that we   associate socially with different pitches –  so whether someone has a high voice or a low   voice. We played around a lot with this in our  episode on vocal folds and how we have different   associations with different pitches for different  genders.(03:22) In our interview with Nicole Holliday,   she talked about how African American English  has different intonation to Standard American   English and what that says about identity, but  today we’re gonna look at more of the ways that   we can use pitch and melody to change sentences  or words in the way that they have meaning.   G: Right.(03:44) Let’s start with the version of  different pitch melodies that is the most   accessible to English speakers, and that’s the  one that operates on a whole phrase and changes   that meaning in relatively predictable ways  no matter what sort of phrase it applies to.   We have our example from earlier, “Ice cream?  Ice cream. Ice cream! Ice cream. Ice cream.”   L: And in all of those cases,  no matter how you say it,   it still refers to the creamy, frozen desert. G: Right.(04:11) But when we add something like question   intonation or if we add list intonation or  exclamation mark intonation, those change   the ways in which it’s interpreted in this very  predictable way. If we add question intonation to   lots of other words, they all sound question-y.  You can have: “Ice cream? Cake? Pizza? Barbecue?   Umbrella? Clarinet?” L: Oh, okay, that’s not   a “What’s for dinner?” list. G: “Om nom nom, clarinet.(04:36) ”   L: They all end up being questions. G: Right. And you can do this with   longer phrases and sentences to – especially,  there’s subtle differences in different kinds   of questions. L: Okay.   G: I’m gonna say one sentence with two different  versions of question intonation, and I want you to   tell me what you think the meaning difference is. L: Okay.(04:56) G: Number One, “Can you bring cake  or ice cream?”, and Number Two,   “Can you bring CAKE or ice cream?” L: Okay, so the first one, I feel like   it’s much more open to like, you just want  some kind of desert situation. I might turn   up with a trifle, and it’s probably okay.(05:13) G: “Can you bring cake or ice cream?” – yeah,   that’s sort of like a yes-no  question, “Can you do this?”   L: Some kind of desert. G: As a general category.   L: Whereas with the other one, I really feel  like my options to bring are cake or ice cream,   and I have to choose one or the other. G: Right, exactly. In that case, I’m asking   a question about these two alternatives  and getting you to pick one, and actually,   if you were to bring both, maybe that’d be  kind of weird because I’m actually gonna get   someone else to bring the other one. L: Yeah, probably gonna hedge my bets(05:36) and bring an ice cream cake though. G: Ice cream cake is always acceptable.   L: Exactly where you go up in the phrase can  really change the effect that the intonation   has on the sentence. Questions rise up towards the  end, but that’s very different to another type of   rising up which is a phrase specifically known as  a “high-rising terminal” but which you may know as   “uptalk” where that also goes up towards the end,  but the point in the sentence at which it goes   up is a little bit different. So, you can tell  the difference between a question and uptalk.(06:08) G: I think this is particularly interesting  because when it comes to writing people often   use a question mark to indicate both types of  intonation. So, if you’re saying something like,   “Ice cream?” But I think most people can actually  tell the difference. Can I say them both to you   and see which one you think is which? L: Yeah.(06:24) G: Here’s Number One, “There’s some ice cream?”  and Number Two, “There’s some ice cream?”   L: That first one goes up and stays up earlier  and stronger, which sounds much more like uptalk   than a question to me. We use that to indicate  that someone wants to continue saying something.   G: Then in the second one, that’s more of  a question which actually goes down first   and then up towards the end.(06:51) That’s “There’s  some ice cream?” and “There’s some ice cream?”   where I’m deliberately going “ice cream”  – just going straight up over going down   versus up. There’s this difference here, even  though we’re not very precise about writing   these sorts of intonational contours in English.  People tend to use a question mark for both, and   it’s obvious from context. But it’s fascinating  to me that we can actually hear the difference.(07:08) L: When it comes to analysing the difference,  sometimes linguists will literally draw a   little up and down pitch contour over the top of  a sentence to show that the question one does have   that downward before upward movement. G: I love these. I feel like   they’re very old school. L: It’s quite old school.(07:26) You know,   they are somewhat subjective, but they do show  you the difference between the two patterns.   G: I love this style. I think it’s really quite  easy to read. You often see them in typewritten   manuscripts because it’s a little bit hard to do  digitally, but it’s sort of easy to just draw with   a pen. I find it quite easy and intuitive  to read.(07:43) Unfortunately, it’s a little bit   harder to do things like technical comparison with  because you’re drawing this very analogue curve,   and then you’re looking at another sentence  and being like, “Okay, is that the exact same   shape that this person drew? Or did this  little dip – was that just like their hand   got jogged or did they mean something by it?” L: Other systems involve using notation, like   you might use “H” for the bit that’s high and “L”  for the bit that’s low.(08:03) I’ve seen other notation   systems that use arrows as well to indicate those  upward and downward movements in the melody.   G: Yeah, the H and L one I feel like is relatively  intuitive, although when you start combining it,   it can get quite complicated. I’ve also seen  people use numbering systems where you number   pitches from one to four.(08:23) The problem with this  for me is that some people prefer a version where   “one” is low and “four” is high, and some systems  do the exact opposite thing, so when I see pitch   numbers, I never quite know what’s going on. L: Always worth checking what their transcription   system is before getting into things is a thing  I’ve learnt when it comes to number systems.   G: Absolutely.(08:41) I think that pitch systems  are something where they’ve been one of the   hardest things for me to learn at a technical  level because when it comes to something like,   “Okay, here’s some sounds. We’re gonna produce  them. We’re gonna transcribe them. We’re gonna   write down a bunch of symbols for them,” that’s  something that I was able to learn in a relatively   concrete way.(08:59) But pitch is this thing that’s  overlaid on top of the individual sounds and   applies to the whole syllable or to the whole  word or the whole sentence and has taken me   quite a while to be able to train my ear to hear  rather than just perceiving the sentence as like,   “This is a question,” or “This is angry,” or  “This is curious,” or something like that.   L: I think it takes practice to step away  because it is something that is often used   for that kind of emotional and stylistic effect,  so it can be harder to step back and think about   what’s actually being done with intonation versus  other things that we use strategically to create(09:29) emotion in the way that we speak. G: I feel like I’m better at it now than   I used to be. I’m still not as good as somebody  who does this full time, but it is something   you can improve at with practice, for sure. L: Absolutely. I think the more you realise   just how much it is dependent on the specific  language, it can help you think a little bit   about what’s happening with intonation.(09:53) A  thing like having rising intonation at the   end of a question where it goes up is not  something that happens in all languages.   G: I mean, I was calling this “question”  intonation, but does every language ask   questions by doing this low and then high thing? L: A lot of languages do, but that doesn’t mean   that it’s all languages do it.(10:10) Hawaiian is a language that has   falling question intonation, as an example. G: This is the Indigenous language of Hawai’i?   L: Yeah. And what’s really interesting is that  the Hawaiian creole that has arisen because of the   contact between Hawaiian and English has actually  continued to use that falling question intonation   instead of English rising question intonation. G: Oh, that’s really neat.(10:29) That’s something that’s   gotten passed on in the creole as well. L: Yeah.   G: Question intonation is easy to talk about, but  there’re also other things that pitch is doing.   I think one of my favourites is using pitch  to indicate things like attitude. A word like   “great” – you could say something like, “Great.” L: Okay.(10:46) G: “Great!” L: Oh, much better.   G: “Great…” L: Oh,   no need to be sarcastic. G: So, that’s “Great. Great!   Great…” It’s sort of starting medium and dropping  to low – “Great.” Enthusiasm with the pitch   starting very high and ending low – “Great!” Or  sarcasm which starts and ends low – “Great…”   L: It just stays low.(11:12) G: I’m picturing a teenager   very sulkily in the corner – “Great…” L: Same word. The intonation gives it very   different meanings. G: Absolutely.   L: And a lot of those meanings are conveyed  by the English writing system in traditional   writing systems, and it’s part of what I love  about how you analyse how people are playing   with new internet grammar and using all kinds of  different techniques with the writing system to   try and capture some of that spoken vibe.(11:37) G: This is something that I talked about a   lot in Because Internet, but there’s also  a Tumblr post that I think very succinctly   summarises it in which the first poster  says, “Part of the New Internet Grammar:   using question marks not to denote questions, but  upturns in voice, so that a tentative statement   gets a question mark but a flatly delivered  question doesn’t.(11:54) ” And then someone comes along,   and I think very tongue-in-cheek  says, all lowercase, no punctuation –   L: “why would you do this”. G: The first person again, “It just seems   right?” – question mark. I think we’re evolving  more subtle ways of indicating intonation like   this, including things like deadpan questions  or tentative statements, but it’s something   that’s kind of a work-in-progress in English,  which is a nifty thing to keep observing.(12:18) L: You can also use intonation for emphasis. So,  where you chose to create a rise in the sentence   can indicate that something is prominent. G: Yeah. If you’re looking in the freezer or   something, and you’re making a list of what’s in  there, you might end up with “ice CREAM” and “ice   CUBES” even though normally you would say them  as “ice cream” and “ice cubes” because they’ve   both got “ice” in it, you wanna stress the  other part – the “cream” and the “cubes” – to   differentiate between them a little bit more.  But intonation isn’t the only way that languages(12:47) can emphasise different parts of a sentence. I  feel like I had to learn how to do this a bit   differently when I was getting more comfortable  speaking French because, in English, we have   this strong tendency to use this pitch part and  also loudness and things like that to emphasise   certain words.(13:06) If you’re in an ice cream place,  and it’s kind of loud, you might emphasise like,   “Can I get TWO SCOOPS of the CHOCOLATE ice cream  in a CONE, please?” to make each of those parts   more distinct. But I feel like French is a bit  more likely to use word order in terms of which   part you say first rather than saying particular  parts in a more emphatic way. That hasn’t been as   effective for me when I’m speaking French. L: Interesting.(13:28) And it’s a good reminder   that when you’re learning a language, you  often don’t overtly get taught how to use   intonation. It’s something that you pick up from  listening experiments and talking to people and   listening to people and trying to imitate them. G: Absolutely. Sometimes, it’s easy to imitate   in the sense that when people are doing mock  versions of an accent, the intonation contours,   the characteristic intonation contours, are some  of the things that come really early.(13:53) But I feel   like it’s also worth noting that sometimes  what’s a characteristic intonation contour –   just sort of a default one in one language –  might be something that carries an emotional   meaning in another language. I guess you wanna be  cautious when you’re reading someone’s intonation   as hostile or as overly friendly that this  might be a relatively baseline thing for them,   and it’s not that people are secretly hating you.(14:20) L: If someone’s language doesn’t have a rise at   the end of a question, it might come across as a  hostile question, but it’s actually just the way   they’re used to asking questions. G: Yeah, it’s something that’s   worth keeping in mind. L: So far, we’ve looked at how we can   use pitch to change the meanings of full phrases  or sentences, but we can also use changes in pitch   to change the meanings of specific words. G: Right.(14:40) This is less of   “ice-cream-question-mark” versus “ice-cream-yay”  or “ice-cream-sarcastic” and more like   “ice cream” versus “doorknob” or  something completely different.   L: Or famously in Mandarin, the difference  in tone creates a difference between the   word “mother” and “horse” but also the  words “hemp” and “scold,” which are all   part of the four-tone system in Mandarin.(15:02) G: They’re all based on “ma” pronounced   with different tones. You have the  word for “mother” which is “mā.”   L: That’s high level. G: “Mā.” The word for “hemp” which is “má.”   L: That’s a rising tone. G: “Má.” The word for “horse” which is “mǎ.”   L: That’s falling with a  bit of a rise at the end.   G: “Mǎ.” And the word for  “scold,” which is “mà.(15:23) ”   L: Which is just directly falling. G: “Mà.” There are four tones in Mandarin.   For the particular syllable “ma,” each of them  corresponds to a word. But you could have other   syllables where there happens to be a gap, and in  this particular tone combined with this particular   syllable, there isn’t a word that corresponds  to that gap, whereas you don’t have something   like in English, “Oh, we just never say this word  with question intonation.(15:48) You can’t question this   word. No one ever questions peanuts.  They just don’t get questioned.”   L: Because the tone is so integral to the  meaning of the word, tone is often much more   likely to be expressed in the writing system  if a language does have a writing system.   G: Both the Mandarin-type thing where the  tone changes the meaning of the word and   the English-type thing where the tone affects the  meaning of the whole phrase, they’re both drawing   on a similar resource at the acoustic level in  terms of how the pitch melody changes as you’re(16:22) producing the thing. But because they have such  different functions in terms of language, they get   referred to by different names. The English one  is “intonation,” and the Mandarin one is “tone.”   These are both words that crop up sometimes  used a bit more loosely, but in the technical   linguistic sense, “tone” is when the meaning  of the word itself changes, and “intonation” is   when the broader meaning of the word as it fits  into the phrase or into the discourse changes.(16:50) L: As far as we know, every spoken language  makes use of intonation. Tone is actually   pretty prevalent. There’re some estimates  that 60-70% of the world’s languages do   have this word meaning-changing tone function  to some extent; it’s just that a lot of these   languages are those languages with really small  populations that you hear less about, and they’re   concentrated outside of the Indo-European family.(17:18) G: With the notable exception of Mandarin and   other Chinese languages – all of which, I think,  have tone – which are not small languages.   L: There’re definitely many large languages  like Vietnamese and Hmong as well as, you said,   the Chinese languages that have tone that are  national languages – very visible – but also many,   many of the world’s smaller languages also  have tone systems of some type or another.(17:38) G: But because all languages make use of  intonation somehow, if you’re not already   familiar with a tone language, and you’re  trying to learn one, sometimes people draw   on the intonation resources by writing Mandarin  tones using question marks and exclamation marks   and things like that as a cue to bridge you  over to using it for tone purposes.(17:56) This can   be pretty effective at a learning level. L: Huh, yeah, I could see how that would   be useful. So, for that second tone, which is  rising, you could map that onto your understanding   of question intonation, which is also rising. G: Exactly. This can be, sometimes, a notation   thing that people can use to take notes with and  help remember how to pronounce it.(18:13) I find, for me,   I haven’t really tried to learn Chinese, but I’ve  been exposed to enough of the same “ma” example   that shows up in linguistics a lot that I can now  hear it and reproduce it immediately after someone   has produced it, but I have a hard time retaining  it in my long-term memory which tone a particular   word has just because this is not something  that I’m in the habit of paying attention to.(18:33) But people do learn tone languages in adulthood.  It’s a thing that’s possible. I just haven’t put   enough effort into it. L: Confessions.   G: Like, there’re a lot of languages.  I’d like to learn them all, but you know,   so many languages, so little time. L: Beyond using your English punctuation   hack to correlate to different tones, there  are a variety of ways of writing especially the   Mandarin tone system – especially if you’re using  a Roman orthography.(19:01) Some of those have been taken   up more than others across different systems. G: I think the most common way that people write   tones in Mandarin these days is just using  accent marks or diacritics on the vowels.   You can have the “mā” tone being written with a  flatline above the vowel. And then you can have   an upwards-pointing line and a downwards-pointing  line, and something that points down and then up,   to match the shape of the tones.(19:24) L: I think it’s become a lot easier to   use these diacritics above the vowel for the  tone with modern computer systems. I’m very   grateful that we have those to make that kind of  writing system easier. But there have been some   other fun proposals over the years as well. G: I am particularly fond of a proposal not   necessarily for its practical benefit but for  its interesting-ness called “Gwoyeu Romatzyh” –   hope I’m pronouncing that right.(19:52) This is  a romanisation system that’s based on,   okay, what if we just spelled each of the  tones differently using Roman letters.   L: Okay, so you spell the vowel part,  which is where we hear the tone,   differently depending on what the tone is? G: Yeah. For example, what if you doubled the   vowel – you know, instead of “A,” you wrote  “AA” – to indicate one variant of tone.(20:14) Or   what if you put a silent R, that would be in your  variety of English, after some vowels to indicate   another kind of tone. Or what if you changed  – instead of writing “NG” you wrote “NQ” and   that was another way of writing a tone.  And you would know based on the spelling,   “Actually, this is different tones.(20:35) ” L: I’ve definitely seen Q used at the   end of words as a silent – it’s not a letter, it’s  just indicating that it’s a particular kind of low   or falling tone in other languages where it was  before the magic of easy computer writing systems   and people were typing thing up on typewriters.(20:52) I didn’t realise that they’ve probably got that   from this older Chinese system. How interesting. G: This is a system that was invented by this   very, very cool Chinese linguist in history  named Zhao Yuanren, who’s my favourite guy.   L: I know Zhao from another way of transcribing  Chinese tones. I didn’t realise he’d come up   with all these different ways. “Zhao numbers” are  where you use a set of numbers to indicate tone.(21:18) I like this one because it gives you a little  bit of information about what’s happening with   the acoustics. You have the numbers one to five  – “one” being the lowest range in the melody that   people are using and “five” being the highest.  Because these Mandarin have contours and movement,   so you’re falling tone is “51” because it’s  going from the highest to the lowest point,   your rising tone is “35” because  it’s rising, but the rise is less   than that full fall on the falling tone. G: That’s a really elegant system because(21:50) it can also work for other languages beyond  just Mandarin. You could use it to describe,   in principle, any tone system as long as it’s  either flat or just doing one transition. I guess   you could put three numbers beside each other if  you wanted to do rising-falling-rising-again.   L: Definitely a lot less opaque than the changing  the way you spell the vowels in a word, which is   probably why it stayed around a bit longer.(22:15) G: But also, not necessarily the most practical   thing because typing numbers every time you  type a vowel so you can indicate what tones   it has might get kind of tedious. L: Especially because they are   written superscript, which is  often quite annoying to type.   G: Especially on computers.(22:32) I just love that  both of these systems are by the same guy, Zhao,   who is also the guy that came up with the famous  Chinese sentence that illustrates the necessity   of writing tone in Chinese – he had some themes –  which is the tongue-twister sentence that’s about   the lion-eating poet in the stone den. L: Ooo, this is the one where it’s the   same consonants and vowels and the only  thing that changes is the tone, right?   G: Yeah, it’s just all versions of “shi” with  different tones.(22:57) If you write with without the   tones, it’s just “shi shi shi shi shi,” and I’m  not gonna go it justice by saying it out loud,   but we are linking to a recording. It’s a really  good demonstration of the necessity for one out of   the many competing systems that he invented. L: It’s worth just saying that the Chinese   writing system is such that because each word  has its own character, the characters are all   sufficiently different. They’re not based on  the consonants and vowels.(23:23) So, you memorise   the character including its tone information.  This is just something we’ve had to solve for   more phonetic writing systems like English. G: Right. And for trying to transliterate Chinese   into Roman characters, which is sort of the  project – he was involved with a lot of the early   Romanisations in the 1930s and trying to figure  out how to go about doing that.(23:41) The neat thing   about this poem is that it reads differently  in different Chinese varieties. In Classical   Chinese and in the writing system, it’s coherent.  In Mandarin, it’s just four syllables because   Mandarin’s just got four tones. But in Cantonese  or Hokkien, it’s got 22 syllables or 15 syllables   because these varieties have more tones.(24:05) L: Another tone language that went through   a Romanisation process but took a different  approach to writing systems was Vietnamese,   which has six tones. Vietnamese has also  gone with this diacritic approach where you   put little additional bits of information  above or below the vowel, but it’s taken   a very different approach to Mandarin.(24:27) G: I’ve seen Vietnamese on signs or on menus   and things like that, and it’s really distinctive  for having that little curved diacritic on the   top of some of the vowels. It looks like a little  backwards C or a hook. For example, in a word like   “phở,” which is a delicious noodle dish, you see  the curve at the top.(24:44) What I didn’t realise until   we were doing research for this episode was that  this is actually from the interrogative question   mark because Vietnamese had a lot of contact  with French, which also uses question marks   to indicate a rising intonation, and so this  indicates a rising intonation because it was   originally modelled after a question mark. They  just made it really tiny and put it on the vowel.(25:03) L: Huh! G: Isn’t that cute?   L: I’m used to diacritics that have a little  rising bit because the intonation goes up,   but I didn’t realise that this was directly  inspired by the rising intonation of the   question mark. G: Yeah!   L: That’s a good story. G: You’ve worked on Tibetan   languages, right? There’s tone in those? L: There is tone in Tibetan languages.(25:25) Yolmo and   Syuba, the languages I work with, have a two-tone  system which only happens with some combinations   of sounds. For sounds like /ma/ you can have  “má” and “mà,” or a sound like /tə/, you can   have “tó,” which is “rice,” and “tò,” which is  “stone.” But there’re some sounds – like if you   have a /kə/, there’s only ever a high tone.  Like, “ká” is the word for “mouth.(25:55) ” If you   have a sound like /gə/, you’ll only ever have a  low tone. The tone isn’t for every combination   of sounds. It is depending on the environment  of the consonants that it’s hanging out with.   G: How do people go about writing that? L: The languages I work with have taken the   Nepali writing system, which was designed for  Indo-Aryan languages but maps pretty well to   their sound system, and they often include a H to  indicate low tone because that low tone is kind of   breathy. They have a silent H there. G: The “H” is not for “high”;(26:27) it’s for the breathy low tone. L: Yeah. Just to be non-English about it. That   word “stone” would be T-O-H in English orthography  and using the H character in Devanagari as well.   G: Not that far off one of  Zhao’s proposals, in fact.   L: Not that far off one of Zhao’s proposals except  that I think the Q was somewhat arbitrary, and the   H does correlate with this kind of /h/ vibe to the  vowel that the low tone brings.(26:56) But for Tibetan   languages that are written with the Tibetan  script, what’s really interesting is the script   doesn’t have anything about tone because it was in  existence before the language developed tone. It’s   something that can come about in a language. G: So, the script is older than   tone in the language itself. L: Yeah.(27:15) And so, you tend to know   what words have high or low tone because it’s that  same kind of environment factor if it’s something   that is more likely to have a high tone or a low  tone. But it’s done with these very elaborate   consonant clusters, which used to be pronounced  and now aren’t and have become the tone system.   G: Sometimes, you get a silent letter like E  that used to be pronounced, and at the time,   it cued sound changes in the words.(27:42) So,  if you have something like “mat” versus   “mate,” the E in “mate” would at one point  have cued the vowel to be different. Now,   even though that letter is silent, it still cues  the same sort of sound changes that it used to.   L: Except that it’s just doing it with  tone in Tibetan. You have this nice   little time capsule of how the language  has changed sounds but still allows you   to read tone into the language as well.(28:08) G: One of the ways of writing tones that   I think is super interesting that we’ve  talked about on the podcast a little bit   before – just switching continents a little  bit from Asia to Mexico – is in Chatino,   which was in our interview with Hilaria Cruz,  which we’ll link to, they’ve got either 14 or   11 tones depending on what you’re counting.(28:29) In  either case, that’s too much to use a diacritic   accent mark-based system because that’s a lot of  teeny-tiny accent marks. It’s also kind of a lot   to use a numerical-based system because that’s  more than nine or ten numerals to put at the   end of your syllables. Instead, they use super  script letters to indicate the different tones.   L: That’s a good solution.(28:51) G: They have super script A, B,   C and so on to indicate the different tones that  are relevant for Chatino. Sometimes, they’re just   written in all caps at the end of the word if the  computer doesn’t support super scripts. These can   convey the tones that they’re using. L: While we’re in the region, Zapotec   is another language that has tone. It uses  tone for something more grammatical.(29:11) So far,   we’ve been talking about how we change between  words like “mother” and “horse,” or “stone”   and “rice.” They’re completely different words  that are unrelated to each other. In Zapotec,   you can use three different tones to create  differences in the grammar of the language.   G: The difference between “I will write” and  “You will write” – there’s a suffix that’s   added on to mean “I.(29:37) ” And then a high tone  also gets added near the beginning of the   word to go with that suffix which indicates  I’m doing it as opposed to you’re doing it.   L: This use of tone for grammatical things like  tense or negation is also incredibly common across   Central and Southern African languages as well. G: There’s an example from Dinka, which   is a language spoken in Sudan, where the tone  is the thing that makes a difference between the   meanings of the following four sentences.(30:06) One  is “I hate Acol,” which looks like a person’s   name. Two is “Acol has been hated.” L: So, we’re moving who is doing the   hating and making it a passive. G: Right. Or “You hate Acol” –   also yet another tone. L: Changing it from “I” to   “you,” so changing the subject. G: And then “Acol is hated.”   L: Oh, the present passive as opposed to past   passive. I feel really sorry for Acol. G: Yeah, I dunno who Acol is.(30:27) I don’t know   why they keep showing up in these examples  sentences and why people hate them so much,   but grammatically it’s very interesting. L: Indeed. The only thing that’s changing is   the tone on the verb “hate,” and that’s  creating different forms of the verb.   G: It’s doing a lot of really interesting  grammatical things in terms of changing   important parts of the meaning.(30:49) The  use of tone for grammatical purposes,   like changing it from “I do something” to “This  has been done” or changing something from “I   did it” to “You did it,” this gets lumped in  together with tone in general – the use of tone   to distinguish between one word and another  word. I think that’s just because languages   that use tone for grammatical purposes also use  it to distinguish between individual words.(31:08) L: There’s an incredibly wide range of ways in  which especially languages like Dinka can use   tone for a whole heap of different grammatical  functions and word-changing functions.   G: That brings us to “Okay, if the majority  of the world’s languages have tone, and all   the world’s languages have intonation, what  happens when you’re trying to do something like,   say, ask a question, which often comes with a  characteristic intonation, and also your language   has individual tones on the individual words?” L: The answer is: it depends on how the tone(31:39) system works, and how that comes  together with intonation. Let’s look   at some contrasting examples to simplify it. G: You mean we’re not gonna run through every   single language in the world and exactly how  its tone and intonation systems work together?   L: Well, I’ve only researched one.  I’m gonna start with that one.(31:57) G: All right. Well, tell us about that one. L: This is one of those “I kind of messed   something up and it turned out to be for the  best” stories. We wanted to collect some tone   data for Syuba, and so I asked some speakers to  read out some word lists. I thought I was trying   to be pretty good at preventing them from doing  list intonation because that would get in the   way of the tones. But for one or two speakers,  we really didn’t do as good a job.(32:22) It’s very   hard when you’re recording long lists and it’s  been long days. We had one or two speakers where   there was this really strong list intonation. G: In English, list intonation would be something   like if you’re reading “Apples, bananas, oranges,  ice cream, cake,” and each of those words is like,   “There’s another word in this list.” L: Yes.(32:41) You have this little rise at   the end. That’s what I was getting in these  recordings. But it turned out to be really   useful because it showed us that intonation  overruled tone in Syuba for speakers.   That’s not a problem because there are only two  tones. Not all words have a contrasting tone   pair. Tone is not doing as much heavy work in  meaning, and intonation can take over from it.(33:07) G: Sometimes, when you have a language with more  tones, the tone and intonation interact with each   other. Say you’re trying to put higher intonation  at the end of a sentence for a question. That   might just make every tone a little bit  higher compared to what it would’ve been   if it wasn’t a question. You can still hear that  the tones are doing slightly different things.(33:25) L: You see this with musical pitch as well. How a  language is sung, the tone system might – again,   with Syuba, speakers are very happy to just  make the words fit into a melody because the   melody of the music is more important – and not  that there’re many songs about stones and rice,   but if you were singing a song, you’d  probably know if someone was talking   about rice or talking about stones. You don’t  need the tone to give you that information.(33:52) G: Ooo, can I talk about my  favourite example of this?   L: Sure. G: This is a difference   between Mandarin and Cantonese. L: Okay.   G: Both of which have tones, but Mandarin has  four and Cantonese has six or nine depending   on how you count.(34:11) In Mandarin, it’s long been  customary in music to not really pay attention   to the relationship of tone and meaning,  and context is just enough to fill it in.   L: A bit like Yolmo and Syuba. G: Yeah, whereas in Cantonese,   there’s a long history in Cantonese opera,  which is carried into Cantonese pop,   of matching the tones to the notes.(34:28) L: Again, that makes complete sense   if that’s the priority your language has. G: Right. This is largely relative, at least in   pop songs. If the next note in the song is lower  in pitch, then you want the word to be lower in   tone. Or if it’s rising in pitch, you want it  to be higher in tone – the next word – and just   sort of continue along that melody.(34:50) But this comes  into problems if you’re trying to translate songs,   and you’ve already got a melody, and you’ve  already got a sense of what the word meaning you   want is. If you are, for example – and this has  happened – a Christian missionary going to China   translating the meanings, the lyrics to hymns – L: Hymns that have existing melodies already   that you probably don’t wanna change.(35:12) G: Nope, that you probably don’t wanna change,   and you have a general vibe to the words already  that you’re not super keen on changing either,   you can end up with really funny things because  if the tone mismatches, people interpret the words   as something different. The example that I have  is a hymn that was intending to say, “I am the   sheep of the lord,” turned into something  that sounded like, “I am a pig’s face.(35:33) ”   L: Not quite the same vibe. G: No. Because apparently   “lord” and “pig” are the same syllables,  the same consonant-vowel combination,   but with different tones on  them. So, this is a confusion   that comes up maybe kind of a lot. L: A very good lesson for those working   with tone languages doing translation.(35:56) G: Make sure to do cultural consultation   if you wanna translate song lyrics. L: Throughout this whole episode,   we’ve been talking about high and low tone and  giving examples and mapping that onto ways of   talking about sound that we’re used to from music  and from melody, but it’s worth just saying very   briefly that this is a cultural metaphor that  we have when we’re talking about sounds.(36:17) G: Oh, yeah, I guess it is. L: Going back to our interview with   Professor Suzy Styles about how we think about  physically abstract things like sounds in terms of   spatial realities and using highness and lowness  as a metaphor. It’s not the only metaphor.   G: What other metaphors do  different cultures use?   L: There’s a metaphor in Farsi for pitch  where you have “thin” or “thick.(36:41) ”   G: Can I guess which one’s thin and which one’s  thick to see if it maps cross-culturally?   L: Have a go. G: All right. I’m gonna say that high notes are   “thin” notes and low notes are “thick” notes? L: Yeah.   G: Excellent. L: But “thin” and “thick” is their default   way of talking about it. There’re probably plenty  of other metaphors cross-culturally.(36:59) In fact,   when I was learning to listen to tone in Syuba,  I would talk to people about “high” and “low.”   But one day we got ourselves into terrible  confusion when I was working with one person   because we were both using “high” and “low,”  but I was using it in terms of musical pitch,   and he was using it in terms of social status  where what I thought of as “high” and “small”   and “thin,” he was thinking of “small and thin and  therefore socially inferior compared to someone   who was big and round and rich.” G: Sitting up on a big chair.(37:29) L: Yes. So, low tone was very solid  and social status and had authority,   and we were using opposite high-low metaphors.  I was using a spatial one; he was using a   social status one. We ended up coming up with an  agreement where we would just talk about whether   it was the “rice” tone or the “stone” tone.(37:53) G: Perhaps something that doesn’t necessarily   translate cross-culturally as much but  definitely a practical solution at the time.   L: Side-stepping any cultural metaphors  that either of us were using.   G: That’s great. L: I like it because it explains this   confusion that we both talked about earlier on  about whether one was a high tone or a low tone.(38:09) It depends on whether you’re thinking of one as  solitary and small, tiny unit, and therefore high,   or if you’re thinking about it as big and grand. G: Sort of the baseline that other things build up   from or something like that. L: Yeah.   G: Going back to our metaphor of playing the same  melody on a small instrument, like a piccolo,   or a large instrument like an oboe, maybe  we could also talk about “small” tone versus   “large” tone. We could even see how many possible  different tone metaphors we can come up with.(38:38) L: I think there’s still a lot that we can  learn across different languages for how they   think and talk about tone and intonation. G: We could try to make a list of how many   different possible tone and intonation  metaphors we can come up with.   [Music] G:   For more Lingthusiasm and links to all  the things mentioned in this episode,   go to lingthusiasm.com.(39:03) You can listen to us  on Apple Podcasts, Google Podcasts, Spotify,   SoundCloud, YouTube, or wherever else you get your  podcasts. You can follow @lingthusiasm on Twitter,   Facebook, Instagram, and Tumblr. You can  appreciate my list intonation right here.   You can get fancy, aesthetic IPA charts,  “Not Judging Your Grammar” stickers,   and other Lingthusiasm merch at  lingthusiasm.com/merch.(39:22) I can be   found as @GretchenAMcC on Twitter, my blog  is AllThingsLinguistic.com, and my book about   internet language is called Because Internet. L: I tweet and blog as Superlinguo. Have you   listened to all the Lingthusiasm episodes, and  you wish there were more? You can get access to   an extra Lingthusiasm episode to listen to every  month plus our entire archive of bonus episodes to   listen to right now at patreon.com/lingthusiasm or  follow the links from our website.(39:46) Have you gotten   really into linguistics, and you wish you had more  people to talk with about it? Patrons can also get   access to our Discord chatroom to talk with other  linguistics fans. Plus, all patrons help keep   the show ad-free. Recent bonus topics include a  language and gender Q&amp;A with Dr.(40:01) Kirby Conrod and   the way science fiction depicts various futures  for the English language. Can’t afford to pledge?   That’s okay, too. We also really appreciate it  if you can recommend Lingthusiasm to anyone in   your life who’s curious about language. G: Lingthusiasm is created and produced by   Gretchen McCulloch and Lauren Gawne.(40:18) Our Senior  Producer is Claire Gawne, our Editorial Producer   is Sarah Dopierala, our Production Assistant is  Martha Tsutsui-Billins. Our music is “Ancient   City” by The Triangles. L: Stay lingthusiastic!   [Music]" }, { "title": "Using Leverage to Overcome Habit Challenges", "url": "/posts/leverage-habits/", "categories": "Habits", "tags": "habits, leverage", "date": "2014-01-18 01:41:58 -0500", "snippet": "Changing habits can be a daunting task, especially when willpower seems insufficient to sustain them. In this post, we’ll explore how leveraging small efforts can be the key to overcoming these cha...", "content": "Changing habits can be a daunting task, especially when willpower seems insufficient to sustain them. In this post, we’ll explore how leveraging small efforts can be the key to overcoming these challenges and ensuring lasting habit changes.What Is Leverage?Leverage refers to the ability to use a small initial effort to create significant, lasting results. Imagine a lever: by applying a small force on one end, you can move a much heavier object on the other. The same principle applies to habit change. By leveraging a small burst of willpower, you can set yourself up for long-term success, even when the habit feels overwhelming at first.Commitment as a Form of LeverageOne of the most powerful ways to create leverage is through commitment. By writing down your goals or publicly declaring your intentions, you hold yourself accountable. This small act of commitment can dramatically increase your chances of success because you don’t want to be seen as a failure or hypocrite. Public accountability can push you to take action when you would otherwise give up.Real-Life Example:Consider the story of a boss who wanted to quit smoking. He put up a billboard with his picture and offered a $100,000 reward to anyone who caught him smoking. This extreme commitment leveraged his willpower and led to his success in quitting.Pain and Pleasure: A Powerful MotivatorAs Tony Robbins famously said, “We fail to act when we associate more pain with action than with staying the same.” To successfully change a habit, you need to flip this dynamic. Create Pain with Inaction: Start associating significant pain with the idea of not changing. For instance, visualize the health consequences of continuing unhealthy habits. Associate Pleasure with New Habits: Conversely, link immense pleasure to the new habit. If you view healthy eating as enjoyable and fulfilling, you’ll be more motivated to make that change.Mental Shift:If unhealthy habits feel comforting, they will be difficult to break. But if you begin to associate them with negative consequences, the desire to change becomes much stronger.Identity and Habit ChangeThe most significant barrier to long-term habit change often comes down to your identity—how you see yourself. If you view yourself as a smoker or someone who can’t stick to a diet, you’re setting yourself up for failure.Redefining Identity:Long-term change happens when you redefine your identity. Start seeing yourself as someone who doesn’t smoke or as someone who makes healthy choices effortlessly. Aligning your identity with the habits you want to build is crucial for making changes stick.Leverage Is Uncomfortable, But It WorksUsing leverage techniques can be uncomfortable. However, this discomfort is often necessary to push through initial resistance. The more pain you associate with your old habits and the more pleasure you tie to your new ones, the easier it becomes to sustain the change.Building Momentum:Once you’ve succeeded with a few habit changes, you’ll find the process becomes much easier and even enjoyable.Final ThoughtsIf you’re struggling to stick with a new habit, leverage may be the missing piece. Start small—make a public commitment, set yourself up for accountability, and begin associating real pain with staying where you are. Over time, you’ll find that this small initial push can lead to massive, lasting transformations in your habits and your life." }, { "title": "Conditioning Habits - The Key to Lasting Change", "url": "/posts/conditioning-habits/", "categories": "Habits", "tags": "habits, conditioning", "date": "2014-01-12 01:28:45 -0500", "snippet": "The Power of ConditioningHabits are formed through repetition. To change an existing habit or build a new one, we must consciously condition it through consistent, repeated actions. This process do...", "content": "The Power of ConditioningHabits are formed through repetition. To change an existing habit or build a new one, we must consciously condition it through consistent, repeated actions. This process doesn’t rely solely on willpower, but rather on continuously overriding default behaviors until the new habit takes hold. Think of it like launching a space shuttle—it requires a lot of initial energy, but once in orbit, it continues effortlessly.Natural Conditioning: The 30 Day TrialOne highly effective method for natural conditioning is the 30 Day Trial, popularized by Steve Pavlina. The idea is to commit to a new habit for 30 consecutive days, practicing it at least once daily. Consistency is crucial, so it’s essential to restart the 30 days if any exceptions are made.The process may be challenging in the first few days, and obstacles often arise after the second or third week, testing your commitment. However, overcoming these challenges reinforces the habit, making it easier as time goes on. By day 30, the new habit often feels natural and sustainable.Simulated Conditioning: Visualizing SuccessFor habits that can’t be practiced daily, simulated conditioning can be a useful alternative. This involves visualizing the experience or pretending to perform the habit in a controlled setting. While more complex than natural conditioning, it can be effective for infrequent habits, such as managing your temper during arguments.Key Principles for Effective Conditioning Use natural conditioning for habits you can practice daily, like the 30 Day Trial. For less frequent habits, try simulated conditioning through visualization or role-playing. Consistency is key: If you aren’t sure the habit is conditioned, repeat it until it feels natural. Focus on changing one habit at a time to maximize your chances of success.ConclusionBy mastering conditioning techniques, you can build better habits and take control of the subconscious patterns that drive your life. Remember, consistency is the key to success. Embrace the challenges, celebrate your progress, and watch as your new habits become an effortless part of your daily routine." }, { "title": "Understanding Habits", "url": "/posts/understanding-habits/", "categories": "Habits", "tags": "habits", "date": "2014-01-10 01:01:35 -0500", "snippet": "In exploring the nature of habits, we uncover essential insights that can help us understand how to change and improve our behaviors. Here are the key principles and messages regarding habits:What ...", "content": "In exploring the nature of habits, we uncover essential insights that can help us understand how to change and improve our behaviors. Here are the key principles and messages regarding habits:What is a Habit? Definition: A habit is a behavior that becomes automatic through repetition, often running subconsciously in our daily lives. Formation: Habits are formed through a cycle involving a trigger (cue), routine (action), and outcome (reward).The Role of the Brain Subconscious Processes: Our brains streamline common tasks by creating automatic pathways, allowing us to perform actions without conscious thought, similar to how a computer runs programs. Emotional Resistance: Emotions like frustration and fear can hinder our ability to learn and modify habits. Recognizing these emotional barriers is crucial for effective habit change.Importance of Awareness Awareness as the First Step: Identifying existing habits is essential for improvement. Without recognizing destructive habits, we are unlikely to initiate change. Methods for Awareness: Internal Review: Engage in self-reflection and analyze your behaviors. Measure habits objectively to uncover those that may be harmful. External Study: Gain knowledge from books, articles, and observing successful individuals to understand and improve your own habits. Challenges of Destructive Habits Pain and Pleasure: Many habits are linked to avoiding pain or seeking pleasure, making them particularly challenging to change. These habits often operate without our conscious awareness.Strategies for Change Self-Reflection: Regularly assess your behaviors and recognize those that need improvement. Measurement: Keep track of your habits to identify patterns and areas for change. Seek Knowledge: Learn from external sources to gain insights into effective habit formation and modification.Looking AheadThe next steps in this journey will explore methods for conditioning habits, the role of willpower, and techniques to facilitate habit change without the frustration of failure.By understanding the nature of habits and employing strategies for awareness and improvement, we can effectively modify our behaviors and enhance our personal growth. Embrace the process, and remember that recognizing and addressing habits is the first step toward meaningful change." } ]
